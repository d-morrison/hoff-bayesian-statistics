% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=15mm,bottom=20mm]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{3}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother

\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
% Make links footnotes instead of hotlinks:
\DeclareRobustCommand{\href}[2]{#2\footnote{\url{#1}}}
\hypersetup{
  pdftitle={Chapter 1: Introduction and examples},
  pdfauthor={Jesse Mu},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Chapter 1: Introduction and examples}
\author{Jesse Mu}
\date{2016-09-07}

\begin{document}
\maketitle

\renewcommand*\contentsname{Contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}

\section{Introduction}\label{introduction}

\emph{Bayesian inference}: the process of learning by updating prior
probabilistic beliefs in light of new information. Data analysis tools
built on these foundations are known as \emph{Bayesian methods}.

\subsection{The Bayesian learning
framework}\label{the-bayesian-learning-framework}

We want to estimate a parameter \(\theta \in \Theta\) from a dataset
\(y \in
\mathcal{Y}\).

\begin{itemize}
\tightlist
\item
  \(p(\theta)\), defined for all \(\theta \in \Theta\), is our prior
  distribution about the space of possible parameters
\item
  Bayesian methods require a \textbf{sampling model}: \(P(y \mid
  \theta)\) describes the probability that of a specific dataset given a
  parameter.

  \begin{itemize}
  \tightlist
  \item
    Note that later, this will be useful for prediction!
  \end{itemize}
\end{itemize}

Then we wish to update our belief distribution about \(\theta\) given.
The \emph{posterior distribution} is defined as

\begin{align}
p(\theta \mid y) = \frac{p(y \mid \theta) p(\theta)}{\int_{\Theta}p(y \mid \tilde{\theta}) p(\tilde{\theta}) \; d\tilde{\theta}}.
\end{align}

Note that the denominator is constant and doesn't need to be computed,
since we can just normalize our posterior distribution such that
\(P(\theta \mid y)\) for all \(\Theta\) sums up to 1. Thus we commonly
write

\begin{align}
p(\theta \mid y) \propto p(y \mid \theta) p(\theta).
\end{align}

\subsubsection{versus frequentist
learning}\label{versus-frequentist-learning}

(from
\href{https://www.umiacs.umd.edu/~resnik/pubs/LAMP-TR-153.pdf}{Resnik \&
Hardisty, ``Gibbs Sampling for the Uninitiated''})

The difference between Bayesian learning and frequentist learning is the
consideration of \emph{prior beliefs} about parameters. In standard
Maximum Likelihood Estimation (MLE), we select the parameter that is
most likely to have generated the observed data:

\[
\theta_{ML} = \underset{\theta}{\text{argmax}} \; p(y \mid \theta).
\]

Using Bayesian Maximum A Posteriori Estimation, however, we select
\(\theta\) that is most likely given the observed data. The difference
is that our measure of ``likelihood given the data'' is influenced by
our prior beliefs about \(\theta\), as in Equation 2:

\begin{align}
\theta_{MAP} &= \underset{\theta}{\text{argmax}} \; p(\theta \mid y) \\
  &= \underset{\theta}{\text{argmax}} p(y \mid \theta) p(\theta).
\end{align}

Note that with an uninformative prior \(\theta \sim \text{Uniform}\),
the MAP estimate is the same as the ML estimate.

\section{Why Bayes?}\label{why-bayes}

\subsection{As an approach to probability and
statistics}\label{as-an-approach-to-probability-and-statistics}

The debate between Bayesians and frequentists dives into some very
philosophical issues. Cox's theorem (1946, 1961) gives a formal proof
for thinking about probabilities using a Bayesian approach. I will
likely want to look at an explanation of these proofs later:
http://ksvanhorn.com/bayes/Papers/rcox.pdf

Outside of formal mathematical grounding, Bayesian methods have
excellent practical benefits as data analysis tools:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Even if prior probabilities are not exactly quantifiable,
  approximations of \(p(\theta)\) and \(p(\theta \mid y)\) are still
  useful for analyzing how rational learners would change beliefs
\item
  Bayesian methods can represent principled ways of doing analysis when
  there are no alternative methods
\end{enumerate}

\subsection{As models of cognition}\label{as-models-of-cognition}

An appeal of Bayesian learning is that it is also cognitively intuitive.
Humans have beliefs about the world, whose uncertainty can be expressed
probabilistically. Then, given data, these beliefs are rationally
updated. There is a rich tradition in modeling human cognition using
Bayesian methods to great success, with plenty of work done in showing
how people's beliefs and knowledge about the world can be expressed
probabilistically
(e.g.~\href{http://web.mit.edu/cocosci/Papers/Griffiths-Tenenbaum-PsychSci06.pdf}{Griffiths
\& Tenenbaum, 2006})

Bayesianism is not without its detractors, however. Some critics argue
that the evidence that Bayesian analysis is weak, and that sufficiently
sophisticated models are unfalsifiable. See
\href{http://psycnet.apa.org/journals/bul/138/3/389/}{Bowers \& Davis
(2012)}, comment by
\href{http://cocosci.berkeley.edu/tom/papers/LabPublications/HowBayesiansGetBeliefs.pdf}{Griffiths,
Chater, Norris, Pouget (2012)}, reply by
\href{http://www.ncbi.nlm.nih.gov/pubmed/22545688}{Bowers \& Davis
(2012)}.

\section{Example 1: Estimating the probability of a rare
event}\label{example-1-estimating-the-probability-of-a-rare-event}

We are interested in the prevelance of a disease in a city. Let \(\theta
\in [0, 1]\) be the fraction of infected individuals. We take a sample
of 20 individuals and record the number of individuals \(y \in Y
= {0, 1, \dots, 20}\) with the disease.

The sampling model is

\[
Y \mid \theta \sim \text{Binomial}(20, \theta),
\]

i.e.~each individual has an independent \(\theta\)\% chance of having
the disease.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{y =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{20}\NormalTok{,}
  \AttributeTok{theta =} \FunctionTok{factor}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.10}\NormalTok{, }\FloatTok{0.20}\NormalTok{), }\AttributeTok{each =} \DecValTok{21}\NormalTok{)),}
  \AttributeTok{probability =} \FunctionTok{c}\NormalTok{(}\FunctionTok{dbinom}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{, }\FloatTok{0.05}\NormalTok{), }\FunctionTok{dbinom}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }\FunctionTok{dbinom}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{20}\NormalTok{, }\DecValTok{20}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}
\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{(d, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ y, }\AttributeTok{y =}\NormalTok{ probability, }\AttributeTok{fill =}\NormalTok{ theta)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{1_files/figure-pdf/unnamed-chunk-2-1.pdf}}

}

\caption{For various theta, the probability of observing y infected
individuals in the sample.}

\end{figure}%

Imagine we believe \(\theta\) is probably in the interval
\([0.05, 0.20]\). For computational convenience, we will encode this
prior as a Beta distribution

\[
\theta \sim \text{Beta}(2, 20)
\]

Conveniently, (we will prove this later), given \(Y \mid \theta \sim
\text{Binomial}(n, \theta)\) and \(\theta \sim \text{Beta}(a, b)\),

\[
(\theta \mid Y = y) \sim \text{Beta}(a + y, b + n - y).
\]

For example, if we observe 0/20 individuals infected,
\(\theta \mid {Y = 0} \sim
\text{Beta}(2, 40)\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\OtherTok{=} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{theta =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.001}\NormalTok{),}
  \AttributeTok{distribution =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"prior"}\NormalTok{, }\StringTok{"posterior"}\NormalTok{), }\AttributeTok{each =} \DecValTok{1001}\NormalTok{),}
  \AttributeTok{density =} \FunctionTok{c}\NormalTok{(}\FunctionTok{dbeta}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.001}\NormalTok{), }\DecValTok{2}\NormalTok{, }\DecValTok{20}\NormalTok{), }\FunctionTok{dbeta}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.001}\NormalTok{), }\DecValTok{2}\NormalTok{, }\DecValTok{40}\NormalTok{))}
\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{(d, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ theta, }\AttributeTok{y =}\NormalTok{ density, }\AttributeTok{color =}\NormalTok{ distribution)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{1_files/figure-pdf/unnamed-chunk-3-1.pdf}}

}

\caption{Prior and posterior distributions on theta after observing 0/20
infected individuals. Note the posterior is more tightly peaked around
near-zero values.}

\end{figure}%

Notice that Bayesian and frequentist approaches to parameter estimation
differ:

\begin{align}
\theta_{ML} &= \underset{\theta}{\text{argmax}} \; P(Y = 0 \mid \theta) = 0 \\
\theta_{MAP} &= \underset{\theta}{\text{argmax}} \; P(\theta \mid Y = 0) = \text{Mode}(\theta \mid Y = 0) = 0.025
\end{align}

but also notice that the point estimate is NOT equal to the expectation
of \(\theta\), since \(\mathbb{E}(\theta \mid Y = 0) = 0.048\). We will
probably later determine when to use the expectation over the mode.

Finally, notice that we can do very intuitive statistical tests (e.g
\(P(\theta <
0.10 \mid Y = 0)\)) by measuring the areas under our posterior
distribution.

\subsection{Sensitivity analysis}\label{sensitivity-analysis}

If we change the confidence in our prior, we get different posterior
distributions. The more ``peaked'' our prior is, the less peaked the
posterior will be given a \(Y = 0\) result (and the less the Bayesian
solution will approximate the ML estimate).

To quantify how changes in the prior beliefs affect our posterior
estimates, we'll do some calculations. Recall the expectation and
variance of Beta distributions. If
\(X \sim \text{Beta}(\alpha, \beta)\), then

\begin{align}
\mathbb{E}(X) &= \frac{\alpha}{\alpha + \beta} \\
\text{Var}(X) &= \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}.
\end{align}

Due to the properties of these functions we can parameterize the Beta
distribution alternatively with

\begin{itemize}
\tightlist
\item
  Expectation: \(\theta_0 = \frac{\alpha}{\alpha + \beta}\)
\item
  Precision: \(w = a + b\)
\end{itemize}

Since \((\theta \mid Y = y) \sim \text{Beta}(a + y, b + n - y)\),

\begin{align}
\mathbb{E}(\theta \mid Y = y) &= \frac{a + y}{a + b + n} \\
&= \frac{n}{w + n}\frac{y}{n} + \frac{w}{w + n}\theta_0.
\end{align}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# What is the expected value of theta after observing result y, given a Beta}
\CommentTok{\# prior parameterized by theta0 and w?}
\NormalTok{N }\OtherTok{=} \DecValTok{20}
\NormalTok{exp.posterior }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(w, theta0, y) \{}
\NormalTok{  (N }\SpecialCharTok{/}\NormalTok{ (w }\SpecialCharTok{+}\NormalTok{ N)) }\SpecialCharTok{*}\NormalTok{ (y }\SpecialCharTok{/}\NormalTok{ N) }\SpecialCharTok{+}\NormalTok{ (w }\SpecialCharTok{/}\NormalTok{ (w }\SpecialCharTok{+}\NormalTok{ N)) }\SpecialCharTok{*}\NormalTok{ theta0}
\NormalTok{\}}
\NormalTok{Theta0 }\OtherTok{=} \FunctionTok{rev}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.01}\NormalTok{))}
\NormalTok{W }\OtherTok{=} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{25}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{d }\OtherTok{=} \FunctionTok{outer}\NormalTok{(Theta0, W, }\AttributeTok{FUN =} \ControlFlowTok{function}\NormalTok{(w, theta0) }\FunctionTok{exp.posterior}\NormalTok{(w, theta0, }\DecValTok{0}\NormalTok{))}
\FunctionTok{rownames}\NormalTok{(d) }\OtherTok{=}\NormalTok{ Theta0}
\FunctionTok{colnames}\NormalTok{(d) }\OtherTok{=}\NormalTok{ W}

\NormalTok{df }\OtherTok{=} \FunctionTok{melt}\NormalTok{(d)}
\FunctionTok{colnames}\NormalTok{(df) }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}theta0\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}w\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{)}

\NormalTok{p }\OtherTok{=} \FunctionTok{ggplot}\NormalTok{(df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ w, }\AttributeTok{y =}\NormalTok{ theta0, }\AttributeTok{z =}\NormalTok{ theta)) }\SpecialCharTok{+}
  \FunctionTok{geom\_contour}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{colour =}\NormalTok{ ..level..))}
\FunctionTok{library}\NormalTok{(directlabels)}
\FunctionTok{direct.label}\NormalTok{(p, }\AttributeTok{method =} \StringTok{\textquotesingle{}bottom.pieces\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{1_files/figure-pdf/unnamed-chunk-4-1.pdf}}

}

\caption{Expected value of the posterior for theta, for combinations of
theta0 and w}

\end{figure}%

\subsection{Comparison to non-Bayesian
methods}\label{comparison-to-non-bayesian-methods}

When we use the frequentist maximum likelihood estimator, we get an
estimated \(\theta_{ML} = 0\). Since our estimate is subject to sampling
error, we commonly construct confidence intervals for these estimates.

The \textbf{Wald interval} is a commonly used confidene interval for a
population proportion. However, it is not meant to be used for small
sample sizes or situations in which the observed proportion is close to
(or equals) 0 or 1, since in these cases the error of a
binomially-distributed observation is not at all like the normal
distribution For an observation \(Y = 20\), for example, the Wald CI is,
regardless of level of confidence, just \(0\). We wouldn't want to say
with 99.999\% confidence that the population mean is 0, given our small
sample size.

The previous Bayesian estimate, however, works well for both small and
large n.~ With small \(n\), the estimator allows us to encode prior
beliefs about the true proportion. With \(w\) and \(\theta_0\) as
before:

\[
\hat{\theta} = \mathbb{E}(\theta \mid Y = y) = \frac{n}{n + w}\frac{y}{n} + \frac{w}{n + w}\theta_0.
\]

Notice that this is kind of an average between the prior expectation
\(\theta_0\) and the observed proportion of the data \(\frac{y}{n}\),
weighted by the amount of data \(n\). For large \(n\), \(\hat{\theta}\)
becomes dominated by the data, regardless of prior estimate and
confidence.

Theoretical details on the properties of Bayesian estimators are covered
later in Section 5.4.

\section{Example 2: Building a predictive
model}\label{example-2-building-a-predictive-model}

A brief synopsis of an example in Chapter 9, where we want to build a
predictive model of diabeates progression from 64 variables such as age,
sex and BMI.

We use a linear regression model, where \(Y_i\) is the disease
progression of subject \(i\),
\(\mathbf{x}_i = (x_{i, 1}, \dots, x_{i, 64})\) is a 64-dimensional
vector. With unknown coefficient \(\beta_i\) and the error term
\(\sigma\), there are 65 unknown parameters.

\[
Y_i = \beta_1 x_{i, 1} + \beta_2 x_{i, 2} + \dots + \beta_{64} x_{i, 64} + \sigma \epsilon_i
\]

We define a sparse prior probability distribution, that most
coefficients are equal to 0. (Spike and slab models?). This allows us to
conduct a Bayesian form of \textbf{feature selection} where we evaluate
the probability that a specific \(\beta_i \neq 0\) given the data
\(\mathbf{y} = (y_1, \dots, y_{342})\) and predictors
\(\mathbf{X} = (\mathbf{x}_1, \dots, \mathbf{x}_342)\).

\subsection{Bayesian regression does better than standard linear
regression}\label{bayesian-regression-does-better-than-standard-linear-regression}

The standard ordinary least squares (OLS) estimate of \(\mathbf{\beta}\)
does worse than the Bayesian method on the test set. This is due to
overfitting, and OLS's ``inability to recognize when the sample size is
too small to accurately estimate the regression coefficients.'' Sparse
linear regression models are key here, and the Bayesian sparsity prior
performs well; the common lasso technique introduced by
\href{http://www.jstor.org/stable/2346178}{Tibshirani (1996)} is also
popular, but this in fact corresponds to Bayesian methods for a special
case.

\section{Next steps}\label{next-steps}

\begin{itemize}
\tightlist
\item
  Chapter 2: probability
\item
  Chapters 3, 4: One-parameter statistical models
\item
  Chapters 5, 6, 7: Bayesian inference with normal models
\item
  Chapters 8, 9, 10, 11, 12: Inference in more complicated statistical
  methods
\end{itemize}




\end{document}
