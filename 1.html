<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jesse Mu">
<meta name="dcterms.date" content="2016-09-07">

<title>Chapter 1: Introduction and examples – Hoff Bayesian Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-68c8bffd90dad8f2b55c52d7b6410dc0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-26500bfc55c7891837a911d6d50a6255.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Hoff Bayesian Statistics</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-chapters" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Chapters</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-chapters">    
        <li>
    <a class="dropdown-item" href="./1.html">
 <span class="dropdown-text">Chapter 1: Introduction and examples</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./2.html">
 <span class="dropdown-text">Chapter 2: Belief, probability, and exchangeability</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./3.html">
 <span class="dropdown-text">Chapter 3: One-parameter models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./4.html">
 <span class="dropdown-text">Chapter 4: Monte Carlo approximation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./5.html">
 <span class="dropdown-text">Chapter 5: The Normal Model</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./6.html">
 <span class="dropdown-text">Chapter 6: Posterior approximation with the Gibbs sampler</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./7.html">
 <span class="dropdown-text">Chapter 7: The multivariate normal model</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./8.html">
 <span class="dropdown-text">Chapter 8: Group comparisons and hierarchical modeling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./9.html">
 <span class="dropdown-text">Chapter 9: Linear regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./10.html">
 <span class="dropdown-text">Chapter 10: Nonconjugate priors and Metropolis-Hastings algorithms</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./irm.html"> 
<span class="menu-text">Infinite Relational Model</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#the-bayesian-learning-framework" id="toc-the-bayesian-learning-framework" class="nav-link" data-scroll-target="#the-bayesian-learning-framework">The Bayesian learning framework</a>
  <ul class="collapse">
  <li><a href="#versus-frequentist-learning" id="toc-versus-frequentist-learning" class="nav-link" data-scroll-target="#versus-frequentist-learning">versus frequentist learning</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#why-bayes" id="toc-why-bayes" class="nav-link" data-scroll-target="#why-bayes">Why Bayes?</a>
  <ul class="collapse">
  <li><a href="#as-an-approach-to-probability-and-statistics" id="toc-as-an-approach-to-probability-and-statistics" class="nav-link" data-scroll-target="#as-an-approach-to-probability-and-statistics">As an approach to probability and statistics</a></li>
  <li><a href="#as-models-of-cognition" id="toc-as-models-of-cognition" class="nav-link" data-scroll-target="#as-models-of-cognition">As models of cognition</a></li>
  </ul></li>
  <li><a href="#example-1-estimating-the-probability-of-a-rare-event" id="toc-example-1-estimating-the-probability-of-a-rare-event" class="nav-link" data-scroll-target="#example-1-estimating-the-probability-of-a-rare-event">Example 1: Estimating the probability of a rare event</a>
  <ul class="collapse">
  <li><a href="#sensitivity-analysis" id="toc-sensitivity-analysis" class="nav-link" data-scroll-target="#sensitivity-analysis">Sensitivity analysis</a></li>
  <li><a href="#comparison-to-non-bayesian-methods" id="toc-comparison-to-non-bayesian-methods" class="nav-link" data-scroll-target="#comparison-to-non-bayesian-methods">Comparison to non-Bayesian methods</a></li>
  </ul></li>
  <li><a href="#example-2-building-a-predictive-model" id="toc-example-2-building-a-predictive-model" class="nav-link" data-scroll-target="#example-2-building-a-predictive-model">Example 2: Building a predictive model</a>
  <ul class="collapse">
  <li><a href="#bayesian-regression-does-better-than-standard-linear-regression" id="toc-bayesian-regression-does-better-than-standard-linear-regression" class="nav-link" data-scroll-target="#bayesian-regression-does-better-than-standard-linear-regression">Bayesian regression does better than standard linear regression</a></li>
  </ul></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps">Next steps</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Chapter 1: Introduction and examples</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Jesse Mu </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 7, 2016</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<!-- Setup -->
<!-- Begin writing -->
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p><em>Bayesian inference</em>: the process of learning by updating prior probabilistic beliefs in light of new information. Data analysis tools built on these foundations are known as <em>Bayesian methods</em>.</p>
<section id="the-bayesian-learning-framework" class="level2">
<h2 class="anchored" data-anchor-id="the-bayesian-learning-framework">The Bayesian learning framework</h2>
<p>We want to estimate a parameter <span class="math inline">\(\theta \in \Theta\)</span> from a dataset <span class="math inline">\(y \in
\mathcal{Y}\)</span>.</p>
<ul>
<li><span class="math inline">\(p(\theta)\)</span>, defined for all <span class="math inline">\(\theta \in \Theta\)</span>, is our prior distribution about the space of possible parameters</li>
<li>Bayesian methods require a <strong>sampling model</strong>: <span class="math inline">\(P(y \mid
\theta)\)</span> describes the probability that of a specific dataset given a parameter.
<ul>
<li>Note that later, this will be useful for prediction!</li>
</ul></li>
</ul>
<p>Then we wish to update our belief distribution about <span class="math inline">\(\theta\)</span> given. The <em>posterior distribution</em> is defined as</p>
<p><span class="math display">\[\begin{align}
p(\theta \mid y) = \frac{p(y \mid \theta) p(\theta)}{\int_{\Theta}p(y \mid \tilde{\theta}) p(\tilde{\theta}) \; d\tilde{\theta}}.
\end{align}\]</span></p>
<p>Note that the denominator is constant and doesn’t need to be computed, since we can just normalize our posterior distribution such that <span class="math inline">\(P(\theta \mid y)\)</span> for all <span class="math inline">\(\Theta\)</span> sums up to 1. Thus we commonly write</p>
<p><span class="math display">\[\begin{align}
p(\theta \mid y) \propto p(y \mid \theta) p(\theta).
\end{align}\]</span></p>
<section id="versus-frequentist-learning" class="level3">
<h3 class="anchored" data-anchor-id="versus-frequentist-learning">versus frequentist learning</h3>
<p>(from <a href="https://www.umiacs.umd.edu/~resnik/pubs/LAMP-TR-153.pdf">Resnik &amp; Hardisty, “Gibbs Sampling for the Uninitiated”</a>)</p>
<p>The difference between Bayesian learning and frequentist learning is the consideration of <em>prior beliefs</em> about parameters. In standard Maximum Likelihood Estimation (MLE), we select the parameter that is most likely to have generated the observed data:</p>
<p><span class="math display">\[
\theta_{ML} = \underset{\theta}{\text{argmax}} \; p(y \mid \theta).
\]</span></p>
<p>Using Bayesian Maximum A Posteriori Estimation, however, we select <span class="math inline">\(\theta\)</span> that is most likely given the observed data. The difference is that our measure of “likelihood given the data” is influenced by our prior beliefs about <span class="math inline">\(\theta\)</span>, as in Equation 2:</p>
<p><span class="math display">\[\begin{align}
\theta_{MAP} &amp;= \underset{\theta}{\text{argmax}} \; p(\theta \mid y) \\
  &amp;= \underset{\theta}{\text{argmax}} p(y \mid \theta) p(\theta).
\end{align}\]</span></p>
<p>Note that with an uninformative prior <span class="math inline">\(\theta \sim \text{Uniform}\)</span>, the MAP estimate is the same as the ML estimate.</p>
</section>
</section>
</section>
<section id="why-bayes" class="level1">
<h1>Why Bayes?</h1>
<section id="as-an-approach-to-probability-and-statistics" class="level2">
<h2 class="anchored" data-anchor-id="as-an-approach-to-probability-and-statistics">As an approach to probability and statistics</h2>
<p>The debate between Bayesians and frequentists dives into some very philosophical issues. Cox’s theorem (1946, 1961) gives a formal proof for thinking about probabilities using a Bayesian approach. I will likely want to look at an explanation of these proofs later: http://ksvanhorn.com/bayes/Papers/rcox.pdf</p>
<p>Outside of formal mathematical grounding, Bayesian methods have excellent practical benefits as data analysis tools:</p>
<ol type="1">
<li>Even if prior probabilities are not exactly quantifiable, approximations of <span class="math inline">\(p(\theta)\)</span> and <span class="math inline">\(p(\theta \mid y)\)</span> are still useful for analyzing how rational learners would change beliefs</li>
<li>Bayesian methods can represent principled ways of doing analysis when there are no alternative methods</li>
</ol>
</section>
<section id="as-models-of-cognition" class="level2">
<h2 class="anchored" data-anchor-id="as-models-of-cognition">As models of cognition</h2>
<p>An appeal of Bayesian learning is that it is also cognitively intuitive. Humans have beliefs about the world, whose uncertainty can be expressed probabilistically. Then, given data, these beliefs are rationally updated. There is a rich tradition in modeling human cognition using Bayesian methods to great success, with plenty of work done in showing how people’s beliefs and knowledge about the world can be expressed probabilistically (e.g.&nbsp;<a href="http://web.mit.edu/cocosci/Papers/Griffiths-Tenenbaum-PsychSci06.pdf">Griffiths &amp; Tenenbaum, 2006</a>)</p>
<p>Bayesianism is not without its detractors, however. Some critics argue that the evidence that Bayesian analysis is weak, and that sufficiently sophisticated models are unfalsifiable. See <a href="http://psycnet.apa.org/journals/bul/138/3/389/">Bowers &amp; Davis (2012)</a>, comment by <a href="http://cocosci.berkeley.edu/tom/papers/LabPublications/HowBayesiansGetBeliefs.pdf">Griffiths, Chater, Norris, Pouget (2012)</a>, reply by <a href="http://www.ncbi.nlm.nih.gov/pubmed/22545688">Bowers &amp; Davis (2012)</a>.</p>
</section>
</section>
<section id="example-1-estimating-the-probability-of-a-rare-event" class="level1">
<h1>Example 1: Estimating the probability of a rare event</h1>
<p>We are interested in the prevelance of a disease in a city. Let <span class="math inline">\(\theta
\in [0, 1]\)</span> be the fraction of infected individuals. We take a sample of 20 individuals and record the number of individuals <span class="math inline">\(y \in Y
= {0, 1, \dots, 20}\)</span> with the disease.</p>
<p>The sampling model is</p>
<p><span class="math display">\[
Y \mid \theta \sim \text{Binomial}(20, \theta),
\]</span></p>
<p>i.e.&nbsp;each individual has an independent <span class="math inline">\(\theta\)</span>% chance of having the disease.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">20</span>,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.10</span>, <span class="fl">0.20</span>), <span class="at">each =</span> <span class="dv">21</span>)),</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">probability =</span> <span class="fu">c</span>(<span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">20</span>, <span class="dv">20</span>, <span class="fl">0.05</span>), <span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">20</span>, <span class="dv">20</span>, <span class="fl">0.1</span>), <span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">20</span>, <span class="dv">20</span>, <span class="fl">0.2</span>))</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(d, <span class="fu">aes</span>(<span class="at">x =</span> y, <span class="at">y =</span> probability, <span class="at">fill =</span> theta)) <span class="sc">+</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>, <span class="at">position =</span> <span class="st">"dodge"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="1_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>For various theta, the probability of observing y infected individuals in the sample.</figcaption>
</figure>
</div>
</div>
</div>
<p>Imagine we believe <span class="math inline">\(\theta\)</span> is probably in the interval <span class="math inline">\([0.05, 0.20]\)</span>. For computational convenience, we will encode this prior as a Beta distribution</p>
<p><span class="math display">\[
\theta \sim \text{Beta}(2, 20)
\]</span></p>
<p>Conveniently, (we will prove this later), given <span class="math inline">\(Y \mid \theta \sim
\text{Binomial}(n, \theta)\)</span> and <span class="math inline">\(\theta \sim \text{Beta}(a, b)\)</span>,</p>
<p><span class="math display">\[
(\theta \mid Y = y) \sim \text{Beta}(a + y, b + n - y).
\]</span></p>
<p>For example, if we observe 0/20 individuals infected, <span class="math inline">\(\theta \mid {Y = 0} \sim
\text{Beta}(2, 40)\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.001</span>),</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">distribution =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">"prior"</span>, <span class="st">"posterior"</span>), <span class="at">each =</span> <span class="dv">1001</span>),</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">density =</span> <span class="fu">c</span>(<span class="fu">dbeta</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.001</span>), <span class="dv">2</span>, <span class="dv">20</span>), <span class="fu">dbeta</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.001</span>), <span class="dv">2</span>, <span class="dv">40</span>))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(d, <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> density, <span class="at">color =</span> distribution)) <span class="sc">+</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="1_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Prior and posterior distributions on theta after observing 0/20 infected individuals. Note the posterior is more tightly peaked around near-zero values.</figcaption>
</figure>
</div>
</div>
</div>
<p>Notice that Bayesian and frequentist approaches to parameter estimation differ:</p>
<p><span class="math display">\[\begin{align}
\theta_{ML} &amp;= \underset{\theta}{\text{argmax}} \; P(Y = 0 \mid \theta) = 0 \\
\theta_{MAP} &amp;= \underset{\theta}{\text{argmax}} \; P(\theta \mid Y = 0) = \text{Mode}(\theta \mid Y = 0) = 0.025
\end{align}\]</span></p>
<p>but also notice that the point estimate is NOT equal to the expectation of <span class="math inline">\(\theta\)</span>, since <span class="math inline">\(\mathbb{E}(\theta \mid Y = 0) = 0.048\)</span>. We will probably later determine when to use the expectation over the mode.</p>
<p>Finally, notice that we can do very intuitive statistical tests (e.g <span class="math inline">\(P(\theta &lt;
0.10 \mid Y = 0)\)</span>) by measuring the areas under our posterior distribution.</p>
<section id="sensitivity-analysis" class="level2">
<h2 class="anchored" data-anchor-id="sensitivity-analysis">Sensitivity analysis</h2>
<p>If we change the confidence in our prior, we get different posterior distributions. The more “peaked” our prior is, the less peaked the posterior will be given a <span class="math inline">\(Y = 0\)</span> result (and the less the Bayesian solution will approximate the ML estimate).</p>
<p>To quantify how changes in the prior beliefs affect our posterior estimates, we’ll do some calculations. Recall the expectation and variance of Beta distributions. If <span class="math inline">\(X \sim \text{Beta}(\alpha, \beta)\)</span>, then</p>
<p><span class="math display">\[\begin{align}
\mathbb{E}(X) &amp;= \frac{\alpha}{\alpha + \beta} \\
\text{Var}(X) &amp;= \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}.
\end{align}\]</span></p>
<p>Due to the properties of these functions we can parameterize the Beta distribution alternatively with</p>
<ul>
<li>Expectation: <span class="math inline">\(\theta_0 = \frac{\alpha}{\alpha + \beta}\)</span></li>
<li>Precision: <span class="math inline">\(w = a + b\)</span></li>
</ul>
<p>Since <span class="math inline">\((\theta \mid Y = y) \sim \text{Beta}(a + y, b + n - y)\)</span>,</p>
<p><span class="math display">\[\begin{align}
\mathbb{E}(\theta \mid Y = y) &amp;= \frac{a + y}{a + b + n} \\
&amp;= \frac{n}{w + n}\frac{y}{n} + \frac{w}{w + n}\theta_0.
\end{align}\]</span></p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># What is the expected value of theta after observing result y, given a Beta</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># prior parameterized by theta0 and w?</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="dv">20</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>exp.posterior <span class="ot">=</span> <span class="cf">function</span>(w, theta0, y) {</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  (N <span class="sc">/</span> (w <span class="sc">+</span> N)) <span class="sc">*</span> (y <span class="sc">/</span> N) <span class="sc">+</span> (w <span class="sc">/</span> (w <span class="sc">+</span> N)) <span class="sc">*</span> theta0</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>Theta0 <span class="ot">=</span> <span class="fu">rev</span>(<span class="fu">seq</span>(<span class="fl">0.0</span>, <span class="fl">0.5</span>, <span class="at">by =</span> <span class="fl">0.01</span>))</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>W <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">25</span>, <span class="at">by =</span> <span class="fl">0.5</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">outer</span>(Theta0, W, <span class="at">FUN =</span> <span class="cf">function</span>(w, theta0) <span class="fu">exp.posterior</span>(w, theta0, <span class="dv">0</span>))</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(d) <span class="ot">=</span> Theta0</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(d) <span class="ot">=</span> W</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">melt</span>(d)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by
the caller; using TRUE
Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by
the caller; using TRUE</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(df) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">'theta0'</span>, <span class="st">'w'</span>, <span class="st">'theta'</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> w, <span class="at">y =</span> theta0, <span class="at">z =</span> theta)) <span class="sc">+</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_contour</span>(<span class="fu">aes</span>(<span class="at">colour =</span> ..level..))</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(directlabels)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="fu">direct.label</span>(p, <span class="at">method =</span> <span class="st">'bottom.pieces'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: `aes_string()` was deprecated in ggplot2 3.0.0.
ℹ Please use tidy evaluation idioms with `aes()`.
ℹ See also `vignette("ggplot2-in-packages")` for more information.
ℹ The deprecated feature was likely used in the directlabels package.
  Please report the issue at &lt;https://github.com/tdhock/directlabels/issues&gt;.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: The dot-dot notation (`..level..`) was deprecated in ggplot2 3.4.0.
ℹ Please use `after_stat(level)` instead.</code></pre>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="1_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Expected value of the posterior for theta, for combinations of theta0 and w</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="comparison-to-non-bayesian-methods" class="level2">
<h2 class="anchored" data-anchor-id="comparison-to-non-bayesian-methods">Comparison to non-Bayesian methods</h2>
<p>When we use the frequentist maximum likelihood estimator, we get an estimated <span class="math inline">\(\theta_{ML} = 0\)</span>. Since our estimate is subject to sampling error, we commonly construct confidence intervals for these estimates.</p>
<p>The <strong>Wald interval</strong> is a commonly used confidene interval for a population proportion. However, it is not meant to be used for small sample sizes or situations in which the observed proportion is close to (or equals) 0 or 1, since in these cases the error of a binomially-distributed observation is not at all like the normal distribution For an observation <span class="math inline">\(Y = 20\)</span>, for example, the Wald CI is, regardless of level of confidence, just <span class="math inline">\(0\)</span>. We wouldn’t want to say with 99.999% confidence that the population mean is 0, given our small sample size.</p>
<p>The previous Bayesian estimate, however, works well for both small and large n.&nbsp; With small <span class="math inline">\(n\)</span>, the estimator allows us to encode prior beliefs about the true proportion. With <span class="math inline">\(w\)</span> and <span class="math inline">\(\theta_0\)</span> as before:</p>
<p><span class="math display">\[
\hat{\theta} = \mathbb{E}(\theta \mid Y = y) = \frac{n}{n + w}\frac{y}{n} + \frac{w}{n + w}\theta_0.
\]</span></p>
<p>Notice that this is kind of an average between the prior expectation <span class="math inline">\(\theta_0\)</span> and the observed proportion of the data <span class="math inline">\(\frac{y}{n}\)</span>, weighted by the amount of data <span class="math inline">\(n\)</span>. For large <span class="math inline">\(n\)</span>, <span class="math inline">\(\hat{\theta}\)</span> becomes dominated by the data, regardless of prior estimate and confidence.</p>
<p>Theoretical details on the properties of Bayesian estimators are covered later in Section 5.4.</p>
</section>
</section>
<section id="example-2-building-a-predictive-model" class="level1">
<h1>Example 2: Building a predictive model</h1>
<p>A brief synopsis of an example in Chapter 9, where we want to build a predictive model of diabeates progression from 64 variables such as age, sex and BMI.</p>
<p>We use a linear regression model, where <span class="math inline">\(Y_i\)</span> is the disease progression of subject <span class="math inline">\(i\)</span>, <span class="math inline">\(\mathbf{x}_i = (x_{i, 1}, \dots, x_{i, 64})\)</span> is a 64-dimensional vector. With unknown coefficient <span class="math inline">\(\beta_i\)</span> and the error term <span class="math inline">\(\sigma\)</span>, there are 65 unknown parameters.</p>
<p><span class="math display">\[
Y_i = \beta_1 x_{i, 1} + \beta_2 x_{i, 2} + \dots + \beta_{64} x_{i, 64} + \sigma \epsilon_i
\]</span></p>
<p>We define a sparse prior probability distribution, that most coefficients are equal to 0. (Spike and slab models?). This allows us to conduct a Bayesian form of <strong>feature selection</strong> where we evaluate the probability that a specific <span class="math inline">\(\beta_i \neq 0\)</span> given the data <span class="math inline">\(\mathbf{y} = (y_1, \dots, y_{342})\)</span> and predictors <span class="math inline">\(\mathbf{X} = (\mathbf{x}_1, \dots, \mathbf{x}_342)\)</span>.</p>
<section id="bayesian-regression-does-better-than-standard-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-regression-does-better-than-standard-linear-regression">Bayesian regression does better than standard linear regression</h2>
<p>The standard ordinary least squares (OLS) estimate of <span class="math inline">\(\mathbf{\beta}\)</span> does worse than the Bayesian method on the test set. This is due to overfitting, and OLS’s “inability to recognize when the sample size is too small to accurately estimate the regression coefficients.” Sparse linear regression models are key here, and the Bayesian sparsity prior performs well; the common lasso technique introduced by <a href="http://www.jstor.org/stable/2346178">Tibshirani (1996)</a> is also popular, but this in fact corresponds to Bayesian methods for a special case.</p>
</section>
</section>
<section id="next-steps" class="level1">
<h1>Next steps</h1>
<ul>
<li>Chapter 2: probability</li>
<li>Chapters 3, 4: One-parameter statistical models</li>
<li>Chapters 5, 6, 7: Bayesian inference with normal models</li>
<li>Chapters 8, 9, 10, 11, 12: Inference in more complicated statistical methods</li>
</ul>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/d-morrison\.github\.io\/hoff-bayesian-statistics\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> 'Chapter 1: Introduction and examples'</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Jesse Mu"</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "September 7, 2016"</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Setup --&gt;</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="in">```{r echo=FALSE, message=FALSE}</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span>opts_chunk<span class="sc">$</span><span class="fu">set</span>(<span class="at">fig.align =</span> <span class="st">'center'</span>, <span class="at">message =</span> <span class="cn">FALSE</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(knitr)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cowplot)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(reshape)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Begin writing --&gt;</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>*Bayesian inference*: the process of learning by updating prior probabilistic</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>beliefs in light of new information. Data analysis tools built on these foundations are known as *Bayesian methods*.</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Bayesian learning framework</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>We want to estimate a parameter $\theta \in \Theta$ from a dataset $y \in</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>\mathcal{Y}$.</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$p(\theta)$, defined for all $\theta \in \Theta$, is our prior distribution about the space of possible parameters</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bayesian methods require a **sampling model**: $P(y \mid </span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>\theta)$ describes the probability that of a specific dataset given a parameter.</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Note that later, this will be useful for prediction!</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>Then we wish to update our belief distribution about $\theta$ given. The</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>*posterior distribution* is defined as</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>p(\theta \mid y) = \frac{p(y \mid \theta) p(\theta)}{\int_{\Theta}p(y \mid \tilde{\theta}) p(\tilde{\theta}) \; d\tilde{\theta}}.</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>Note that the denominator is constant and doesn't need to be computed, since we can just normalize our posterior distribution such that $P(\theta \mid y)$ for all $\Theta$ sums up to 1. Thus we commonly write</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>p(\theta \mid y) \propto p(y \mid \theta) p(\theta).</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a><span class="fu">### versus frequentist learning</span></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>(from <span class="co">[</span><span class="ot">Resnik &amp; Hardisty, "Gibbs Sampling for the Uninitiated"</span><span class="co">](https://www.umiacs.umd.edu/~resnik/pubs/LAMP-TR-153.pdf)</span>)</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>The difference between Bayesian learning and frequentist learning is the</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>consideration of *prior beliefs* about parameters. In standard Maximum</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>Likelihood Estimation (MLE), we select the parameter that is most likely to have</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>generated the observed data:</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>\theta_{ML} = \underset{\theta}{\text{argmax}} \; p(y \mid \theta).</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>Using Bayesian Maximum A Posteriori Estimation, however, we select $\theta$ that</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>is most likely given the observed data. The difference is that our measure of</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>"likelihood given the data" is influenced by our prior beliefs about $\theta$, as in Equation 2:</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>\theta_{MAP} &amp;= \underset{\theta}{\text{argmax}} \; p(\theta \mid y) <span class="sc">\\</span></span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>  &amp;= \underset{\theta}{\text{argmax}} p(y \mid \theta) p(\theta).</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>Note that with an uninformative prior $\theta \sim \text{Uniform}$, the MAP</span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>estimate is the same as the ML estimate.</span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a><span class="fu"># Why Bayes?</span></span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a><span class="fu">## As an approach to probability and statistics</span></span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a>The debate between Bayesians and frequentists dives into some very philosophical</span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a>issues. Cox's theorem (1946, 1961) gives a formal proof for thinking about</span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a>probabilities using a Bayesian approach. I will likely want to look at an</span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a>explanation of these proofs later: http://ksvanhorn.com/bayes/Papers/rcox.pdf</span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>Outside of formal mathematical grounding, Bayesian methods have excellent </span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a>practical benefits as data analysis tools:</span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Even if prior probabilities are not exactly quantifiable, approximations of</span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a>$p(\theta)$ and $p(\theta \mid y)$ are still useful for analyzing how rational</span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a>learners would change beliefs</span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Bayesian methods can represent principled ways of doing analysis when there</span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a>are no alternative methods</span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a><span class="fu">## As models of cognition</span></span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a>An appeal of Bayesian learning is that it is also cognitively intuitive. Humans </span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a>have beliefs about the world, whose uncertainty can be expressed </span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a>probabilistically. Then, given data, these beliefs are rationally updated. There</span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a>is a rich tradition in modeling human cognition using Bayesian methods to great</span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a>success, with plenty of work done in showing how people's beliefs and knowledge</span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true" tabindex="-1"></a>about the world can be expressed probabilistically (e.g. <span class="co">[</span><span class="ot">Griffiths &amp; Tenenbaum, 2006</span><span class="co">](http://web.mit.edu/cocosci/Papers/Griffiths-Tenenbaum-PsychSci06.pdf)</span>)</span>
<span id="cb8-98"><a href="#cb8-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-99"><a href="#cb8-99" aria-hidden="true" tabindex="-1"></a>Bayesianism is not without its detractors, however. Some critics argue that the </span>
<span id="cb8-100"><a href="#cb8-100" aria-hidden="true" tabindex="-1"></a>evidence that Bayesian analysis is weak, and that sufficiently sophisticated</span>
<span id="cb8-101"><a href="#cb8-101" aria-hidden="true" tabindex="-1"></a>models are unfalsifiable. See <span class="co">[</span><span class="ot">Bowers &amp; Davis (2012)</span><span class="co">](http://psycnet.apa.org/journals/bul/138/3/389/)</span>, comment by <span class="co">[</span><span class="ot">Griffiths, Chater, Norris, Pouget (2012)</span><span class="co">](http://cocosci.berkeley.edu/tom/papers/LabPublications/HowBayesiansGetBeliefs.pdf)</span>, reply by <span class="co">[</span><span class="ot">Bowers &amp; Davis (2012)</span><span class="co">](http://www.ncbi.nlm.nih.gov/pubmed/22545688)</span>.</span>
<span id="cb8-102"><a href="#cb8-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-103"><a href="#cb8-103" aria-hidden="true" tabindex="-1"></a><span class="fu"># Example 1: Estimating the probability of a rare event</span></span>
<span id="cb8-104"><a href="#cb8-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-105"><a href="#cb8-105" aria-hidden="true" tabindex="-1"></a>We are interested in the prevelance of a disease in a city. Let $\theta</span>
<span id="cb8-106"><a href="#cb8-106" aria-hidden="true" tabindex="-1"></a>\in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$ be the fraction of infected individuals.</span>
<span id="cb8-107"><a href="#cb8-107" aria-hidden="true" tabindex="-1"></a>We take a sample of 20 individuals and record the number of individuals $y \in Y</span>
<span id="cb8-108"><a href="#cb8-108" aria-hidden="true" tabindex="-1"></a>= {0, 1, \dots, 20}$ with the disease.</span>
<span id="cb8-109"><a href="#cb8-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-110"><a href="#cb8-110" aria-hidden="true" tabindex="-1"></a>The sampling model is</span>
<span id="cb8-111"><a href="#cb8-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-112"><a href="#cb8-112" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb8-113"><a href="#cb8-113" aria-hidden="true" tabindex="-1"></a>Y \mid \theta \sim \text{Binomial}(20, \theta),</span>
<span id="cb8-114"><a href="#cb8-114" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb8-115"><a href="#cb8-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-116"><a href="#cb8-116" aria-hidden="true" tabindex="-1"></a>i.e. each individual has an independent $\theta$% chance of having the disease.</span>
<span id="cb8-117"><a href="#cb8-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-118"><a href="#cb8-118" aria-hidden="true" tabindex="-1"></a><span class="in">```{r fig.cap = "For various theta, the probability of observing y infected individuals in the sample."}</span></span>
<span id="cb8-119"><a href="#cb8-119" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb8-120"><a href="#cb8-120" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">20</span>,</span>
<span id="cb8-121"><a href="#cb8-121" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.10</span>, <span class="fl">0.20</span>), <span class="at">each =</span> <span class="dv">21</span>)),</span>
<span id="cb8-122"><a href="#cb8-122" aria-hidden="true" tabindex="-1"></a>  <span class="at">probability =</span> <span class="fu">c</span>(<span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">20</span>, <span class="dv">20</span>, <span class="fl">0.05</span>), <span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">20</span>, <span class="dv">20</span>, <span class="fl">0.1</span>), <span class="fu">dbinom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">20</span>, <span class="dv">20</span>, <span class="fl">0.2</span>))</span>
<span id="cb8-123"><a href="#cb8-123" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-124"><a href="#cb8-124" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(d, <span class="fu">aes</span>(<span class="at">x =</span> y, <span class="at">y =</span> probability, <span class="at">fill =</span> theta)) <span class="sc">+</span></span>
<span id="cb8-125"><a href="#cb8-125" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>, <span class="at">position =</span> <span class="st">"dodge"</span>)</span>
<span id="cb8-126"><a href="#cb8-126" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-127"><a href="#cb8-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-128"><a href="#cb8-128" aria-hidden="true" tabindex="-1"></a>Imagine we believe $\theta$ is probably in the interval $<span class="co">[</span><span class="ot">0.05, 0.20</span><span class="co">]</span>$. For</span>
<span id="cb8-129"><a href="#cb8-129" aria-hidden="true" tabindex="-1"></a>computational convenience, we will encode this prior as a Beta distribution</span>
<span id="cb8-130"><a href="#cb8-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-131"><a href="#cb8-131" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb8-132"><a href="#cb8-132" aria-hidden="true" tabindex="-1"></a>\theta \sim \text{Beta}(2, 20)</span>
<span id="cb8-133"><a href="#cb8-133" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb8-134"><a href="#cb8-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-135"><a href="#cb8-135" aria-hidden="true" tabindex="-1"></a>Conveniently, (we will prove this later), given $Y \mid \theta \sim</span>
<span id="cb8-136"><a href="#cb8-136" aria-hidden="true" tabindex="-1"></a>\text{Binomial}(n, \theta)$ and $\theta \sim \text{Beta}(a, b)$,</span>
<span id="cb8-137"><a href="#cb8-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-138"><a href="#cb8-138" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb8-139"><a href="#cb8-139" aria-hidden="true" tabindex="-1"></a>(\theta \mid Y = y) \sim \text{Beta}(a + y, b + n - y).</span>
<span id="cb8-140"><a href="#cb8-140" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb8-141"><a href="#cb8-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-142"><a href="#cb8-142" aria-hidden="true" tabindex="-1"></a>For example, if we observe 0/20 individuals infected, $\theta \mid {Y = 0} \sim</span>
<span id="cb8-143"><a href="#cb8-143" aria-hidden="true" tabindex="-1"></a>\text{Beta}(2, 40)$.</span>
<span id="cb8-144"><a href="#cb8-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-145"><a href="#cb8-145" aria-hidden="true" tabindex="-1"></a><span class="in">```{r fig.cap = "Prior and posterior distributions on theta after observing 0/20 infected individuals. Note the posterior is more tightly peaked around near-zero values."}</span></span>
<span id="cb8-146"><a href="#cb8-146" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb8-147"><a href="#cb8-147" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.001</span>),</span>
<span id="cb8-148"><a href="#cb8-148" aria-hidden="true" tabindex="-1"></a>  <span class="at">distribution =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">"prior"</span>, <span class="st">"posterior"</span>), <span class="at">each =</span> <span class="dv">1001</span>),</span>
<span id="cb8-149"><a href="#cb8-149" aria-hidden="true" tabindex="-1"></a>  <span class="at">density =</span> <span class="fu">c</span>(<span class="fu">dbeta</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.001</span>), <span class="dv">2</span>, <span class="dv">20</span>), <span class="fu">dbeta</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.001</span>), <span class="dv">2</span>, <span class="dv">40</span>))</span>
<span id="cb8-150"><a href="#cb8-150" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-151"><a href="#cb8-151" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(d, <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> density, <span class="at">color =</span> distribution)) <span class="sc">+</span></span>
<span id="cb8-152"><a href="#cb8-152" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>()</span>
<span id="cb8-153"><a href="#cb8-153" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-154"><a href="#cb8-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-155"><a href="#cb8-155" aria-hidden="true" tabindex="-1"></a>Notice that Bayesian and frequentist approaches to parameter estimation differ:</span>
<span id="cb8-156"><a href="#cb8-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-157"><a href="#cb8-157" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb8-158"><a href="#cb8-158" aria-hidden="true" tabindex="-1"></a>\theta_{ML} &amp;= \underset{\theta}{\text{argmax}} \; P(Y = 0 \mid \theta) = 0 <span class="sc">\\</span></span>
<span id="cb8-159"><a href="#cb8-159" aria-hidden="true" tabindex="-1"></a>\theta_{MAP} &amp;= \underset{\theta}{\text{argmax}} \; P(\theta \mid Y = 0) = \text{Mode}(\theta \mid Y = 0) = 0.025</span>
<span id="cb8-160"><a href="#cb8-160" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb8-161"><a href="#cb8-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-162"><a href="#cb8-162" aria-hidden="true" tabindex="-1"></a>but also notice that the point estimate is NOT equal to the expectation of </span>
<span id="cb8-163"><a href="#cb8-163" aria-hidden="true" tabindex="-1"></a>$\theta$, since $\mathbb{E}(\theta \mid Y = 0) = 0.048$. We will probably later </span>
<span id="cb8-164"><a href="#cb8-164" aria-hidden="true" tabindex="-1"></a>determine when to use the expectation over the mode.</span>
<span id="cb8-165"><a href="#cb8-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-166"><a href="#cb8-166" aria-hidden="true" tabindex="-1"></a>Finally, notice that we can do very intuitive statistical tests (e.g $P(\theta &lt;</span>
<span id="cb8-167"><a href="#cb8-167" aria-hidden="true" tabindex="-1"></a>0.10 \mid Y = 0)$) by measuring the areas under our posterior distribution.</span>
<span id="cb8-168"><a href="#cb8-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-169"><a href="#cb8-169" aria-hidden="true" tabindex="-1"></a><span class="fu">## Sensitivity analysis</span></span>
<span id="cb8-170"><a href="#cb8-170" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-171"><a href="#cb8-171" aria-hidden="true" tabindex="-1"></a>If we change the confidence in our prior, we get different posterior </span>
<span id="cb8-172"><a href="#cb8-172" aria-hidden="true" tabindex="-1"></a>distributions. The more "peaked" our prior is, the less peaked the posterior </span>
<span id="cb8-173"><a href="#cb8-173" aria-hidden="true" tabindex="-1"></a>will be given a $Y = 0$ result (and the less the Bayesian solution will </span>
<span id="cb8-174"><a href="#cb8-174" aria-hidden="true" tabindex="-1"></a>approximate the ML estimate).</span>
<span id="cb8-175"><a href="#cb8-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-176"><a href="#cb8-176" aria-hidden="true" tabindex="-1"></a>To quantify how changes in the prior beliefs affect our posterior estimates, </span>
<span id="cb8-177"><a href="#cb8-177" aria-hidden="true" tabindex="-1"></a>we'll do some calculations. Recall the expectation and variance of Beta </span>
<span id="cb8-178"><a href="#cb8-178" aria-hidden="true" tabindex="-1"></a>distributions. If $X \sim \text{Beta}(\alpha, \beta)$, then</span>
<span id="cb8-179"><a href="#cb8-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-180"><a href="#cb8-180" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb8-181"><a href="#cb8-181" aria-hidden="true" tabindex="-1"></a>\mathbb{E}(X) &amp;= \frac{\alpha}{\alpha + \beta} <span class="sc">\\</span></span>
<span id="cb8-182"><a href="#cb8-182" aria-hidden="true" tabindex="-1"></a>\text{Var}(X) &amp;= \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}.</span>
<span id="cb8-183"><a href="#cb8-183" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb8-184"><a href="#cb8-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-185"><a href="#cb8-185" aria-hidden="true" tabindex="-1"></a>Due to the properties of these functions we can parameterize the Beta distribution alternatively with</span>
<span id="cb8-186"><a href="#cb8-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-187"><a href="#cb8-187" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Expectation: $\theta_0 = \frac{\alpha}{\alpha + \beta}$</span>
<span id="cb8-188"><a href="#cb8-188" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Precision: $w = a + b$</span>
<span id="cb8-189"><a href="#cb8-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-190"><a href="#cb8-190" aria-hidden="true" tabindex="-1"></a>Since $(\theta \mid Y = y) \sim \text{Beta}(a + y, b + n - y)$,</span>
<span id="cb8-191"><a href="#cb8-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-192"><a href="#cb8-192" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb8-193"><a href="#cb8-193" aria-hidden="true" tabindex="-1"></a>\mathbb{E}(\theta \mid Y = y) &amp;= \frac{a + y}{a + b + n} <span class="sc">\\</span></span>
<span id="cb8-194"><a href="#cb8-194" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{n}{w + n}\frac{y}{n} + \frac{w}{w + n}\theta_0.</span>
<span id="cb8-195"><a href="#cb8-195" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb8-196"><a href="#cb8-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-197"><a href="#cb8-197" aria-hidden="true" tabindex="-1"></a><span class="in">```{r fig.cap = "Expected value of the posterior for theta, for combinations of theta0 and w"}</span></span>
<span id="cb8-198"><a href="#cb8-198" aria-hidden="true" tabindex="-1"></a><span class="co"># What is the expected value of theta after observing result y, given a Beta</span></span>
<span id="cb8-199"><a href="#cb8-199" aria-hidden="true" tabindex="-1"></a><span class="co"># prior parameterized by theta0 and w?</span></span>
<span id="cb8-200"><a href="#cb8-200" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="dv">20</span></span>
<span id="cb8-201"><a href="#cb8-201" aria-hidden="true" tabindex="-1"></a>exp.posterior <span class="ot">=</span> <span class="cf">function</span>(w, theta0, y) {</span>
<span id="cb8-202"><a href="#cb8-202" aria-hidden="true" tabindex="-1"></a>  (N <span class="sc">/</span> (w <span class="sc">+</span> N)) <span class="sc">*</span> (y <span class="sc">/</span> N) <span class="sc">+</span> (w <span class="sc">/</span> (w <span class="sc">+</span> N)) <span class="sc">*</span> theta0</span>
<span id="cb8-203"><a href="#cb8-203" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-204"><a href="#cb8-204" aria-hidden="true" tabindex="-1"></a>Theta0 <span class="ot">=</span> <span class="fu">rev</span>(<span class="fu">seq</span>(<span class="fl">0.0</span>, <span class="fl">0.5</span>, <span class="at">by =</span> <span class="fl">0.01</span>))</span>
<span id="cb8-205"><a href="#cb8-205" aria-hidden="true" tabindex="-1"></a>W <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">25</span>, <span class="at">by =</span> <span class="fl">0.5</span>)</span>
<span id="cb8-206"><a href="#cb8-206" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">outer</span>(Theta0, W, <span class="at">FUN =</span> <span class="cf">function</span>(w, theta0) <span class="fu">exp.posterior</span>(w, theta0, <span class="dv">0</span>))</span>
<span id="cb8-207"><a href="#cb8-207" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(d) <span class="ot">=</span> Theta0</span>
<span id="cb8-208"><a href="#cb8-208" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(d) <span class="ot">=</span> W</span>
<span id="cb8-209"><a href="#cb8-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-210"><a href="#cb8-210" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">melt</span>(d)</span>
<span id="cb8-211"><a href="#cb8-211" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(df) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">'theta0'</span>, <span class="st">'w'</span>, <span class="st">'theta'</span>)</span>
<span id="cb8-212"><a href="#cb8-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-213"><a href="#cb8-213" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> w, <span class="at">y =</span> theta0, <span class="at">z =</span> theta)) <span class="sc">+</span></span>
<span id="cb8-214"><a href="#cb8-214" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_contour</span>(<span class="fu">aes</span>(<span class="at">colour =</span> ..level..))</span>
<span id="cb8-215"><a href="#cb8-215" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(directlabels)</span>
<span id="cb8-216"><a href="#cb8-216" aria-hidden="true" tabindex="-1"></a><span class="fu">direct.label</span>(p, <span class="at">method =</span> <span class="st">'bottom.pieces'</span>)</span>
<span id="cb8-217"><a href="#cb8-217" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-218"><a href="#cb8-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-219"><a href="#cb8-219" aria-hidden="true" tabindex="-1"></a><span class="fu">## Comparison to non-Bayesian methods</span></span>
<span id="cb8-220"><a href="#cb8-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-221"><a href="#cb8-221" aria-hidden="true" tabindex="-1"></a>When we use the frequentist maximum likelihood estimator, we get an estimated </span>
<span id="cb8-222"><a href="#cb8-222" aria-hidden="true" tabindex="-1"></a>$\theta_{ML} = 0$. Since our estimate is subject to sampling error, we commonly </span>
<span id="cb8-223"><a href="#cb8-223" aria-hidden="true" tabindex="-1"></a>construct confidence intervals for these estimates.</span>
<span id="cb8-224"><a href="#cb8-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-225"><a href="#cb8-225" aria-hidden="true" tabindex="-1"></a>The **Wald interval** is a commonly used confidene interval for a population</span>
<span id="cb8-226"><a href="#cb8-226" aria-hidden="true" tabindex="-1"></a>proportion. However, it is not meant to be used for small sample sizes or</span>
<span id="cb8-227"><a href="#cb8-227" aria-hidden="true" tabindex="-1"></a>situations in which the observed proportion is close to (or equals) 0 or 1,</span>
<span id="cb8-228"><a href="#cb8-228" aria-hidden="true" tabindex="-1"></a>since in these cases the error of a binomially-distributed observation is not</span>
<span id="cb8-229"><a href="#cb8-229" aria-hidden="true" tabindex="-1"></a>at all like the normal distribution For an observation $Y = 20$, for example,</span>
<span id="cb8-230"><a href="#cb8-230" aria-hidden="true" tabindex="-1"></a>the Wald CI is, regardless of level of confidence, just $0$. We wouldn't want</span>
<span id="cb8-231"><a href="#cb8-231" aria-hidden="true" tabindex="-1"></a>to say with 99.999% confidence that the population mean is 0, given our small</span>
<span id="cb8-232"><a href="#cb8-232" aria-hidden="true" tabindex="-1"></a>sample size.</span>
<span id="cb8-233"><a href="#cb8-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-234"><a href="#cb8-234" aria-hidden="true" tabindex="-1"></a>The previous Bayesian estimate, however, works well for both small and large n. </span>
<span id="cb8-235"><a href="#cb8-235" aria-hidden="true" tabindex="-1"></a>With small $n$, the estimator allows us to encode prior beliefs about the true </span>
<span id="cb8-236"><a href="#cb8-236" aria-hidden="true" tabindex="-1"></a>proportion. With $w$ and $\theta_0$ as before:</span>
<span id="cb8-237"><a href="#cb8-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-238"><a href="#cb8-238" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb8-239"><a href="#cb8-239" aria-hidden="true" tabindex="-1"></a>\hat{\theta} = \mathbb{E}(\theta \mid Y = y) = \frac{n}{n + w}\frac{y}{n} + \frac{w}{n + w}\theta_0.</span>
<span id="cb8-240"><a href="#cb8-240" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb8-241"><a href="#cb8-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-242"><a href="#cb8-242" aria-hidden="true" tabindex="-1"></a>Notice that this is kind of an average between the prior expectation $\theta_0$</span>
<span id="cb8-243"><a href="#cb8-243" aria-hidden="true" tabindex="-1"></a>and the observed proportion of the data $\frac{y}{n}$, weighted by the amount</span>
<span id="cb8-244"><a href="#cb8-244" aria-hidden="true" tabindex="-1"></a>of data $n$. For large $n$, $\hat{\theta}$ becomes dominated by the data,</span>
<span id="cb8-245"><a href="#cb8-245" aria-hidden="true" tabindex="-1"></a>regardless of prior estimate and confidence.</span>
<span id="cb8-246"><a href="#cb8-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-247"><a href="#cb8-247" aria-hidden="true" tabindex="-1"></a>Theoretical details on the properties of Bayesian estimators are covered later</span>
<span id="cb8-248"><a href="#cb8-248" aria-hidden="true" tabindex="-1"></a>in Section 5.4.</span>
<span id="cb8-249"><a href="#cb8-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-250"><a href="#cb8-250" aria-hidden="true" tabindex="-1"></a><span class="fu"># Example 2: Building a predictive model</span></span>
<span id="cb8-251"><a href="#cb8-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-252"><a href="#cb8-252" aria-hidden="true" tabindex="-1"></a>A brief synopsis of an example in Chapter 9, where we want to build a </span>
<span id="cb8-253"><a href="#cb8-253" aria-hidden="true" tabindex="-1"></a>predictive model of diabeates progression from 64 variables such as age, sex and</span>
<span id="cb8-254"><a href="#cb8-254" aria-hidden="true" tabindex="-1"></a>BMI.</span>
<span id="cb8-255"><a href="#cb8-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-256"><a href="#cb8-256" aria-hidden="true" tabindex="-1"></a>We use a linear regression model, where $Y_i$ is the disease progression of </span>
<span id="cb8-257"><a href="#cb8-257" aria-hidden="true" tabindex="-1"></a>subject $i$, $\mathbf{x}_i = (x_{i, 1}, \dots, x_{i, 64})$ is a 64-dimensional </span>
<span id="cb8-258"><a href="#cb8-258" aria-hidden="true" tabindex="-1"></a>vector. With unknown coefficient $\beta_i$ and the error term $\sigma$, there </span>
<span id="cb8-259"><a href="#cb8-259" aria-hidden="true" tabindex="-1"></a>are 65 unknown parameters.</span>
<span id="cb8-260"><a href="#cb8-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-261"><a href="#cb8-261" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb8-262"><a href="#cb8-262" aria-hidden="true" tabindex="-1"></a>Y_i = \beta_1 x_{i, 1} + \beta_2 x_{i, 2} + \dots + \beta_{64} x_{i, 64} + \sigma \epsilon_i</span>
<span id="cb8-263"><a href="#cb8-263" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb8-264"><a href="#cb8-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-265"><a href="#cb8-265" aria-hidden="true" tabindex="-1"></a>We define a sparse prior probability distribution, that most coefficients are </span>
<span id="cb8-266"><a href="#cb8-266" aria-hidden="true" tabindex="-1"></a>equal to 0. (Spike and slab models?). This allows us to conduct a Bayesian form </span>
<span id="cb8-267"><a href="#cb8-267" aria-hidden="true" tabindex="-1"></a>of **feature selection** where we evaluate the probability that a specific </span>
<span id="cb8-268"><a href="#cb8-268" aria-hidden="true" tabindex="-1"></a>$\beta_i \neq 0$ given the data $\mathbf{y} = (y_1, \dots, y_{342})$ and </span>
<span id="cb8-269"><a href="#cb8-269" aria-hidden="true" tabindex="-1"></a>predictors $\mathbf{X} = (\mathbf{x}_1, \dots, \mathbf{x}_342)$.</span>
<span id="cb8-270"><a href="#cb8-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-271"><a href="#cb8-271" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bayesian regression does better than standard linear regression</span></span>
<span id="cb8-272"><a href="#cb8-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-273"><a href="#cb8-273" aria-hidden="true" tabindex="-1"></a>The standard ordinary least squares (OLS) estimate of $\mathbf{\beta}$ does </span>
<span id="cb8-274"><a href="#cb8-274" aria-hidden="true" tabindex="-1"></a>worse than the Bayesian method on the test set. This is due to overfitting, and </span>
<span id="cb8-275"><a href="#cb8-275" aria-hidden="true" tabindex="-1"></a>OLS's "inability to recognize when the sample size is too small to accurately </span>
<span id="cb8-276"><a href="#cb8-276" aria-hidden="true" tabindex="-1"></a>estimate the regression coefficients." Sparse linear regression models are key </span>
<span id="cb8-277"><a href="#cb8-277" aria-hidden="true" tabindex="-1"></a>here, and the Bayesian sparsity prior performs well; the common lasso technique </span>
<span id="cb8-278"><a href="#cb8-278" aria-hidden="true" tabindex="-1"></a>introduced by <span class="co">[</span><span class="ot">Tibshirani (1996)</span><span class="co">](http://www.jstor.org/stable/2346178)</span> is also</span>
<span id="cb8-279"><a href="#cb8-279" aria-hidden="true" tabindex="-1"></a>popular, but this in fact corresponds to Bayesian methods for a special case.</span>
<span id="cb8-280"><a href="#cb8-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-281"><a href="#cb8-281" aria-hidden="true" tabindex="-1"></a><span class="fu"># Next steps</span></span>
<span id="cb8-282"><a href="#cb8-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-283"><a href="#cb8-283" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Chapter 2: probability</span>
<span id="cb8-284"><a href="#cb8-284" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Chapters 3, 4: One-parameter statistical models</span>
<span id="cb8-285"><a href="#cb8-285" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Chapters 5, 6, 7: Bayesian inference with normal models</span>
<span id="cb8-286"><a href="#cb8-286" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Chapters 8, 9, 10, 11, 12: Inference in more complicated statistical methods</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>