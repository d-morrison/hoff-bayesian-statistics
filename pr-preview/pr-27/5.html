<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jesse Mu">
<meta name="dcterms.date" content="2016-10-07">

<title>Chapter 5: The Normal Model – Hoff Bayesian Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-68c8bffd90dad8f2b55c52d7b6410dc0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-26500bfc55c7891837a911d6d50a6255.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Hoff Bayesian Statistics</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-chapters" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Chapters</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-chapters">    
        <li>
    <a class="dropdown-item" href="./1.html">
 <span class="dropdown-text">Chapter 1: Introduction and examples</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./2.html">
 <span class="dropdown-text">Chapter 2: Belief, probability, and exchangeability</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./3.html">
 <span class="dropdown-text">Chapter 3: One-parameter models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./4.html">
 <span class="dropdown-text">Chapter 4: Monte Carlo approximation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./5.html">
 <span class="dropdown-text">Chapter 5: The Normal Model</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./6.html">
 <span class="dropdown-text">Chapter 6: Posterior approximation with the Gibbs sampler</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./7.html">
 <span class="dropdown-text">Chapter 7: The multivariate normal model</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./8.html">
 <span class="dropdown-text">Chapter 8: Group comparisons and hierarchical modeling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./9.html">
 <span class="dropdown-text">Chapter 9: Linear regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./10.html">
 <span class="dropdown-text">Chapter 10: Nonconjugate priors and Metropolis-Hastings algorithms</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./irm.html"> 
<span class="menu-text">Infinite Relational Model</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-normal-model" id="toc-the-normal-model" class="nav-link active" data-scroll-target="#the-normal-model">The normal model</a></li>
  <li><a href="#inference-for-the-mean-conditional-on-the-variance" id="toc-inference-for-the-mean-conditional-on-the-variance" class="nav-link" data-scroll-target="#inference-for-the-mean-conditional-on-the-variance">Inference for the mean, conditional on the variance</a>
  <ul class="collapse">
  <li><a href="#combining-information" id="toc-combining-information" class="nav-link" data-scroll-target="#combining-information">Combining information</a></li>
  <li><a href="#prediction" id="toc-prediction" class="nav-link" data-scroll-target="#prediction">Prediction</a></li>
  <li><a href="#example-midge-wing-data" id="toc-example-midge-wing-data" class="nav-link" data-scroll-target="#example-midge-wing-data">Example: Midge wing data</a></li>
  </ul></li>
  <li><a href="#joint-inference-for-the-mean-and-variance" id="toc-joint-inference-for-the-mean-and-variance" class="nav-link" data-scroll-target="#joint-inference-for-the-mean-and-variance">Joint inference for the mean and variance</a>
  <ul class="collapse">
  <li><a href="#posterior-inference" id="toc-posterior-inference" class="nav-link" data-scroll-target="#posterior-inference">Posterior inference</a></li>
  <li><a href="#summary-of-posterior-inference" id="toc-summary-of-posterior-inference" class="nav-link" data-scroll-target="#summary-of-posterior-inference">Summary of posterior inference</a></li>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example</a></li>
  <li><a href="#monte-carlo-sampling" id="toc-monte-carlo-sampling" class="nav-link" data-scroll-target="#monte-carlo-sampling">Monte carlo sampling</a></li>
  <li><a href="#improper-priors" id="toc-improper-priors" class="nav-link" data-scroll-target="#improper-priors">Improper priors</a></li>
  </ul></li>
  <li><a href="#bias-variance-and-mean-squared-error" id="toc-bias-variance-and-mean-squared-error" class="nav-link" data-scroll-target="#bias-variance-and-mean-squared-error">Bias, variance, and mean squared error</a></li>
  <li><a href="#prior-specification-based-on-expectations" id="toc-prior-specification-based-on-expectations" class="nav-link" data-scroll-target="#prior-specification-based-on-expectations">Prior specification based on expectations</a></li>
  <li><a href="#normal-model-for-non-normal-data" id="toc-normal-model-for-non-normal-data" class="nav-link" data-scroll-target="#normal-model-for-non-normal-data">Normal model for non-normal data</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a>
  <ul class="collapse">
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section">5.1</a>
  <ul class="collapse">
  <li><a href="#a" id="toc-a" class="nav-link" data-scroll-target="#a">a</a></li>
  <li><a href="#b" id="toc-b" class="nav-link" data-scroll-target="#b">b</a></li>
  <li><a href="#c" id="toc-c" class="nav-link" data-scroll-target="#c">c</a></li>
  <li><a href="#d" id="toc-d" class="nav-link" data-scroll-target="#d">d</a></li>
  </ul></li>
  <li><a href="#section-1" id="toc-section-1" class="nav-link" data-scroll-target="#section-1">5.2</a></li>
  <li><a href="#section-2" id="toc-section-2" class="nav-link" data-scroll-target="#section-2">5.3</a></li>
  <li><a href="#section-3" id="toc-section-3" class="nav-link" data-scroll-target="#section-3">5.4</a>
  <ul class="collapse">
  <li><a href="#a-1" id="toc-a-1" class="nav-link" data-scroll-target="#a-1">a</a></li>
  <li><a href="#b-1" id="toc-b-1" class="nav-link" data-scroll-target="#b-1">b</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Chapter 5: The Normal Model</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Jesse Mu </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 7, 2016</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<!-- Setup -->
<!-- Begin writing -->
<section id="the-normal-model" class="level1">
<h1>The normal model</h1>
<p>A normal variable <span class="math inline">\(Y\)</span> with mean <span class="math inline">\(\theta\)</span> and variance <span class="math inline">\(\sigma^2\)</span> (and thus standard deviation <span class="math inline">\(\sigma\)</span>) we denote</p>
<p><span class="math display">\[Y \sim \mathcal{N}(\theta, \sigma^2)\]</span></p>
<p>and <span class="math inline">\(Y\)</span> has PDF</p>
<p><span class="math display">\[
p(y) = \frac{1}{\sqrt{2\pi\sigma}} \text{exp}\left(-\frac{1}{2} \frac{(y - \theta)^2}{\sigma^2}\right)
\]</span></p>
<p>Due to the central limit theorem, the normal model is used all the time to model sample averages or values known to be the additive result of several random variables.</p>
<ul>
<li>It’s useful to remember the percentage of values lying within 1, 2, or 3 standard deviations of the mean when constructing priors: 68, 95, and 99.7%, respectively.</li>
</ul>
</section>
<section id="inference-for-the-mean-conditional-on-the-variance" class="level1">
<h1>Inference for the mean, conditional on the variance</h1>
<p>There are two parameters in the normal model. For simplicity, let’s first assume that the variance is known. Later we will show how we can perform inference jointly for the mean and variance, but this result will still be useful especially in Chapter 6, where Gibbs sampling requires full conditional distributions of individual parameters.</p>
<p>To do inference assuming <span class="math inline">\(\sigma^2\)</span> is known, we need to identify the sampling distribution and prior distribution, since we must calculate</p>
<p><span class="math display">\[\begin{align}
p(\theta \mid \sigma^2, y_1, \dots, y_n) \propto p(y_1, \dots, y_n \mid \theta, \sigma^2) \times p(\theta \mid \sigma^2)
\end{align}\]</span></p>
<p>For the sampling distribution,</p>
<p><span class="math display">\[\begin{align}
p(y_1, \dots, y_n \mid \theta, \sigma^2) &amp;= \prod_{i = 1}^n p(y_i \mid \theta, \sigma^2) \\
&amp;= \prod_{i = 1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \text{exp}\left(-\frac{1}{2} \frac{(y_i - \theta)^2}{\sigma^2} \right) \\
&amp;\propto \text{exp}\left[ -\frac{1}{2} \sum_{i = 1}^n \frac{(y_1 - \theta)^2}{\sigma^2} \right] \\
&amp;\propto \text{exp}\left[ -\frac{1}{2} \left( \frac{\sum y_i^2}{\sigma^2} - 2 \frac{\theta \sum y_i}{\sigma^2} + \frac{n\theta^2}{\sigma^2} \right) \right] \\
&amp;\propto \text{exp}\left[ -\frac{1}{2} \left( \frac{\sum y_i^2}{\sigma^2} - 2 \frac{\theta \sum y_i}{\sigma^2} + \frac{n\theta^2}{\sigma^2} \right) \right] \\
&amp;\propto \text{exp}\left[ -\frac{1}{2} \left( \frac{\sum y_i^2}{\sigma^2} - 2 \frac{\theta \sum y_i}{\sigma^2} \right) \right]
\end{align}\]</span></p>
<p>From this we know two things:</p>
<ol type="1">
<li><span class="math inline">\(p(\theta \mid \sigma^2, y_1, \dots, y_n) \propto p(y_1, \dots, y_n \mid
\theta, \sigma^2) \times p(\theta \mid \sigma^2)\)</span> depends only on <span class="math inline">\(\{\sum y_i^2,
\sum y_i\}\)</span>, so that is a sufficient statistic, as is <span class="math inline">\(\{\bar{y}, s^2\}\)</span> (from which <span class="math inline">\(\sum y_i^2, \sum y_i\)</span> are recoverable).</li>
<li>For <span class="math inline">\(p(\theta \mid \sigma^2)\)</span> to be conjugate, the posterior needs to have quadratic terms in the exponential function, i.e.&nbsp;<span class="math inline">\(\text{exp}(c_1 (\theta -
c_2)^2)\)</span></li>
</ol>
<p>In particular, 2 is a new strategy for trying to identify the conjugate prior family. Since the exponential terms in the sampling model and prior distribution must be combined to produce the same class of posterior distribution, we must pick a prior distribution <span class="math inline">\(\propto \text{exp}(c_1, (\theta - c_2)^2)\)</span>. Conveniently, normal distributions themselves have these terms. We can verify that the normal family is conjugate to the normal sampling model. Let <span class="math inline">\(\theta \mid \sigma^2 \sim \mathcal{N}(\mu_0, \tau_0^2)\)</span> (interpretations of the prior parameters comes later):</p>
<p><span class="math display">\[\begin{align}
p(\theta \mid \sigma^2, y_1, \dots, y_n) &amp;\propto p(y_1, \dots, y_n \mid \theta, \sigma^2) \times p(\theta \mid \sigma^2) \\
&amp;\propto \text{exp}\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \theta)^2 \right) \times \text{exp}\left(-\frac{1}{2 \tau_0^2} (\theta - \mu_0)^2\right) \\
&amp;= \text{exp}\left[ -\frac{1}{2} \left( \frac{1}{\tau_0^2}(\theta^2 - 2\theta\mu_0 + \mu_0^2) + \frac{1}{\sigma^2}(\sum y_i^2 - 2\theta y_i + n\theta^2) \right) \right] \\
&amp;= \text{exp}\left[ -\frac{1}{2} \left( \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}\right)\theta^2 + 2\left( \frac{\mu_0}{\tau_0^2} + \frac{\sum y_i}{\sigma^2} \right)\theta \right) \right] \\
\end{align}\]</span></p>
<p>To simplify this, let</p>
<ul>
<li><span class="math inline">\(a = \frac{1}{\tau_0^2} + \frac{n}{\sigma^2}\)</span></li>
<li><span class="math inline">\(b = \frac{\mu_0}{\tau_0^2} + \frac{\sum y_i}{\sigma^2}\)</span></li>
</ul>
<p>Then</p>
<p><span class="math display">\[\begin{align}
p(\theta \mid \sigma^2, y_1, \dots, y_n) &amp;\propto \text{exp}\left[ -\frac{1}{2} (a\theta^2 - 2b\theta) \right] \\
&amp;\propto \text{exp}\left[ -\frac{1}{2}(a\theta^2 - 2b\theta + b^2/a) + \frac{1}{2} b^2 / a \right] &amp; \text{Completing the square} \\
&amp;\propto \text{exp}\left[ -\frac{1}{2}(a\theta^2 - 2b\theta + b^2/a) \right] &amp;\text{Throw away constants} \\
&amp;\propto \text{exp}\left[ -\frac{1}{2}a(\theta^2 - 2b\theta / a + b^2/a^2) \right] \\
&amp;\propto \text{exp}\left[ -\frac{1}{2}a(\theta - b/a)^2 \right] \\
&amp;\propto \text{exp}\left[ -\frac{1}{2}\left( \frac{\theta - b/a}{1 / \sqrt{a}} \right)^2 \right] \\
&amp;= \text{dnorm}(\theta, b/a, 1/a).
\end{align}\]</span></p>
<p>Let these posterior parameters be <span class="math inline">\(\mu_n\)</span> and <span class="math inline">\(\tau_n^2\)</span>. In later chapters we will commonly follow this naming scheme: initial guesses of parameters are denoted <span class="math inline">\(\theta_0\)</span>, then the posterior parameters are denoted <span class="math inline">\(\theta_n\)</span>, i.e. the updated parameters after a sample of size <span class="math inline">\(n\)</span>.</p>
<p>Specifically,</p>
<p><span class="math display">\[\begin{align}
\mu_n &amp;= b/a = \frac{\frac{1}{\tau_0^2} \mu_0 + \frac{n}{\sigma^2}\bar{y}}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}} \\
\tau_n^2 &amp;= \frac{1}{a} = \frac{1}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}.
\end{align}\]</span></p>
<p>So <span class="math inline">\(\theta \mid \sigma^2, y_1, \dots, y_n \sim \mathcal{N}(\mu_n, \tau_n^2)\)</span>.</p>
<section id="combining-information" class="level2">
<h2 class="anchored" data-anchor-id="combining-information">Combining information</h2>
<p>Notice in the posterior parameters the frequency of <em>inverse variances</em> i.e. <span class="math inline">\(\frac{1}{\tau_0^2}, \frac{n}{\sigma^2}\)</span>. This hints at the importance of using <strong>precision</strong> to understand and parameterize our normal prior and posterior distributions. Specifically, it is much more concise to express the above parameters in terms of variance. Specifically, if we let <span class="math inline">\(\tilde{\tau_n^2} = 1 /
\tau_n^2\)</span>, i.e.&nbsp;the posterior <em>precision</em>, and similar tildes for the over variables,</p>
<p><span class="math display">\[\tilde{\tau_n^2} = \tilde{\tau_0^2} + n\tilde{\sigma^2}\]</span></p>
<p>So intuitively, our posterior precision is a combination of our prior belief in the precision of the true population mean of the data, plus the (assumed known) precision, where a larger sample size <span class="math inline">\(n\)</span> increases this precision.</p>
<p>Using precision, the fact that <span class="math inline">\(\mu_n\)</span> is a weighted average of prior and sample information becomes more clear. Notice</p>
<p><span class="math display">\[\begin{align}
\mu_n &amp;= \frac{\frac{1}{\tau_0^2}}{ \frac{1}{\tau_0^2} + \frac{n}{\sigma^2} } \mu_0 +
\frac{\frac{n}{\sigma^2}}{ \frac{1}{\tau_0^2} + \frac{n}{\sigma^2} } \bar{y} \\
&amp;= \frac{\tilde{\tau_0^2}}{\tilde{\tau_0^2} + n\tilde{\sigma^2}} \mu_0 + \frac{n\tilde{\sigma^2}}{\tilde{\tau_0^2} + n\tilde{\sigma^2}} \bar{y}
\end{align}\]</span></p>
<p>So the posterior mean is a weighted average of the prior expectation of the mean <span class="math inline">\(\mu_0\)</span> weighted by the precision of that mean <span class="math inline">\(\tilde{\tau_0^2}\)</span>, and the observed sample mean <span class="math inline">\(\bar{y}\)</span> weighted by our sample size <span class="math inline">\(n\)</span> and the (assumed known) precision <span class="math inline">\(\tilde{\sigma^2}\)</span>.</p>
<p>How do we select <span class="math inline">\(\tau_0^2\)</span>? One intuitive way to think about it (as we have done with one-parameter models) is by treating our prior parameters for <span class="math inline">\(\theta\)</span> as derived from <span class="math inline">\(\kappa_0\)</span> prior “observations” from the same (or similar) population that we are sampling from. Then <span class="math inline">\(\mu_0\)</span> is the average of these prior observations, and let <span class="math inline">\(\tau_0^2 = \sigma^2 / \kappa_0\)</span> be the variance of the <em>mean</em> of these prior observations. Then the posterior mean simplifies quite nicely to:</p>
<p><span class="math display">\[\begin{align}
\mu_n &amp;= \frac{\kappa_0}{\kappa_0 + n}\mu_0 + \frac{n}{\kappa_0 + n}\bar{y} \\
&amp;= \frac{\kappa_0}{\kappa_n}\mu_0 + \frac{n}{\kappa_n}\bar{y} &amp; \text{Let $\kappa_n = \kappa_0 + n$} \\
\end{align}\]</span></p>
<p>which is just a weighted average of <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\bar{y}\)</span> given the number of prior “observations” <span class="math inline">\(\kappa_0\)</span> and the sample size <span class="math inline">\(n\)</span>. We will take advantage of this when jointly estimating the mean and variance for the normal model. The idea is to first estimate the variance, then assume that variance <span class="math inline">\(\sigma^2\)</span> is known such that <span class="math inline">\(\tau_0^2 = \sigma^2 / \kappa_0\)</span> can be estimated.</p>
</section>
<section id="prediction" class="level2">
<h2 class="anchored" data-anchor-id="prediction">Prediction</h2>
<p>To obtain the posterior predictive distribution, instead of doing complex integration, we can use a trick.</p>
<p><span class="math inline">\(\tilde{Y}\)</span> is normally distributed with mean <span class="math inline">\(\theta\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. This is equivalent to saying</p>
<p><span class="math display">\[\tilde{Y} = \theta + \tilde{\epsilon}\]</span></p>
<p>where <span class="math inline">\(\theta \sim \mathcal{N}(\mu_n, \tau_n^2\)</span>, <span class="math inline">\(\tilde{\epsilon} \sim
\mathcal{N}(0, \sigma^2)\)</span>. So adding these normal distributions together gives</p>
<p><span class="math display">\[\tilde{Y} \mid \sigma^2, y_1, \dots, y_n \sim \mathcal{N}(\mu_n, \tau_n^2 + \sigma^2)\]</span></p>
</section>
<section id="example-midge-wing-data" class="level2">
<h2 class="anchored" data-anchor-id="example-midge-wing-data">Example: Midge wing data</h2>
<p>The wing lengths of 9 members of a species of “midge” are measured. We are interested in estimates of the mean wing length and variance. Prior information from other populations suggests that wing lengths are typically around 1.9mm, so our initial estimate <span class="math inline">\(\mu_0 = 1.9\)</span>. One way of assigning a prior estimate of the variance of the mean <span class="math inline">\(\tau_0^2\)</span> is to pick the spread of the prior such that all of its mass is above 0, since wing lengths can’t be negative. So we select <span class="math inline">\(\tau_0\)</span> such that 2 standard deviations from 1.9 &gt; 0: <span class="math inline">\(\tau_0 = 0.95\)</span>.</p>
<p>Our data are:</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">1.64</span>, <span class="fl">1.70</span>, <span class="fl">1.72</span>, <span class="fl">1.74</span>, <span class="fl">1.82</span>, <span class="fl">1.82</span>, <span class="fl">1.82</span>, <span class="fl">1.90</span>, <span class="fl">2.08</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(y)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.804444</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(y)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.01687778</code></pre>
</div>
</div>
<p>Since we are assuming for now that <span class="math inline">\(\sigma^2\)</span> is known, let’s use <span class="math inline">\(s^2 = \sigma^2\)</span>.</p>
<p>Now calculating <span class="math inline">\(\mu_n, \tau_n^2\)</span> is simply done by plugging in the relevant formulas:</p>
<p><span class="math display">\[\begin{align}
\mu_n &amp;= \frac{1.11 (1.9) + \frac{9}{0.017} 1.804}{1.11 + \frac{9}{0.017}} = 1.805 \\
\tau_n^2 &amp;= \frac{1}{1.11 + \frac{9}{0.017}} = 0.002
\end{align}\]</span></p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="fl">1.805</span>, <span class="fu">sqrt</span>(<span class="fl">0.002</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.717348 1.892652</code></pre>
</div>
</div>
</section>
</section>
<section id="joint-inference-for-the-mean-and-variance" class="level1">
<h1>Joint inference for the mean and variance</h1>
<p>For joint inference, we wish to compute the joint probability distribution of <span class="math inline">\((\theta, \sigma^2)\)</span> given the data, which proceeds much like before</p>
<p><span class="math display">\[
p(\theta, \sigma^2 \mid y_1, \dots, y_n) = \frac{p(y_1, \dots, y_n \mid \theta, \sigma^2) p(\theta, \sigma^2)}{p(y_1, \dots, y_n)}
\]</span></p>
<p>Notice that the only real difference in this two parameter case is the joint prior <span class="math inline">\(p(\theta, \sigma^2)\)</span>, which we should select a conjugate prior distribution for to simplify posterior calculation.</p>
<p>Notice that if we split up</p>
<p><span class="math display">\[p(\theta, \sigma^2) = p(\theta \mid \sigma^2) p(\sigma^2)\]</span> then, from the previous section, we already know that the normal distribution is a conjugate prior for the <span class="math inline">\(p(\theta \mid \sigma^2)\)</span>: <span class="math inline">\(\mathcal{N}(\mu_0, \tau_0^2)\)</span>. With this selection, we have</p>
<p><span class="math display">\[\begin{align}
p(\theta, \sigma^2) &amp;= p(\theta \mid \sigma^2) p(\sigma^2) \\
&amp;= \text{dnorm}(\theta, \mu_0, \tau_0) \times p(\sigma^2) \\
\end{align}\]</span></p>
<p>Now if we let <span class="math inline">\(\tau_0\)</span> depend on <span class="math inline">\(\sigma^2\)</span> as we explored in the previous section, this simplifies calculations. If <span class="math inline">\(\tau_0\)</span> is not proportional to <span class="math inline">\(\sigma^2\)</span>, then there is no good closed-form solution for a posterior distribution, which is a “semiconjugate” prior; see Chapter 6 for details. Specifically, if we let <span class="math inline">\(\tau_0^2 = \sigma^2 / \kappa_0 \implies \tau_0 = \sigma
/ \sqrt{\kappa_0}\)</span>, i.e.&nbsp;<span class="math inline">\(\tau_0^2\)</span> is the variance of the mean of a sample of size <span class="math inline">\(\kappa_0\)</span> from a population with variance <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[\begin{align}
p(\theta, \sigma^2) &amp;= \text{dnorm}(\theta, \mu_0, \tau = \sigma / \sqrt{\kappa_0}) \times p(\sigma^2)
\end{align}\]</span></p>
<p>Now we need to specify <span class="math inline">\(p(\sigma^2)\)</span>. We are told that the Gamma distribtion (with support on <span class="math inline">\((0, \infty)\)</span>) is not conjugate for the normal variance, but it <em>is</em> conjugate for the normal <em>precision</em> <span class="math inline">\(1 / \sigma^2\)</span>. It’s not mentioned how this is determined, but it probably has something to do with the ease of expressing posterior estimates in terms of precision in the previous section where <span class="math inline">\(\sigma^2\)</span> is known.</p>
<p>Let <span class="math inline">\(1 / \sigma^2 \sim \text{Gamma}(a, b)\)</span>. Like we have done previously, we would like to parameterize this distribution such that we can interpret choices of the parameters of the prior as sensibly conveying some prior expectation about the precision in this case. If we let</p>
<ul>
<li><span class="math inline">\(a = \nu_0 / 2\)</span></li>
<li><span class="math inline">\(b = a \sigma_0^2 = \frac{\nu_0}{2}\sigma^2_0\)</span></li>
</ul>
<p>We will show later that we can interpret <span class="math inline">\((\sigma^2_0, \nu_0)\)</span> as the sample variance and sample size of a set of prior observations.</p>
<p>If <span class="math inline">\(1 / \sigma^2 \sim \text{Gamma}(\nu_0 / 2, \sigma^2_0 \nu_0 / 2)\)</span>, then notice that <span class="math inline">\(\mathbb{E}(1 / \sigma^2) \neq 1 / \mathbb{E}(\sigma^2)\)</span> since the inverse is not a linear function. To calculate <span class="math inline">\(\mathbb{E}(\sigma^2)\)</span> requires something more complicated (law of the unconscious statistician?), or we can use the fact that <span class="math inline">\(\sigma^2 \sim \text{Inverse-Gamma}(\nu_0/2, \sigma^2_0 \nu_0 / 2)\)</span>, for which</p>
<ul>
<li><span class="math inline">\(\mathbb{E}(\sigma^2) = \frac{\sigma^2_0 \nu_0 / 2}{(\nu_0 / 2) - 1}\)</span></li>
<li><span class="math inline">\(\text{mode}(\sigma^2) = \frac{\sigma^2_0 \nu_0 / 2}{(\nu_0 / 2) + 1}\)</span></li>
<li><span class="math inline">\(\text{Var}(\sigma^2)\)</span> decreases as <span class="math inline">\(\nu_0\)</span> increases.</li>
</ul>
<p>From this you can already intuit how <span class="math inline">\(\nu_0\)</span> is a sample size, and <span class="math inline">\(\sigma^2_0\)</span> is an initial guess of the sample variance where the expectation of <span class="math inline">\(\sigma^2\)</span> more closely approaches <span class="math inline">\(\sigma^2_0\)</span> as <span class="math inline">\(\nu_0\)</span> increases.</p>
<section id="posterior-inference" class="level2">
<h2 class="anchored" data-anchor-id="posterior-inference">Posterior inference</h2>
<p>Now we have fully specified (1) our prior distributions:</p>
<p><span class="math display">\[\begin{align}
1 / \sigma^2 &amp;\sim \text{Gamma}(\nu_0 / 2, \sigma^2_0 \nu_0 / 2) \\
\theta \mid \sigma^2 &amp;\sim \mathcal{N}(\mu_0, \sigma^2 / \kappa_0), \\
\end{align}\]</span></p>
<p>and (2) our sampling model:</p>
<p><span class="math display">\[
Y_1, \dots, Y_n \mid \theta, \sigma^2 \sim \text{ i.i.d. } \mathcal{N}(\theta,
\sigma^2)
\]</span></p>
<p>Now we wish to calculate <span class="math inline">\(p(\theta, \sigma^2 \mid y_1, \dots, y_n)\)</span> which we can decompose to a product of marginal and conditional probabilities, just like the prior:</p>
<p><span class="math display">\[
p(\theta, \sigma^2 \mid y_1, \dots, y_n) =  p(\theta \mid \sigma^2, y_1, \dots, y_n) p(\sigma^2 \mid y_1, \dots, y_n)
\]</span></p>
<p>This is convenient because we already know <span class="math inline">\(p(\theta \mid \sigma^2, y_1, \dots, y_n)\)</span> from the one-parameter case:</p>
<p><span class="math display">\[\begin{align}
\theta \mid \sigma^2, y_1, \dots y_n \sim \mathcal{N}(\mu_n, \tau_n^2)
\end{align}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{align}
\mu_n &amp;= \frac{ \frac{1}{\tau_0^2} \mu_0 + \frac{n}{\sigma^2} \bar{y} }{ \frac{1}{\tau_0^2} + \frac{n}{\sigma^2} } \\
&amp;= \frac{ \frac{\kappa_0}{\sigma^2} \mu_0 + \frac{n}{\sigma^2} \bar{y} }{ \frac{\kappa_0}{\sigma^2} + \frac{n}{\sigma^2} } &amp; \text{Sub $\tau_0^2 = \sigma^2 / \kappa_0$} \\
&amp;= \frac{ \kappa_0 \mu_0 + n \bar{y} } { \kappa_0 + n } &amp; \text{$\sigma^2$s cancel} \\
\end{align}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align}
\tau_n^2 &amp;= \frac{1}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}} \\
&amp;= \frac{1}{\frac{\kappa_0}{\sigma^2} + \frac{n}{\sigma^2}} &amp; \text{Sub $\tau_0^2 = \sigma^2 / \kappa_0$} \\
&amp;= \frac{\sigma^2}{\kappa_0 + n}.
\end{align}\]</span></p>
<p>If we let <span class="math inline">\(\kappa_n = \kappa_0 + n\)</span> (remember we will interpret <span class="math inline">\(\kappa_0\)</span> as a prior sample size, and <span class="math inline">\(n\)</span> as this sample size), then we have</p>
<p><span class="math display">\[\begin{align}
\theta \mid \sigma^2, y_1, \dots y_n \sim \mathcal{N}(\mu_n, \sigma^2 / \kappa_n)
\end{align}\]</span></p>
<p>Where like before, <span class="math inline">\(\mu_n\)</span> is a weighted average of <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\bar{y}\)</span> dependent on the “prior” sample size <span class="math inline">\(\kappa_0\)</span> and the sample size <span class="math inline">\(n\)</span>, and <span class="math inline">\(\sigma^2 / \kappa_n\)</span> is the sampling variance of the sample mean given known variance <span class="math inline">\(\sigma^2\)</span> and our “sample size” <span class="math inline">\(\kappa_n\)</span>.</p>
<p>Recall our posterior distribution decomposition:</p>
<p><span class="math display">\[
p(\theta, \sigma^2 \mid y_1, \dots, y_n) =  p(\theta \mid \sigma^2, y_1, \dots, y_n) p(\sigma^2 \mid y_1, \dots, y_n)
\]</span></p>
<p>Once we calculate the second component, the posterior distribution of <span class="math inline">\(\sigma^2\)</span>, we will have fully specified the joint posterior distribution.</p>
<p><span class="math display">\[\begin{align}
p(\sigma^2 \mid y_1, \dots, y_n) &amp;\propto p(\sigma^2) p(y_1, \dots, y_n \mid \sigma^2) \\
&amp;= p(\sigma^2) \int p(y_1, \dots, y_n \mid \theta, \sigma^2) p(\theta \mid \sigma^2) \; d\theta \\
&amp;= \text{dinverse-gamma}(\sigma^2, \nu_0 / 2, \sigma_0^2 \nu_0 / 2) \times \\ &amp;\quad \int \left[ \left( \prod_{i = 1}^{n} p(y_i \mid \theta, \sigma^2) \right) \times \text{dnorm}(\theta, \mu_0, \sigma^2 / \kappa_0)  \right] \; d\theta \\
\end{align}\]</span></p>
<p>This integral is left as an exercise (Exercise 5.3). The result is that</p>
<p><span class="math display">\[\begin{align}
\sigma^2 \mid y_1, \dots, y_n &amp; \sim \text{Inverse-Gamma}(\nu_n / 2, \sigma_n^2 \nu_n / 2) \\
1 / \sigma^2 \mid y_1, \dots, y_n &amp;\sim \text{Gamma}(\nu_n / 2, \sigma_n^2 \nu_n / 2)
\end{align}\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\nu_n = \nu_0 + n\)</span>, like <span class="math inline">\(\kappa_n\)</span></li>
<li><span class="math inline">\(\sigma_n^2 = \frac{1}{\nu_n} \left[ \nu_0 \sigma_0^2 + (n - 1)s^2 + \frac{\kappa_0 n}{\kappa_n} (\bar{y} - \mu_0)^2 \right]\)</span></li>
</ul>
<p><span class="math inline">\(\nu_n\)</span> is fairly intuitive, it acts as a sample size which is the “prior sample size” of the variance plus the sample size <span class="math inline">\(n\)</span>. <span class="math inline">\(\sigma_n^2\)</span> is a bit harder to understand. There are three terms here. The first, <span class="math inline">\(\nu_0 \sigma_0^2\)</span>, can be thought of as a prior sum of squared observations from the sample mean (<span class="math inline">\(\nu_0\)</span> prior samples with variance <span class="math inline">\(\sigma_0^2\)</span>). Similarly, <span class="math inline">\((n - 1)s^2\)</span>, where <span class="math inline">\(s^2 =
\sum_{i = 1}^n (y_i - \bar{y})^2 / (n - 1)\)</span>, is literally the sum of squared (actually observed) observations from the sample mean. Lastly, the third term increases the posterior variance if the observed sample mean <span class="math inline">\((\bar{y})\)</span> is <em>far</em> away from the expected prior mean <span class="math inline">\(\mu_0\)</span>, since this would suggest higher variance. All three “sum of squares-ish” terms are combined, then divided by the total number of “observations” <span class="math inline">\(\nu_n = n + \nu_0\)</span>, as commonly done to estimate variance from a sample.</p>
</section>
<section id="summary-of-posterior-inference" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-posterior-inference">Summary of posterior inference</h2>
<p>This is a lot to handle, since there are a lot of moving parts. In sum, for inference with the normal model, there are four prior parameters to specify:</p>
<ul>
<li><span class="math inline">\(\sigma_0^2\)</span>, an initial estimate for the variance;</li>
<li><span class="math inline">\(\nu_0\)</span>, a “prior sample size” from which the initial estimate of the <em>variance</em> is observed;</li>
<li><span class="math inline">\(\mu_0\)</span>, an initial estimate for the population mean;</li>
<li><span class="math inline">\(\kappa_0\)</span>, a “prior sample size” from which the initial estimate of the <em>mean</em> is observed</li>
</ul>
<p>Then we have</p>
<ul>
<li><span class="math inline">\(1 / \sigma^2 \sim \text{Gamma}(\nu_0 / 2, \sigma^2_0 \nu_0 / 2)\)</span></li>
<li><span class="math inline">\(\implies \mathbb{E}(\sigma^2) = \sigma^2_0 \frac{\nu_0 / 2}{\nu_0 / 2 - 1}\)</span> (use expectation of inverse gamma)</li>
<li><span class="math inline">\(\theta \mid \sigma^2 \sim \mathcal{N}(\mu_0, \sigma^2 / \kappa_0)\)</span></li>
<li><span class="math inline">\(\implies \mathbb{E}(\theta) = \mu_0\)</span></li>
</ul>
<p>The updated parameters are</p>
<ul>
<li><span class="math inline">\(\nu_n = \nu_0 + n\)</span></li>
<li><span class="math inline">\(\sigma_n^2 = \frac{1}{\nu_n} \left[ \nu_0 \sigma_0^2 + (n - 1)s^2 + \frac{\kappa_0 n}{\kappa_n} (\bar{y} - \mu_0)^2 \right]\)</span></li>
<li><span class="math inline">\(\mu_n = \frac{\kappa_0 \mu_0 + n\bar{y}}{\kappa_n}\)</span></li>
<li><span class="math inline">\(\kappa_n = \kappa_0 + n\)</span></li>
</ul>
<p>So that the posterior is finally</p>
<ul>
<li><span class="math inline">\(1 / \sigma^2 \mid y_1, \dots, y_n \sim \text{Gamma}(\nu_n / 2, \sigma^2_n \nu_n / 2)\)</span></li>
<li>Where <span class="math inline">\(\mathbb{E}(\sigma^2 \mid y_1, \dots, y_n) = \frac{\sigma^2_n \nu_n}{2 (\nu_n / 2 - 1)}\)</span> (using the expectation of the inverse gamma)</li>
<li><span class="math inline">\(\theta \mid \sigma^2, y_1, \dots, y_n \sim \mathcal{N}(\mu_n, \sigma^2 / \kappa_n)\)</span></li>
<li>Where <span class="math inline">\(\mathbb{E}(\theta \mid y_1, \dots, y_n, \sigma^2) = \mu_n = \frac{\kappa_0 \mu_0 + n \bar{y}}{\kappa_n}\)</span></li>
</ul>
<p>Note how the prior sample sizes for the variance and the mean are decoupled because they update differently. However, it’s common to set <span class="math inline">\(\nu_0 =
\kappa_0\)</span>.</p>
</section>
<section id="example" class="level2">
<h2 class="anchored" data-anchor-id="example">Example</h2>
<p>Back to midge wing length, although this time, we are leaving our estimate of the variance of the population free as well.</p>
<p>From other populations, say that we weakly believe that our prior estimates of the population mean and variance are <span class="math inline">\(\mu_0 = 1.9\)</span> and <span class="math inline">\(\sigma_0^2 = 0.01\)</span>, respectively. Since this is a weak belief we will pick <span class="math inline">\(\kappa_0 = \nu_0 = 1\)</span>. Now our prior distributions are</p>
<ul>
<li><span class="math inline">\(1 / \sigma^2 \sim \text{Gamma}(0.5, 0.005)\)</span></li>
<li><span class="math inline">\(\theta \mid \sigma^2 \sim \mathcal{N}(1.9, \sigma^2 / \kappa_0)\)</span></li>
</ul>
<p>Recall that our data are</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">1.64</span>, <span class="fl">1.70</span>, <span class="fl">1.72</span>, <span class="fl">1.74</span>, <span class="fl">1.82</span>, <span class="fl">1.82</span>, <span class="fl">1.82</span>, <span class="fl">1.90</span>, <span class="fl">2.08</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">length</span>(y)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>ybar <span class="ot">=</span> <span class="fu">mean</span>(y)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>s2 <span class="ot">=</span> <span class="fu">var</span>(y)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Now we calculate the parameters of the posterior distributions</p>
<ul>
<li><span class="math inline">\(\kappa_n = \kappa_0 + n = 1 + 9 = 10\)</span></li>
<li><span class="math inline">\(\nu_n = \nu_0 + n = 1 + 9 =10\)</span></li>
<li><span class="math inline">\(\mu_n = \frac{\kappa_0 \mu_0 + n\bar{y}}{\kappa_n} = \frac{1.9 + 9(1.804)}{10} = 1.814\)</span></li>
<li><span class="math display">\[\begin{align}
\sigma_n^2 &amp;= \frac{1}{\nu_n} \left[ \nu_0 \sigma_0^2 + (n - 1)s^2 + \frac{\kappa_0 n}{\kappa_n} (\bar{y} - \mu_0)^2 \right] \\
&amp;= \frac{1}{10} \left[ 0.01 + 8(0.168) + \frac{9}{10} (1.804 - 1.9)^2 \right] \\
&amp;= \frac{1}{10} \left[ 0.01 + 0.135 + 0.008 \right] \\
&amp;= 0.015
\end{align}\]</span></li>
</ul>
<p>So our joint posterior distribution is</p>
<p><span class="math display">\[\begin{align}
1 / \sigma^2 \mid y_1, \dots, y_n &amp;\sim \text{Gamma}(10/2 = 5, 10(0.015 / 2) = 0.075) \\
\theta \mid \sigma^2, y_1, \dots, y_n &amp;\sim \mathcal{N}(1.814, \sigma^2 / 10)
\end{align}\]</span></p>
<p>Now we can plot the posterior distribution for various values of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="fl">1.9</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>kappa0 <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>s20 <span class="ot">=</span> <span class="fl">0.01</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>nu0 <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>kappan <span class="ot">=</span> kappa0 <span class="sc">+</span> n</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>nun <span class="ot">=</span> nu0 <span class="sc">+</span> n</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>mun <span class="ot">=</span> (kappa0 <span class="sc">*</span> mu0 <span class="sc">+</span> n <span class="sc">*</span> ybar) <span class="sc">/</span> kappan</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>s2n <span class="ot">=</span> (<span class="dv">1</span> <span class="sc">/</span> nun) <span class="sc">*</span> (nu0 <span class="sc">*</span> s20 <span class="sc">+</span> (n <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> s2 <span class="sc">+</span> (kappa0 <span class="sc">*</span> n <span class="sc">/</span> kappan) <span class="sc">*</span> (ybar <span class="sc">-</span> mu0)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>Theta <span class="ot">=</span> <span class="fu">seq</span>(<span class="fl">1.6</span>, <span class="fl">2.0</span>, <span class="at">by =</span> <span class="fl">0.005</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>Sigma2 <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">0.04</span>, <span class="at">by =</span> <span class="fl">0.0001</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(invgamma)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>post.func <span class="ot">=</span> <span class="cf">function</span>(theta, sigma2) {</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dnorm</span>(theta, mun, <span class="fu">sqrt</span>(sigma2 <span class="sc">/</span> kappan)) <span class="sc">*</span> <span class="fu">dinvgamma</span>(sigma2, nun <span class="sc">/</span> <span class="dv">2</span>, s2n <span class="sc">*</span> nun <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">outer</span>(Theta, Sigma2, post.func)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(d) <span class="ot">=</span> Theta</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(d) <span class="ot">=</span> Sigma2</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">melt</span>(d)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by
the caller; using TRUE
Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by
the caller; using TRUE</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(df) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">'theta'</span>, <span class="st">'sigma2'</span>, <span class="st">'density'</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> sigma2, <span class="at">z =</span> density)) <span class="sc">+</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_contour</span>(<span class="fu">aes</span>(<span class="at">color =</span> ..level..)) <span class="sc">+</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">guides</span>(<span class="at">color =</span> <span class="cn">FALSE</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use "none" instead as
of ggplot2 3.3.4.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: The dot-dot notation (`..level..`) was deprecated in ggplot2 3.4.0.
ℹ Please use `after_stat(level)` instead.</code></pre>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="5_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="monte-carlo-sampling" class="level2">
<h2 class="anchored" data-anchor-id="monte-carlo-sampling">Monte carlo sampling</h2>
<p>We can simulate values from the posterior by first sampling <span class="math inline">\(\sigma^{2(n)}\)</span> from its inverse gamma distribution, and <span class="math inline">\(\theta^{(n)}\)</span> from its normal distribution conditioned on <span class="math inline">\(\sigma^{2(n)}\)</span>. Then <span class="math inline">\(\{\theta^{(n)}, \sigma^{2(n)}\}\)</span> represent samples from the joint distribution <span class="math inline">\(p(\theta, \sigma^2 \mid y_1, \dots, y_n)\)</span>, and either set of values by themselves represents samples from the full marginal distribution. This is intuitive for <span class="math inline">\(\sigma^{2(n)}\)</span> but less so for <span class="math inline">\(\theta^{(n)}\)</span>. The key is to notice that, although <span class="math inline">\(\theta^{(n)}\)</span> is sampled conditioned on <span class="math inline">\(\sigma^{2(n)}\)</span>, multiple <span class="math inline">\(\theta^{(n)}\)</span> samples are conditioned on multiple <em>different</em> <span class="math inline">\(\sigma^{2(n)}\)</span>s, so the <span class="math inline">\(\theta^{(n)}\)</span> do indeed represent samples from the marginal distribution.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>s2.mc <span class="ot">=</span> <span class="fu">rinvgamma</span>(<span class="dv">10000</span>, nun <span class="sc">/</span> <span class="dv">2</span>, s2n <span class="sc">*</span> nun <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>theta.mc <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>, mun, <span class="fu">sqrt</span>(s2.mc <span class="sc">/</span> kappan)) <span class="co"># Accepts a vector of parameters</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(theta.mc)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.813692</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(theta.mc, <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>    2.5%    97.5% 
1.725999 1.901429 </code></pre>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">sigma2 =</span> s2.mc, <span class="at">theta =</span> theta.mc)) <span class="sc">+</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> sigma2), <span class="at">alpha =</span> <span class="fl">0.1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="5_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="improper-priors" class="level2">
<h2 class="anchored" data-anchor-id="improper-priors">Improper priors</h2>
<p>What if we want to use <em>no</em> prior information? See what happens to our posterior distribution <span class="math inline">\(\kappa_0, \nu_0 \rightarrow 0\)</span>. Using the formula above,</p>
<ul>
<li><span class="math inline">\(\sigma_n^2 \rightarrow \frac{n - 1}{n}s^2\)</span></li>
<li><span class="math inline">\(\mu_n \rightarrow \bar{y}\)</span>.</li>
</ul>
<p>Then, the “posterior” (plugging in <span class="math inline">\(\kappa_0 = \nu_0 = 0\)</span> and the posterior parameters <span class="math inline">\(\sigma_n^2, \mu_n\)</span> and simplifying) would be</p>
<p><span class="math display">\[\begin{align}
1 / \sigma^2 \mid y_1, \dots, y_n &amp;\sim \text{Gamma}(\frac{n}{2}, \frac{1}{n} \frac{n}{2}\sum (y_i - y)^2)$ \\
\theta \mid \sigma^2, y_1, \dots, y_n &amp;\sim \mathcal{N}(\bar{y}, \frac{\sigma^2}{n})
\end{align}\]</span></p>
<!-- and the *full conditional* distribution of $1 / \sigma^2$ is -->
<!-- \begin{align} -->
<!-- p(1 / \sigma^2 \mid \theta, y_1, \dots, y_n) &= \int p(1 / sigma^2 \mid) -->
<!-- \end{align} -->
<p>With <a href="http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/nuisance_parameters.pdf">“significant algebra”</a>, you can show that inference this way results in</p>
<p><span class="math display">\[\frac{\theta - \bar{y}}{s / \sqrt{n}} \mid y_1, \dots, y_n \sim t_{n - 1}\]</span></p>
<p>i.e.&nbsp;a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n - 1\)</span> degrees of freedom. This is similar to the sampling distribution of <span class="math inline">\(t\)</span> statistic:</p>
<p><span class="math display">\[\frac{\bar{Y} - \theta}{s / \sqrt{n}} \mid \theta \sim t_{n - 1}\]</span></p>
<p>but like the Bayesian vs Frequentist confidence intervals discussion in Chapter 3, they are philosophically different. The first describes uncertainty about the true mean conditional on the data, while the second describes uncertainty about the observed sample mean given the true population mean.</p>
</section>
</section>
<section id="bias-variance-and-mean-squared-error" class="level1">
<h1>Bias, variance, and mean squared error</h1>
<p>Now we are diving into the properties of estimators for posterior parameters.</p>
<blockquote class="blockquote">
<p>A <em>point estimator</em> of an unknown parameter <span class="math inline">\(\theta\)</span> is a function that converts your data into a single element of the parameter space <span class="math inline">\(\Theta\)</span>. Good point estimators should hopefully approximate (and <em>reliably</em> approximate) the true value of <span class="math inline">\(\theta\)</span>; we can formalize these properties as the bias and mean squared error of estimators.</p>
</blockquote>
<p>In Bayesian analysis, point estimators are usually functions of the posterior distribution of the parameter, such as the expectation.</p>
<p>The point estimator for the posterior of our normal sampling model and a normal prior is (call it <span class="math inline">\(\hat{\theta_b}\)</span>)</p>
<p><span class="math display">\[\begin{align}
\hat{\theta_b} &amp;= \frac{\kappa_0 \mu_0 + n\bar{y}}{\kappa_n} \\
&amp;= \frac{\kappa_0}{\kappa_0 + n} \mu_0 + \frac{n}{\kappa_0 + n}\bar{y}
&amp;= w \bar{y} + (1 - w) \mu_0
\end{align}\]</span></p>
<p>where <span class="math inline">\(w = \frac{n}{\kappa_0 + n}\)</span>.</p>
<blockquote class="blockquote">
<p>The <strong>Bias</strong> of an estimator <span class="math inline">\(\hat{\theta}\)</span> is <span class="math display">\[\text{Bias}(\hat{\theta}) = \mathbb{E}(\hat{\theta}) - \theta\]</span>. If <span class="math inline">\(\text{Bias}(\hat{\theta}) = 0\)</span> we say that <span class="math inline">\(\hat{\theta}\)</span> is an <em>unbiased</em> estimator; otherwise we say it is biased.</p>
</blockquote>
<p>Consider the Bayesian <span class="math inline">\(\hat{\theta_b}\)</span> above versus the standard maximum likelihood estimator, <span class="math inline">\(\hat{\theta_e} = \bar{y}\)</span>.</p>
<p>Notice that</p>
<ul>
<li><span class="math inline">\(\text{Bias}(\hat{\theta_e}) = \mathbb{E}(\hat{\theta_e}) - \theta = 0\)</span>, so <span class="math inline">\(\hat{\theta_b}\)</span> is unbiased;</li>
<li><span class="math inline">\(\text{Bias}(\hat{\theta_b}) = \mathbb{E}(\hat{\theta_b}) - \theta = w\theta + (1 - w)\mu_0 - \theta\)</span>. Notice that the first two terms add up to <span class="math inline">\(\theta\)</span> only if <span class="math inline">\(\mu_0 = \theta\)</span>. For all <span class="math inline">\(\mu \neq \theta\)</span>, <span class="math inline">\(\hat{\theta_b}\)</span> is biased!</li>
</ul>
<p>A biased estimator seems undesirable, but can actually be useful in this setting. Imagine “biasing” the estimator <em>towards the true mean</em> to obtain a more accurate estimate. Thus it is useful to recall using the Mean Squared Error as another measure of estimator performance, which measures how close an estimator <span class="math inline">\(\hat{\theta}\)</span> will be to the true population parameter <span class="math inline">\(\theta\)</span>, on average:</p>
<blockquote class="blockquote">
<p>The <strong>Mean Squared Error</strong> (MSE) of an estimator <span class="math inline">\(\hat{\theta}\)</span> is <span class="math display">\[\text{MSE}(\hat{\theta}) = \text{Var}(\hat{\theta}) + \text{Bias}^2(\hat{\theta})\]</span></p>
</blockquote>
<p>Let’s compare the MSE of <span class="math inline">\(\hat{\theta_b}\)</span> and <span class="math inline">\(\hat{\theta_e}\)</span>:</p>
<ul>
<li><span class="math inline">\(\text{Var}(\hat{\theta_e}) = \text{Var}(\bar{y}) = \frac{\sigma^2}{n}\)</span></li>
<li><span class="math inline">\(\text{Var}(\hat{\theta_b}) = \text{Var}(w \theta_0 + (1 - w)\mu_0) = \text{Var}(w \theta) = w^2 \frac{\sigma^2}{n}\)</span></li>
</ul>
<p>So</p>
<p><span class="math display">\[\text{MSE}(\hat{\theta_e}) = \text{Var}(\hat{\theta_e}) + \text{Bias}^2(\hat{\theta_e}) = \frac{\sigma^2}{n} + 0\]</span> <span class="math display">\[\begin{align}
\text{MSE}(\hat{\theta_b}) &amp;= \text{Var}(\hat{\theta_b}) + \text{Bias}^2(\hat{\theta_b}) \\
&amp;= w^2 \frac{\sigma^2}{n} + \left[ w\theta + (1 - w)\mu_0 - \theta \right]^2 \\
&amp;= w^2 \frac{\sigma^2}{n} + \left[ (1 - w)\mu_0 - (1 - w)\theta \right]^2 \\
&amp;= w^2 \frac{\sigma^2}{n} + (1 - w)^2(\mu_0 - \theta)^2 \\
\end{align}\]</span></p>
<p>Notice that <span class="math display">\[\begin{align}
&amp; \text{MSE}(\hat{\theta_b}) &lt; \text{MSE}(\hat{\theta_e}) \\
\implies&amp; w^2 \frac{\sigma^2}{n} + (1 - w)^2 (\mu_0 - \theta)^2 &lt; \frac{\sigma^2}{n} \\
\implies&amp; (1-w)^2(\mu_0 - \theta)^2 &lt; (1 - w^2) \frac{\sigma^2}{n} \\
\implies&amp; (\mu_0 - \theta)^2 &lt; \frac{\sigma^2}{n} \frac{1-w^2}{(1 - w)^2} \\
\implies&amp; (\mu_0 - \theta)^2 &lt; \frac{\sigma^2}{n} \frac{(1 - w)(1 + w)}{(1 - w)^2} &amp; \text{Difference of squares} \\
\implies&amp; (\mu_0 - \theta)^2 &lt; \frac{\sigma^2}{n} \frac{1 + w}{1 - w} \\
\implies&amp; (\mu_0 - \theta)^2 &lt; \frac{\sigma^2}{n} \frac{1 + \frac{n}{\kappa_0 + n}}{1 - \frac{n}{\kappa_0 + n}} &amp; \text{Def. of $w$} \\
\implies&amp; (\mu_0 - \theta)^2 &lt; \frac{\sigma^2}{n} \frac{\frac{\kappa_0 + 2n}{\kappa_0 + n}}{\frac{\kappa_0}{\kappa_0 + n}} \\
\implies&amp; (\mu_0 - \theta)^2 &lt; \frac{\sigma^2}{n} \frac{\kappa_0 + 2n}{\kappa_0 + n}\frac{\kappa_0 + n}{\kappa_0} \\
\implies&amp; (\mu_0 - \theta)^2 &lt; \frac{\sigma^2}{n} \frac{\kappa_0 + 2n}{\kappa_0} \\
\implies&amp; (\mu_0 - \theta)^2 &lt; \sigma^2 \frac{\kappa_0 + 2n}{n\kappa_0} \\
\implies&amp; (\mu_0 - \theta)^2 &lt; \sigma^2 \left( \frac{\kappa_0}{n\kappa_0} + \frac{2n}{n\kappa_0} \right) \\
\implies&amp; (\mu_0 - \theta)^2 &lt; \sigma^2 \left( \frac{1}{n} + \frac{2}{\kappa_0} \right) \\
\end{align}\]</span></p>
<p>So the Bayesian estimator has lower mean squared error than the ML estimate as long as values of <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\kappa_0\)</span> are picked such that this inequality holds - intutively, if your “guess” about the prior is not far from the truth.</p>
</section>
<section id="prior-specification-based-on-expectations" class="level1">
<h1>Prior specification based on expectations</h1>
<p>The normal model can be shown to be an 2-dimensional exponential model. A <span class="math inline">\(p\)</span>-dimensional exponential family model has densities of the form</p>
<p><span class="math display">\[p(y \mid \boldsymbol{\phi}) = h(y) c(\boldsymbol{\phi}) \text{exp}\left( \boldsymbol{\phi}^T \mathbf{t}(y) \right)\]</span></p>
<p>Recall the normal density: <span class="math display">\[p(y \mid \theta, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \text{exp} \left( - \frac{(y - \theta)^2}{2\sigma^2} \right)\]</span> For clarity later, let’s expand the quadratic term in the exponential: <span class="math display">\[p(y \mid \theta, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \text{exp} \left( - \frac{y^2 - 2\theta y + \theta^2}{2\sigma^2} \right)\]</span></p>
<p>Given the exponential family parameters</p>
<ul>
<li><span class="math inline">\(\mathbf{t}(y) = (y, y^2)\)</span></li>
<li><span class="math inline">\(\boldsymbol{\phi} = (\theta / \sigma^2, -(2\sigma^2)^{-1})\)</span></li>
<li><span class="math inline">\(c(\boldsymbol{\phi}) = | \phi_2 |^{1 / 2} \text{exp}\left(\phi_1^2 / (2\phi_2) \right)\)</span></li>
<li><span class="math inline">\(h(y) = 1 / \sqrt{\pi}\)</span></li>
</ul>
<p>You can reconstruct the normal density:</p>
<p><span class="math display">\[\begin{align}
p(y \mid \boldsymbol{\phi}) &amp;= \frac{1}{\sqrt{\pi}} | \phi_2 |^{1/2} \text{exp}\left( \frac{\phi_1^2}{2\phi_2} \right) \text{exp} \left(\begin{pmatrix} y &amp; y^2 \end{pmatrix}  \begin{pmatrix} \theta/\sigma^2 \\ -(2\sigma^2)^{-1} \end{pmatrix}  \right) \\
&amp;= \frac{1}{\sqrt{\pi}} (2\sigma^2)^{-1/2} \text{exp}\left( \frac{(\theta / \sigma^2)^2}{-2(2\sigma^2)^{-1}} \right) \text{exp} \left(\begin{pmatrix} y &amp; y^2 \end{pmatrix}  \begin{pmatrix} \theta/\sigma^2 \\ -(2\sigma^2)^{-1} \end{pmatrix} \right) \\
\end{align}\]</span></p>
<p>I am not going to do the exact algebra here, but notice that once you combine the exponential terms (and the matrix multiplication in the second exp), there are three separate terms added together. With a common factor of <span class="math inline">\(1 /
-2\sigma^2\)</span>, those three terms are the <span class="math inline">\(y^2\)</span>, <span class="math inline">\(2 \theta y\)</span>, and <span class="math inline">\(\theta^2\)</span> of the expanded normal density above.</p>
<p>With exponential family models, we can now “read off” conjugate priors; for the <span class="math inline">\(p\)</span>-dimensional case, the prior is <span class="math inline">\(p(\boldsymbol{\phi} \mid n_0,
\mathbf{t}_0) \propto c(\boldsymbol{\phi})^{n_0} \text{exp}(n_0 \mathbf{t}_0^T
\boldsymbol{\phi})\)</span>. Using the change of variables formula (which seems very complicated), you can reparamaterize the corresponding prior in terms of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\sigma^2\)</span>, which gives a prior that is the product of two priors we had determined previously to be conjugate: the normal and inverse-gamma densities.</p>
<p>There are some more details on the significance of specifying <span class="math inline">\(n_0\)</span> and <span class="math inline">\(\mathbf{t}_0\)</span> that I am skipping, since it essentially mirrors the prior specification advice in the previous sections.</p>
</section>
<section id="normal-model-for-non-normal-data" class="level1">
<h1>Normal model for non-normal data</h1>
<p>Because of the central limit theorem etc., we often use the normal model for non-normal data. This is especially applicable when 1) we are measuring summary statistics of a population, such as the mean, and 2) when we are measuring variables that might be the additive result of many underlying factors, which results in an approximately normal variable.</p>
</section>
<section id="exercises" class="level1">
<h1>Exercises</h1>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section">5.1</h2>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>school1 <span class="ot">=</span> <span class="fu">scan</span>(<span class="st">'Exercises/school1.dat'</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>school2 <span class="ot">=</span> <span class="fu">scan</span>(<span class="st">'Exercises/school2.dat'</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>school3 <span class="ot">=</span> <span class="fu">scan</span>(<span class="st">'Exercises/school3.dat'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="a" class="level3">
<h3 class="anchored" data-anchor-id="a">a</h3>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>s20 <span class="ot">=</span> <span class="dv">4</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>k0 <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>nu0 <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>params <span class="ot">=</span> <span class="fu">lapply</span>(<span class="fu">list</span>(school1, school2, school3), <span class="cf">function</span>(sdata) {</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Statistics of data</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="fu">length</span>(sdata)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>  ybar <span class="ot">=</span> <span class="fu">mean</span>(sdata)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>  s2 <span class="ot">=</span> <span class="fu">var</span>(sdata)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute posterior values, mun, s2n, kappan, nun</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>  kn <span class="ot">=</span> k0 <span class="sc">+</span> n</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>  nun <span class="ot">=</span> nu0 <span class="sc">+</span> n</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>  mun <span class="ot">=</span> (k0 <span class="sc">*</span> mu0 <span class="sc">+</span> n <span class="sc">*</span> ybar) <span class="sc">/</span> kn</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>  s2n <span class="ot">=</span> (<span class="dv">1</span> <span class="sc">/</span> nun) <span class="sc">*</span> (nu0 <span class="sc">*</span> s20 <span class="sc">+</span> (n <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> s2 <span class="sc">+</span> ((k0 <span class="sc">*</span> n) <span class="sc">/</span> kn) <span class="sc">*</span> (ybar <span class="sc">-</span> mu0)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="st">'mun'</span> <span class="ot">=</span> mun, <span class="st">'s2n'</span> <span class="ot">=</span> s2n, <span class="st">'kn'</span> <span class="ot">=</span> kn, <span class="st">'nun'</span> <span class="ot">=</span> nun)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>params.df <span class="ot">=</span> <span class="fu">as.data.frame</span>(<span class="fu">rbind</span>(params[[<span class="dv">1</span>]], params[[<span class="dv">2</span>]], params[[<span class="dv">3</span>]]))</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(params.df) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">'school1'</span>, <span class="st">'school2'</span>, <span class="st">'school3'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 5000 monte carlo samples. Need to estimate \sigma^2 before \theta.</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co"># I can easily do means and confidence intervals of the \sigma^2, but for</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># brevity, I will output only \theta</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>school1.s2.mc <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">rgamma</span>(<span class="dv">5000</span>, params.df[<span class="dv">1</span>, ]<span class="sc">$</span>nun <span class="sc">/</span> <span class="dv">2</span>, params.df[<span class="dv">1</span>, ]<span class="sc">$</span>s2n <span class="sc">*</span> params.df[<span class="dv">1</span>, ]<span class="sc">$</span>nun <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>school1.theta.mc <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">5000</span>, params.df[<span class="dv">1</span>, ]<span class="sc">$</span>mun, <span class="fu">sqrt</span>(school1.s2.mc <span class="sc">/</span> params.df[<span class="dv">1</span>, ]<span class="sc">$</span>kn))</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(school1.theta.mc, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>     2.5%       50%     97.5% 
 7.727896  9.284662 10.849475 </code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>school2.s2.mc <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">rgamma</span>(<span class="dv">5000</span>, params.df[<span class="dv">2</span>, ]<span class="sc">$</span>nun <span class="sc">/</span> <span class="dv">2</span>, params.df[<span class="dv">2</span>, ]<span class="sc">$</span>s2n <span class="sc">*</span> params.df[<span class="dv">2</span>, ]<span class="sc">$</span>nun <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>school2.theta.mc <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">5000</span>, params.df[<span class="dv">2</span>, ]<span class="sc">$</span>mun, <span class="fu">sqrt</span>(school2.s2.mc <span class="sc">/</span> params.df[<span class="dv">2</span>, ]<span class="sc">$</span>kn))</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(school2.theta.mc, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>    2.5%      50%    97.5% 
5.134124 6.930626 8.688361 </code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>school3.s2.mc <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">rgamma</span>(<span class="dv">5000</span>, params.df[<span class="dv">3</span>, ]<span class="sc">$</span>nun <span class="sc">/</span> <span class="dv">3</span>, params.df[<span class="dv">3</span>, ]<span class="sc">$</span>s2n <span class="sc">*</span> params.df[<span class="dv">3</span>, ]<span class="sc">$</span>nun <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>school3.theta.mc <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">5000</span>, params.df[<span class="dv">3</span>, ]<span class="sc">$</span>mun, <span class="fu">sqrt</span>(school3.s2.mc <span class="sc">/</span> params.df[<span class="dv">3</span>, ]<span class="sc">$</span>kn))</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(school3.theta.mc, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>    2.5%      50%    97.5% 
5.710521 7.812961 9.884127 </code></pre>
</div>
</div>
</section>
<section id="b" class="level3">
<h3 class="anchored" data-anchor-id="b">b</h3>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(combinat)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>school.theta.mc <span class="ot">=</span> <span class="fu">list</span>(school1.theta.mc, school2.theta.mc, school3.theta.mc)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>perms <span class="ot">=</span> <span class="fu">permn</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>theta.lt.probs <span class="ot">=</span> <span class="fu">lapply</span>(perms, <span class="cf">function</span>(perm) {</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This is a vector e.g. c(1, 3, 2)</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(school.theta.mc[[perm[<span class="dv">1</span>]]] <span class="sc">&lt;</span> school.theta.mc[[perm[<span class="dv">2</span>]]] <span class="sc">&amp;</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>         school.theta.mc[[perm[<span class="dv">2</span>]]] <span class="sc">&lt;</span> school.theta.mc[[perm[<span class="dv">3</span>]]])</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(theta.lt.probs) <span class="ot">=</span> <span class="fu">sapply</span>(perms, <span class="cf">function</span>(v) <span class="fu">paste</span>(v, <span class="at">collapse =</span><span class="st">' &lt; '</span>))</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>theta.lt.probs.stacked <span class="ot">=</span> <span class="fu">stack</span>(theta.lt.probs)[, <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>)] <span class="co"># Reverse stack order</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(theta.lt.probs.stacked, <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">'inequality'</span>, <span class="st">'prob'</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">inequality</th>
<th style="text-align: right;">prob</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1 &lt; 2 &lt; 3</td>
<td style="text-align: right;">0.0072</td>
</tr>
<tr class="even">
<td style="text-align: left;">1 &lt; 3 &lt; 2</td>
<td style="text-align: right;">0.0026</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3 &lt; 1 &lt; 2</td>
<td style="text-align: right;">0.0156</td>
</tr>
<tr class="even">
<td style="text-align: left;">3 &lt; 2 &lt; 1</td>
<td style="text-align: right;">0.2438</td>
</tr>
<tr class="odd">
<td style="text-align: left;">2 &lt; 3 &lt; 1</td>
<td style="text-align: right;">0.6164</td>
</tr>
<tr class="even">
<td style="text-align: left;">2 &lt; 1 &lt; 3</td>
<td style="text-align: right;">0.1144</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="c" class="level3">
<h3 class="anchored" data-anchor-id="c">c</h3>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>school.s2.mc <span class="ot">=</span> <span class="fu">list</span>(school1.s2.mc, school2.s2.mc, school3.s2.mc)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>school.y.mc <span class="ot">=</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="cf">function</span>(i) {</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  this.s2 <span class="ot">=</span> school.s2.mc[[i]]</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>  this.theta <span class="ot">=</span> school.theta.mc[[i]]</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rnorm</span>(<span class="dv">5000</span>, this.theta, <span class="fu">sqrt</span>(this.s2))</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>y.lt.probs <span class="ot">=</span> <span class="fu">lapply</span>(perms, <span class="cf">function</span>(perm) {</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This is a vector e.g. c(1, 3, 2)</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(school.y.mc[[perm[<span class="dv">1</span>]]] <span class="sc">&lt;</span> school.y.mc[[perm[<span class="dv">2</span>]]] <span class="sc">&amp;</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>         school.y.mc[[perm[<span class="dv">2</span>]]] <span class="sc">&lt;</span> school.y.mc[[perm[<span class="dv">3</span>]]])</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(y.lt.probs) <span class="ot">=</span> <span class="fu">sapply</span>(perms, <span class="cf">function</span>(v) <span class="fu">paste</span>(v, <span class="at">collapse =</span><span class="st">' &lt; '</span>))</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>y.lt.probs.stacked <span class="ot">=</span> <span class="fu">stack</span>(y.lt.probs)[, <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>)] <span class="co"># Reverse stack order</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(y.lt.probs.stacked, <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">'inequality'</span>, <span class="st">'prob'</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;">inequality</th>
<th style="text-align: right;">prob</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1 &lt; 2 &lt; 3</td>
<td style="text-align: right;">0.1112</td>
</tr>
<tr class="even">
<td style="text-align: left;">1 &lt; 3 &lt; 2</td>
<td style="text-align: right;">0.0854</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3 &lt; 1 &lt; 2</td>
<td style="text-align: right;">0.1446</td>
</tr>
<tr class="even">
<td style="text-align: left;">3 &lt; 2 &lt; 1</td>
<td style="text-align: right;">0.2180</td>
</tr>
<tr class="odd">
<td style="text-align: left;">2 &lt; 3 &lt; 1</td>
<td style="text-align: right;">0.2438</td>
</tr>
<tr class="even">
<td style="text-align: left;">2 &lt; 1 &lt; 3</td>
<td style="text-align: right;">0.1970</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="d" class="level3">
<h3 class="anchored" data-anchor-id="d">d</h3>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>theta1.big.prob <span class="ot">=</span> <span class="fu">mean</span>(school.theta.mc[[<span class="dv">1</span>]] <span class="sc">&gt;</span> school.theta.mc[[<span class="dv">2</span>]] <span class="sc">&amp;</span> school.theta.mc[[<span class="dv">1</span>]] <span class="sc">&gt;</span> school.theta.mc[[<span class="dv">3</span>]])</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(theta1.big.prob)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.8602</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>y1.big.prob <span class="ot">=</span> <span class="fu">mean</span>(school.y.mc[[<span class="dv">1</span>]] <span class="sc">&gt;</span> school.y.mc[[<span class="dv">2</span>]] <span class="sc">&amp;</span> school.y.mc[[<span class="dv">1</span>]] <span class="sc">&gt;</span> school.y.mc[[<span class="dv">3</span>]])</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(y1.big.prob)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4618</code></pre>
</div>
</div>
</section>
</section>
<section id="section-1" class="level2">
<h2 class="anchored" data-anchor-id="section-1">5.2</h2>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="dv">75</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>s20 <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>n.a <span class="ot">=</span> n.b <span class="ot">=</span> <span class="dv">16</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>ybar.a <span class="ot">=</span> <span class="fl">75.2</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>s2.a <span class="ot">=</span> <span class="fl">7.3</span><span class="sc">^</span><span class="dv">2</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>ybar.b <span class="ot">=</span> <span class="fl">77.5</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>s2.b <span class="ot">=</span> <span class="fl">8.1</span><span class="sc">^</span><span class="dv">2</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>k0nu0 <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>prob <span class="ot">=</span> <span class="fu">sapply</span>(k0nu0, <span class="cf">function</span>(p) {</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># p is the common parameter for k0 and nu0</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate posterior parameters</span></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>  kn.a <span class="ot">=</span> p <span class="sc">+</span> n.a</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>  nun.a <span class="ot">=</span> p <span class="sc">+</span> n.a</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>  mun.a <span class="ot">=</span> (p <span class="sc">*</span> mu0 <span class="sc">+</span> n.a <span class="sc">*</span> ybar.a) <span class="sc">/</span> kn.a</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>  s2n.a <span class="ot">=</span> (<span class="dv">1</span> <span class="sc">/</span> nun.a) <span class="sc">*</span> (p <span class="sc">*</span> s20 <span class="sc">+</span> (n.a <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> s2.a <span class="sc">+</span> ((p <span class="sc">*</span> n.a) <span class="sc">/</span> kn.a) <span class="sc">*</span> (ybar.a <span class="sc">-</span> mu0)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>  s2.a.mc <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">rgamma</span>(<span class="dv">10000</span>, nun.a <span class="sc">/</span> <span class="dv">2</span>, s2n.a <span class="sc">*</span> nun.a <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>  theta.a.mc <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>, mun.a, <span class="fu">sqrt</span>(s2.a.mc<span class="sc">/</span>kn.a))</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>  kn.b <span class="ot">=</span> p <span class="sc">+</span> n.b</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>  nun.b <span class="ot">=</span> p <span class="sc">+</span> n.b</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>  mun.b <span class="ot">=</span> (p <span class="sc">*</span> mu0 <span class="sc">+</span> n.b <span class="sc">*</span> ybar.b) <span class="sc">/</span> kn.b</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>  s2n.b <span class="ot">=</span> (<span class="dv">1</span> <span class="sc">/</span> nun.b) <span class="sc">*</span> (p <span class="sc">*</span> s20 <span class="sc">+</span> (n.b <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> s2.b <span class="sc">+</span> ((p <span class="sc">*</span> n.b) <span class="sc">/</span> kn.b) <span class="sc">*</span> (ybar.b <span class="sc">-</span> mu0)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>  s2.b.mc <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">rgamma</span>(<span class="dv">10000</span>, nun.b <span class="sc">/</span> <span class="dv">2</span>, s2n.b <span class="sc">*</span> nun.b <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>  theta.b.mc <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>, mun.b, <span class="fu">sqrt</span>(s2.b.mc<span class="sc">/</span>kn.b))</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(theta.a.mc <span class="sc">&lt;</span> theta.b.mc)</span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(k0nu0, prob, <span class="at">geom =</span> <span class="fu">c</span>(<span class="st">'line'</span>, <span class="st">'point'</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: `qplot()` was deprecated in ggplot2 3.4.0.</code></pre>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="5_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>In general, there is weak evidence that <span class="math inline">\(\theta_A &lt; \theta_B\)</span>. Depending on the strength of a person’s confidence in the prior, as quantified by a “prior sample size” <span class="math inline">\(\nu_0 = \kappa_0\)</span>, the posterior probability starts at ~0.58 and declines as strength increases. However, it takes a <em>very</em> strong prior belief for the probability to dip below 0.50.</p>
</section>
<section id="section-2" class="level2">
<h2 class="anchored" data-anchor-id="section-2">5.3</h2>
<p>I’ll derive <span class="math inline">\(p(\sigma^2 \mid y_1, \dots, y_n)\)</span>:</p>
<p><span class="math display">\[\begin{align}
p(\sigma^2 \mid y_1, \dots, y_n) &amp;\propto p(\sigma^2)p(y_1, \dots, y_n \mid \sigma^2) \\
&amp;= p(\sigma^2) \times \int p(y_1, \dots, y_n \mid \theta, \sigma^2) p(\theta \mid \sigma^2) \; d\theta \\
&amp;= \text{dinvgamma}(\sigma^2, \nu_0 / 2, \nu_0 \sigma_0^2 / 2) \int \left[ \prod_{i=1}^n \text{dnorm}(y_i, \theta, \sigma^2) \right] \times \text{dnorm}(\theta, \mu_0, \sigma^2 / \kappa_0)\; d\theta \\
&amp;= \text{dinvgamma}(\sigma^2, \nu_0 / 2, \nu_0 \sigma_0^2 / 2) \; \times \\ &amp;\quad \int (2\pi
\sigma^2)^{-n/2} \text{exp}\left( -\frac{1}{2} \sum \frac{(y_i -
\theta)^2}{\sigma^2} \right) \times \frac{1}{\sqrt{2\pi\sigma^2 / \kappa_0}}
\text{exp}\left(-\frac{1}{2} \frac{(\theta - \mu_0)^2}{\sigma^2 / \kappa_0}\right) \; d\theta \\
&amp;\propto \text{dinvgamma}(\sigma^2, \nu_0 / 2, \nu_0 \sigma_0^2 / 2) \; \times \\ &amp;\quad \sigma^{2(-(n + 1)/2)} \int \text{exp}\left( -\frac{1}{2} \sum \frac{(y_i -
\theta)^2}{\sigma^2} \right) \times
\text{exp}\left(-\frac{1}{2} \frac{(\theta - \mu_0)^2}{\sigma^2 / \kappa_0}\right) \; d\theta \\
&amp;\propto \dots
\end{align}\]</span></p>
<p>This is apparently non-trivial, and requires expanding the quadratic terms in the <span class="math inline">\(\text{exp}\)</span> terms. I’ll skip this for now.</p>
<!--
The marginal posterior distribution on $\theta^2$ can be calculated using Bayes'
rule above, or by simply plugging in the above posterior into


\begin{align}
p(\theta \mid y_1, \dots, y_n) &= \int p(\theta, \sigma^2 \mid y_1, \dots, y_n) \; d\sigma \\
&= \int \text{dnorm}(\theta, )
&= \int p(\theta \mid \sigma^2, y_1, \dots, y_n) \times p(\sigma^2 \mid y_1, \dots, y_n) \; d\sigma \\
&= \int \left[ \frac{1}{\sqrt{2\pi \tau_n^2}}} \text{exp}(-\frac{1}{2} \frac{(\theta - \mu_n)^2}{\tau_n^2})  \right] \times \left[ \right] \\
\end{align}
-->
</section>
<section id="section-3" class="level2">
<h2 class="anchored" data-anchor-id="section-3">5.4</h2>
<section id="a-1" class="level3">
<h3 class="anchored" data-anchor-id="a-1">a</h3>
<p>The log-likelihood function <span class="math inline">\(\ell\)</span> is</p>
<p><span class="math display">\[\begin{align}
\ell(Y \mid \theta, \sigma^2) &amp;= \log \left[ \frac{1}{\sqrt{2\pi\sigma^2}} \text{exp}\left(-\frac{1}{2}\sum_{i=1}^n \frac{(y_i - \theta)^2}{\sigma^2} \right) \right] \\
&amp;= -\frac{n}{2} \log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \theta)^2 \\
&amp;= -\frac{n}{2} \log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n y_i^2 -2\theta y_i + \theta^2 \\
&amp;= -\frac{n}{2} \log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2} \left(  \sum y_i^2 - 2\theta \sum y_i + n \theta^2 \right)
\end{align}\]</span></p>
<p>The first derivatives are</p>
<p><span class="math display">\[\begin{align}
\ell_\theta(Y \mid \theta, \sigma) &amp;= \frac{\sum y_i - n\theta}{\sigma^2} \\
\ell_{\sigma^2}(Y \mid \theta, \sigma) &amp;= -\frac{n}{2\sigma^2} + \frac{\sum y_i^2}{2(\sigma^2)^2} - \frac{2\theta \sum y_i}{2(\sigma^2)^2} + \frac{n\theta^2}{2(\sigma^2)^2} \\
\end{align}\]</span></p>
<p>So the <span class="math inline">\(2 \times 2\)</span> matrix <span class="math inline">\(I(\theta, \sigma^2)\)</span> is (expectations wrt <span class="math inline">\(Y\)</span>)</p>
<p><span class="math display">\[\begin{align}
I(\theta, \sigma^2) &amp;=
\begin{bmatrix}
-\mathbb{E}\left( \ell_{\theta\theta}(Y \mid \theta, \sigma) \right) &amp; -\mathbb{E}\left( \ell_{\theta\sigma^2}(Y \mid \theta, \sigma) \right) \\
-\mathbb{E}\left( \ell_{\sigma^2\theta}(Y \mid \theta, \sigma) \right) &amp; -\mathbb{E}\left( \ell_{\sigma^2\sigma^2}(Y \mid \theta, \sigma) \right)\\
\end{bmatrix} \\
&amp;= \begin{bmatrix}
-\mathbb{E}\left( -\frac{n}{\sigma^2} \right) &amp;
-\mathbb{E}\left( -\frac{\sum y_i - n\theta}{(\sigma^2)^2}\right) \\
-\mathbb{E}\left( -\frac{\sum y_i - n\theta}{(\sigma^2)^2}\right) &amp;
-\mathbb{E}\left( \frac{n}{2(\sigma^2)^2} - \frac{\sum y_i^2 - 2\theta\sum y_i + n\theta^2}{(\sigma^2)^3}\right) \\
\end{bmatrix} \\
&amp;= \begin{bmatrix}
\frac{n}{\sigma^2} &amp;
-\left( -\frac{n\theta - n\theta}{(\sigma^2)^2} \right) \\
-\left( -\frac{n\theta - n\theta}{(\sigma^2)^2} \right) &amp;
-\frac{n}{2(\sigma^2)^2} + \frac{\mathbb{E}(\sum y_i^2 - 2\theta\sum y_i + n\theta^2)}{(\sigma^2)^3} \\
\end{bmatrix} \\
&amp;= \begin{bmatrix}
\frac{n}{\sigma^2} &amp;
0 \\
0 &amp;
\frac{n}{2(\sigma^2)^2} \\
\end{bmatrix}
\end{align}\]</span></p>
<p>The last one is derived as follows:</p>
<p><span class="math display">\[\begin{align}
-\frac{n}{2(\sigma^2)^2} + \frac{\mathbb{E}(\sum y_i^2 - 2\theta\sum y_i + n\theta^2)}{(\sigma^2)^3} &amp;= -\frac{n}{2(\sigma^2)^2} + \frac{n(\theta^2 + \sigma^2) - 2\theta(n\theta) + n\theta^2}{(\sigma^2)^3} \\
&amp;= -\frac{n}{2(\sigma^2)^2} + \frac{n\sigma^2}{(\sigma^2)^3} \\
&amp;= -\frac{n}{2(\sigma^2)^2} + \frac{2n}{2(\sigma^2)^2} \\
&amp;= \frac{n}{2(\sigma^2)^2}
\end{align}\]</span></p>
<p>So Jeffrey’s prior is</p>
<p><span class="math display">\[\begin{align}
p_J(\theta, \sigma^2) &amp;\propto \sqrt{|I(\theta, \sigma^2)|} \\
&amp;= \sqrt{\frac{n^2}{2(\sigma^2)^3}} \\
&amp;\propto \sqrt{\frac{n^2}{2}} \sqrt{\frac{1}{(\sigma^2)^3}} \\
&amp;\propto (\sigma^2)^{-3/2}.
\end{align}\]</span></p>
</section>
<section id="b-1" class="level3">
<h3 class="anchored" data-anchor-id="b-1">b</h3>
<p><span class="math display">\[\begin{align}
p_J(\theta, \sigma^2 \mid \mathbf{y}) &amp;\propto p_J(\theta, \sigma^2) p(\mathbf{y} \mid \theta, \sigma^2) \\
&amp;\propto (\sigma^2)^{-3/2} \times (\sigma^2)^{-n/2} \text{exp}\left(-\frac{1}{2\sigma^2} \sum_{i = 1}^n (y_i - \theta)^2 \right) \\
&amp;= (\sigma^2)^{-(3 + n)/2} \text{exp}\left(-\frac{1}{2\sigma^2} \sum_{i = 1}^n (y_i - \theta)^2 \right)
\end{align}\]</span></p>
<p>Yes, since there is some normalizing constant that results in the integral over <span class="math inline">\(p_J\)</span> being equal to 1.</p>
<p>After some poking around, this is a <a href="https://www2.stat.duke.edu/courses/Fall10/sta114/notes15.pdf">normal-inverse-chi-squared distribution</a>.</p>
<!--
For random variables $W, V$, the density function is

\begin{align}
f(w, v; m, k, r, s) &\propto v^{-(r + 3)/2} \text{exp}\left(-\frac{k(w-m)^2 +
rs}{2v} \right)
\end{align}

For the random variables $\theta, \sigma^2$, $r = n$ is obvious from $p_J$. The
other parameters can be seen by expanding the exponential, where I borrow the
normal likelihood reparameterization trick from
[here](http://faculty.washington.edu/ezivot/econ583/mleLectures.pdf) (page 3):

\begin{align}
\text{exp}\left(-\frac{1}{2\sigma^2} \sum_{i = 1}^n (y_i - \theta)^2 \right) &=
\text{exp}\left(-\frac{1}{2\sigma^2} \sum_{i = 1}^n \left(y_i - \bar{y} + \bar{y} - \theta \right) \right) \\
&= \text{exp}\left(-\frac{1}{2\sigma^2} \sum_{i = 1}^n \left((y_i -\bar{y})^2 + 2(y_i - \bar{y})(\bar{y} - \theta) + (\bar{y} - \theta)^2 \right) \right)\\
&= \text{exp}\left(-\frac{1}{2\sigma^2} \sum_{i = 1}^n \left((y_i - \bar{y})^2 + n(\bar{y} - \theta)^2 \right) \right) \\
&= \text{exp}\left(-\frac{1}{2\sigma^2} \sum_{i = 1}^n \left((y_i - \bar{y})^2 + n(\bar{y} - \theta)^2 \right) \right) \\
&= \text{exp}\left(-\frac{1}{2\sigma^2} \sum_{i = 1}^n \left(n(\theta - \bar{y})^2 + (y_i - \bar{y})^2 \right) \right)  \\
\end{align}

So:

\begin{align}
\left( \{\theta, \sigma^2\} \; \middle| \; \mathbf{y} \right) &\sim \mathcal{N}_{\chi^{-2}}(\bar{y}, n, n, \frac{1}{n}\sum (y_i - \bar{y})^2)$$
\end{align}
-->


<!-- -->

</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb34" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Chapter 5: The Normal Model"</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Jesse Mu"</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "October 7, 2016"</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Setup --&gt;</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="in">```{r echo=FALSE, message=FALSE}</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span>opts_chunk<span class="sc">$</span><span class="fu">set</span>(<span class="at">fig.align =</span> <span class="st">'center'</span>, <span class="at">message =</span> <span class="cn">FALSE</span>)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(knitr)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cowplot)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(reshape)</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Begin writing --&gt;</span></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="fu"># The normal model</span></span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>A normal variable $Y$ with mean $\theta$ and variance $\sigma^2$ (and thus</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>standard deviation $\sigma$) we denote</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>$$Y \sim \mathcal{N}(\theta, \sigma^2)$$</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>and $Y$ has PDF</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>p(y) = \frac{1}{\sqrt{2\pi\sigma}} \text{exp}\left(-\frac{1}{2} \frac{(y - \theta)^2}{\sigma^2}\right)</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>Due to the central limit theorem, the normal model is used all the time to model</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>sample averages or values known to be the additive result of several random</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>variables.</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It's useful to remember the percentage of values lying within 1, 2, or 3</span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>standard deviations of the mean when constructing priors: 68, 95, and 99.7%,</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>respectively.</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a><span class="fu"># Inference for the mean, conditional on the variance</span></span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a>There are two parameters in the normal model. For simplicity, let's first assume</span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a>that the variance is known. Later we will show how we can perform inference</span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a>jointly for the mean and variance, but this result will still be useful</span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a>especially in Chapter 6, where Gibbs sampling requires full conditional</span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a>distributions of individual parameters.</span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a>To do inference assuming $\sigma^2$ is known, we need to identify the sampling</span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a>distribution and prior distribution, since we must calculate</span>
<span id="cb34-50"><a href="#cb34-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-51"><a href="#cb34-51" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-52"><a href="#cb34-52" aria-hidden="true" tabindex="-1"></a>p(\theta \mid \sigma^2, y_1, \dots, y_n) \propto p(y_1, \dots, y_n \mid \theta, \sigma^2) \times p(\theta \mid \sigma^2)</span>
<span id="cb34-53"><a href="#cb34-53" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-54"><a href="#cb34-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-55"><a href="#cb34-55" aria-hidden="true" tabindex="-1"></a>For the sampling distribution,</span>
<span id="cb34-56"><a href="#cb34-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-57"><a href="#cb34-57" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-58"><a href="#cb34-58" aria-hidden="true" tabindex="-1"></a>p(y_1, \dots, y_n \mid \theta, \sigma^2) &amp;= \prod_{i = 1}^n p(y_i \mid \theta, \sigma^2) <span class="sc">\\</span></span>
<span id="cb34-59"><a href="#cb34-59" aria-hidden="true" tabindex="-1"></a>&amp;= \prod_{i = 1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \text{exp}\left(-\frac{1}{2} \frac{(y_i - \theta)^2}{\sigma^2} \right) <span class="sc">\\</span></span>
<span id="cb34-60"><a href="#cb34-60" aria-hidden="true" tabindex="-1"></a>&amp;\propto \text{exp}\left<span class="co">[</span><span class="ot"> -\frac{1}{2} \sum_{i = 1}^n \frac{(y_1 - \theta)^2}{\sigma^2} \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb34-61"><a href="#cb34-61" aria-hidden="true" tabindex="-1"></a>&amp;\propto \text{exp}\left<span class="co">[</span><span class="ot"> -\frac{1}{2} \left( \frac{\sum y_i^2}{\sigma^2} - 2 \frac{\theta \sum y_i}{\sigma^2} + \frac{n\theta^2}{\sigma^2} \right) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb34-62"><a href="#cb34-62" aria-hidden="true" tabindex="-1"></a>&amp;\propto \text{exp}\left<span class="co">[</span><span class="ot"> -\frac{1}{2} \left( \frac{\sum y_i^2}{\sigma^2} - 2 \frac{\theta \sum y_i}{\sigma^2} + \frac{n\theta^2}{\sigma^2} \right) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb34-63"><a href="#cb34-63" aria-hidden="true" tabindex="-1"></a>&amp;\propto \text{exp}\left<span class="co">[</span><span class="ot"> -\frac{1}{2} \left( \frac{\sum y_i^2}{\sigma^2} - 2 \frac{\theta \sum y_i}{\sigma^2} \right) \right</span><span class="co">]</span></span>
<span id="cb34-64"><a href="#cb34-64" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-65"><a href="#cb34-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-66"><a href="#cb34-66" aria-hidden="true" tabindex="-1"></a>From this we know two things:</span>
<span id="cb34-67"><a href="#cb34-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-68"><a href="#cb34-68" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$p(\theta \mid \sigma^2, y_1, \dots, y_n) \propto p(y_1, \dots, y_n \mid</span>
<span id="cb34-69"><a href="#cb34-69" aria-hidden="true" tabindex="-1"></a>\theta, \sigma^2) \times p(\theta \mid \sigma^2)$ depends only on $<span class="sc">\{</span>\sum y_i^2,</span>
<span id="cb34-70"><a href="#cb34-70" aria-hidden="true" tabindex="-1"></a>\sum y_i<span class="sc">\}</span>$, so that is a sufficient statistic, as is $<span class="sc">\{</span>\bar{y}, s^2<span class="sc">\}</span>$ (from</span>
<span id="cb34-71"><a href="#cb34-71" aria-hidden="true" tabindex="-1"></a>which $\sum y_i^2, \sum y_i$ are recoverable).</span>
<span id="cb34-72"><a href="#cb34-72" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>For $p(\theta \mid \sigma^2)$ to be conjugate, the posterior needs to have</span>
<span id="cb34-73"><a href="#cb34-73" aria-hidden="true" tabindex="-1"></a>quadratic terms in the exponential function, i.e. $\text{exp}(c_1 (\theta -</span>
<span id="cb34-74"><a href="#cb34-74" aria-hidden="true" tabindex="-1"></a>c_2)^2)$</span>
<span id="cb34-75"><a href="#cb34-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-76"><a href="#cb34-76" aria-hidden="true" tabindex="-1"></a>In particular, 2 is a new strategy for trying to identify the conjugate prior </span>
<span id="cb34-77"><a href="#cb34-77" aria-hidden="true" tabindex="-1"></a>family. Since the exponential terms in the sampling model and prior distribution</span>
<span id="cb34-78"><a href="#cb34-78" aria-hidden="true" tabindex="-1"></a>must be combined to produce the same class of posterior distribution, we must</span>
<span id="cb34-79"><a href="#cb34-79" aria-hidden="true" tabindex="-1"></a>pick a prior distribution $\propto \text{exp}(c_1, (\theta - c_2)^2)$.</span>
<span id="cb34-80"><a href="#cb34-80" aria-hidden="true" tabindex="-1"></a>Conveniently, normal distributions themselves have these terms. We can verify</span>
<span id="cb34-81"><a href="#cb34-81" aria-hidden="true" tabindex="-1"></a>that the normal family is conjugate to the normal sampling model. Let $\theta \mid \sigma^2 \sim \mathcal{N}(\mu_0, \tau_0^2)$ (interpretations of the prior parameters comes later):</span>
<span id="cb34-82"><a href="#cb34-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-83"><a href="#cb34-83" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-84"><a href="#cb34-84" aria-hidden="true" tabindex="-1"></a>p(\theta \mid \sigma^2, y_1, \dots, y_n) &amp;\propto p(y_1, \dots, y_n \mid \theta, \sigma^2) \times p(\theta \mid \sigma^2) <span class="sc">\\</span></span>
<span id="cb34-85"><a href="#cb34-85" aria-hidden="true" tabindex="-1"></a>&amp;\propto \text{exp}\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \theta)^2 \right) \times \text{exp}\left(-\frac{1}{2 \tau_0^2} (\theta - \mu_0)^2\right) <span class="sc">\\</span></span>
<span id="cb34-86"><a href="#cb34-86" aria-hidden="true" tabindex="-1"></a>&amp;= \text{exp}\left<span class="co">[</span><span class="ot"> -\frac{1}{2} \left( \frac{1}{\tau_0^2}(\theta^2 - 2\theta\mu_0 + \mu_0^2) + \frac{1}{\sigma^2}(\sum y_i^2 - 2\theta y_i + n\theta^2) \right) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb34-87"><a href="#cb34-87" aria-hidden="true" tabindex="-1"></a>&amp;= \text{exp}\left<span class="co">[</span><span class="ot"> -\frac{1}{2} \left( \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}\right)\theta^2 + 2\left( \frac{\mu_0}{\tau_0^2} + \frac{\sum y_i}{\sigma^2} \right)\theta \right) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb34-88"><a href="#cb34-88" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-89"><a href="#cb34-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-90"><a href="#cb34-90" aria-hidden="true" tabindex="-1"></a>To simplify this, let</span>
<span id="cb34-91"><a href="#cb34-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-92"><a href="#cb34-92" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$a = \frac{1}{\tau_0^2} + \frac{n}{\sigma^2}$</span>
<span id="cb34-93"><a href="#cb34-93" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$b = \frac{\mu_0}{\tau_0^2} + \frac{\sum y_i}{\sigma^2}$</span>
<span id="cb34-94"><a href="#cb34-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-95"><a href="#cb34-95" aria-hidden="true" tabindex="-1"></a>Then</span>
<span id="cb34-96"><a href="#cb34-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-97"><a href="#cb34-97" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-98"><a href="#cb34-98" aria-hidden="true" tabindex="-1"></a>p(\theta \mid \sigma^2, y_1, \dots, y_n) &amp;\propto \text{exp}\left<span class="co">[</span><span class="ot"> -\frac{1}{2} (a\theta^2 - 2b\theta) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb34-99"><a href="#cb34-99" aria-hidden="true" tabindex="-1"></a>&amp;\propto \text{exp}\left<span class="co">[</span><span class="ot"> -\frac{1}{2}(a\theta^2 - 2b\theta + b^2/a) + \frac{1}{2} b^2 / a \right</span><span class="co">]</span> &amp; \text{Completing the square} <span class="sc">\\</span></span>
<span id="cb34-100"><a href="#cb34-100" aria-hidden="true" tabindex="-1"></a>&amp;\propto \text{exp}\left<span class="co">[</span><span class="ot"> -\frac{1}{2}(a\theta^2 - 2b\theta + b^2/a) \right</span><span class="co">]</span> &amp;\text{Throw away constants} <span class="sc">\\</span></span>
<span id="cb34-101"><a href="#cb34-101" aria-hidden="true" tabindex="-1"></a>&amp;\propto \text{exp}\left<span class="co">[</span><span class="ot"> -\frac{1}{2}a(\theta^2 - 2b\theta / a + b^2/a^2) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb34-102"><a href="#cb34-102" aria-hidden="true" tabindex="-1"></a>&amp;\propto \text{exp}\left<span class="co">[</span><span class="ot"> -\frac{1}{2}a(\theta - b/a)^2 \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb34-103"><a href="#cb34-103" aria-hidden="true" tabindex="-1"></a>&amp;\propto \text{exp}\left<span class="co">[</span><span class="ot"> -\frac{1}{2}\left( \frac{\theta - b/a}{1 / \sqrt{a}} \right)^2 \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb34-104"><a href="#cb34-104" aria-hidden="true" tabindex="-1"></a>&amp;= \text{dnorm}(\theta, b/a, 1/a).</span>
<span id="cb34-105"><a href="#cb34-105" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-106"><a href="#cb34-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-107"><a href="#cb34-107" aria-hidden="true" tabindex="-1"></a>Let these posterior parameters be $\mu_n$ and $\tau_n^2$. In later chapters we</span>
<span id="cb34-108"><a href="#cb34-108" aria-hidden="true" tabindex="-1"></a>will commonly follow this naming scheme: initial guesses of parameters are</span>
<span id="cb34-109"><a href="#cb34-109" aria-hidden="true" tabindex="-1"></a>denoted $\theta_0$, then the posterior parameters are denoted $\theta_n$, i.e.</span>
<span id="cb34-110"><a href="#cb34-110" aria-hidden="true" tabindex="-1"></a>the updated parameters after a sample of size $n$.</span>
<span id="cb34-111"><a href="#cb34-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-112"><a href="#cb34-112" aria-hidden="true" tabindex="-1"></a>Specifically,</span>
<span id="cb34-113"><a href="#cb34-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-114"><a href="#cb34-114" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-115"><a href="#cb34-115" aria-hidden="true" tabindex="-1"></a>\mu_n &amp;= b/a = \frac{\frac{1}{\tau_0^2} \mu_0 + \frac{n}{\sigma^2}\bar{y}}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}} <span class="sc">\\</span></span>
<span id="cb34-116"><a href="#cb34-116" aria-hidden="true" tabindex="-1"></a>\tau_n^2 &amp;= \frac{1}{a} = \frac{1}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}.</span>
<span id="cb34-117"><a href="#cb34-117" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-118"><a href="#cb34-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-119"><a href="#cb34-119" aria-hidden="true" tabindex="-1"></a>So $\theta \mid \sigma^2, y_1, \dots, y_n \sim \mathcal{N}(\mu_n, \tau_n^2)$.</span>
<span id="cb34-120"><a href="#cb34-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-121"><a href="#cb34-121" aria-hidden="true" tabindex="-1"></a><span class="fu">## Combining information</span></span>
<span id="cb34-122"><a href="#cb34-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-123"><a href="#cb34-123" aria-hidden="true" tabindex="-1"></a>Notice in the posterior parameters the frequency of *inverse variances* i.e.</span>
<span id="cb34-124"><a href="#cb34-124" aria-hidden="true" tabindex="-1"></a>$\frac{1}{\tau_0^2}, \frac{n}{\sigma^2}$. This hints at the importance of using</span>
<span id="cb34-125"><a href="#cb34-125" aria-hidden="true" tabindex="-1"></a>**precision** to understand and parameterize our normal prior and posterior</span>
<span id="cb34-126"><a href="#cb34-126" aria-hidden="true" tabindex="-1"></a>distributions. Specifically, it is much more concise to express the above</span>
<span id="cb34-127"><a href="#cb34-127" aria-hidden="true" tabindex="-1"></a>parameters in terms of variance. Specifically, if we let $\tilde{\tau_n^2} = 1 /</span>
<span id="cb34-128"><a href="#cb34-128" aria-hidden="true" tabindex="-1"></a>\tau_n^2$, i.e. the posterior *precision*, and similar tildes for the over variables,</span>
<span id="cb34-129"><a href="#cb34-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-130"><a href="#cb34-130" aria-hidden="true" tabindex="-1"></a>$$\tilde{\tau_n^2} = \tilde{\tau_0^2} + n\tilde{\sigma^2}$$</span>
<span id="cb34-131"><a href="#cb34-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-132"><a href="#cb34-132" aria-hidden="true" tabindex="-1"></a>So intuitively, our posterior precision is a combination of our prior belief in</span>
<span id="cb34-133"><a href="#cb34-133" aria-hidden="true" tabindex="-1"></a>the precision of the true population mean of the data, plus the (assumed known)</span>
<span id="cb34-134"><a href="#cb34-134" aria-hidden="true" tabindex="-1"></a>precision, where a larger sample size $n$ increases this precision.</span>
<span id="cb34-135"><a href="#cb34-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-136"><a href="#cb34-136" aria-hidden="true" tabindex="-1"></a>Using precision, the fact that $\mu_n$ is a weighted average of prior and sample</span>
<span id="cb34-137"><a href="#cb34-137" aria-hidden="true" tabindex="-1"></a>information becomes more clear. Notice</span>
<span id="cb34-138"><a href="#cb34-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-139"><a href="#cb34-139" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-140"><a href="#cb34-140" aria-hidden="true" tabindex="-1"></a>\mu_n &amp;= \frac{\frac{1}{\tau_0^2}}{ \frac{1}{\tau_0^2} + \frac{n}{\sigma^2} } \mu_0 + </span>
<span id="cb34-141"><a href="#cb34-141" aria-hidden="true" tabindex="-1"></a>\frac{\frac{n}{\sigma^2}}{ \frac{1}{\tau_0^2} + \frac{n}{\sigma^2} } \bar{y} <span class="sc">\\</span></span>
<span id="cb34-142"><a href="#cb34-142" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\tilde{\tau_0^2}}{\tilde{\tau_0^2} + n\tilde{\sigma^2}} \mu_0 + \frac{n\tilde{\sigma^2}}{\tilde{\tau_0^2} + n\tilde{\sigma^2}} \bar{y}</span>
<span id="cb34-143"><a href="#cb34-143" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-144"><a href="#cb34-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-145"><a href="#cb34-145" aria-hidden="true" tabindex="-1"></a>So the posterior mean is a weighted average of the prior expectation of the mean</span>
<span id="cb34-146"><a href="#cb34-146" aria-hidden="true" tabindex="-1"></a>$\mu_0$ weighted by the precision of that mean $\tilde{\tau_0^2}$, and the</span>
<span id="cb34-147"><a href="#cb34-147" aria-hidden="true" tabindex="-1"></a>observed sample mean $\bar{y}$ weighted by our sample size $n$ and the (assumed</span>
<span id="cb34-148"><a href="#cb34-148" aria-hidden="true" tabindex="-1"></a>known) precision $\tilde{\sigma^2}$.</span>
<span id="cb34-149"><a href="#cb34-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-150"><a href="#cb34-150" aria-hidden="true" tabindex="-1"></a>How do we select $\tau_0^2$? One intuitive way to think about it (as we have</span>
<span id="cb34-151"><a href="#cb34-151" aria-hidden="true" tabindex="-1"></a>done with one-parameter models) is by treating our prior parameters for $\theta$</span>
<span id="cb34-152"><a href="#cb34-152" aria-hidden="true" tabindex="-1"></a>as derived from $\kappa_0$ prior "observations" from the same (or similar)</span>
<span id="cb34-153"><a href="#cb34-153" aria-hidden="true" tabindex="-1"></a>population that we are sampling from. Then $\mu_0$ is the average of these prior</span>
<span id="cb34-154"><a href="#cb34-154" aria-hidden="true" tabindex="-1"></a>observations, and let $\tau_0^2 = \sigma^2 / \kappa_0$ be the variance of the</span>
<span id="cb34-155"><a href="#cb34-155" aria-hidden="true" tabindex="-1"></a>*mean* of these prior observations. Then the posterior mean simplifies quite nicely to:</span>
<span id="cb34-156"><a href="#cb34-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-157"><a href="#cb34-157" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-158"><a href="#cb34-158" aria-hidden="true" tabindex="-1"></a>\mu_n &amp;= \frac{\kappa_0}{\kappa_0 + n}\mu_0 + \frac{n}{\kappa_0 + n}\bar{y} <span class="sc">\\</span></span>
<span id="cb34-159"><a href="#cb34-159" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\kappa_0}{\kappa_n}\mu_0 + \frac{n}{\kappa_n}\bar{y} &amp; \text{Let $\kappa_n = \kappa_0 + n$} <span class="sc">\\</span></span>
<span id="cb34-160"><a href="#cb34-160" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-161"><a href="#cb34-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-162"><a href="#cb34-162" aria-hidden="true" tabindex="-1"></a>which is just a weighted average of $\mu_0$ and $\bar{y}$ given the number of</span>
<span id="cb34-163"><a href="#cb34-163" aria-hidden="true" tabindex="-1"></a>prior "observations" $\kappa_0$ and the sample size $n$. We will take advantage of this when jointly estimating the mean and variance for</span>
<span id="cb34-164"><a href="#cb34-164" aria-hidden="true" tabindex="-1"></a>the normal model. The idea is to first estimate the variance, then assume that </span>
<span id="cb34-165"><a href="#cb34-165" aria-hidden="true" tabindex="-1"></a>variance $\sigma^2$ is known such that $\tau_0^2 = \sigma^2 / \kappa_0$ can be </span>
<span id="cb34-166"><a href="#cb34-166" aria-hidden="true" tabindex="-1"></a>estimated.</span>
<span id="cb34-167"><a href="#cb34-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-168"><a href="#cb34-168" aria-hidden="true" tabindex="-1"></a><span class="fu">## Prediction</span></span>
<span id="cb34-169"><a href="#cb34-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-170"><a href="#cb34-170" aria-hidden="true" tabindex="-1"></a>To obtain the posterior predictive distribution, instead of doing complex </span>
<span id="cb34-171"><a href="#cb34-171" aria-hidden="true" tabindex="-1"></a>integration, we can use a trick.</span>
<span id="cb34-172"><a href="#cb34-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-173"><a href="#cb34-173" aria-hidden="true" tabindex="-1"></a>$\tilde{Y}$ is normally distributed with mean $\theta$ and variance $\sigma^2$. This is equivalent to saying</span>
<span id="cb34-174"><a href="#cb34-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-175"><a href="#cb34-175" aria-hidden="true" tabindex="-1"></a>$$\tilde{Y} = \theta + \tilde{\epsilon}$$</span>
<span id="cb34-176"><a href="#cb34-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-177"><a href="#cb34-177" aria-hidden="true" tabindex="-1"></a>where $\theta \sim \mathcal{N}(\mu_n, \tau_n^2$, $\tilde{\epsilon} \sim</span>
<span id="cb34-178"><a href="#cb34-178" aria-hidden="true" tabindex="-1"></a>\mathcal{N}(0, \sigma^2)$. So adding these normal distributions together gives</span>
<span id="cb34-179"><a href="#cb34-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-180"><a href="#cb34-180" aria-hidden="true" tabindex="-1"></a>$$\tilde{Y} \mid \sigma^2, y_1, \dots, y_n \sim \mathcal{N}(\mu_n, \tau_n^2 + \sigma^2)$$</span>
<span id="cb34-181"><a href="#cb34-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-182"><a href="#cb34-182" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Midge wing data</span></span>
<span id="cb34-183"><a href="#cb34-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-184"><a href="#cb34-184" aria-hidden="true" tabindex="-1"></a>The wing lengths of 9 members of a species of "midge" are measured. We are </span>
<span id="cb34-185"><a href="#cb34-185" aria-hidden="true" tabindex="-1"></a>interested in estimates of the mean wing length and variance. Prior information </span>
<span id="cb34-186"><a href="#cb34-186" aria-hidden="true" tabindex="-1"></a>from other populations suggests that wing lengths are typically around 1.9mm, so</span>
<span id="cb34-187"><a href="#cb34-187" aria-hidden="true" tabindex="-1"></a>our initial estimate $\mu_0 = 1.9$. One way of assigning a prior estimate of the</span>
<span id="cb34-188"><a href="#cb34-188" aria-hidden="true" tabindex="-1"></a>variance of the mean $\tau_0^2$ is to pick the spread of the prior such that all</span>
<span id="cb34-189"><a href="#cb34-189" aria-hidden="true" tabindex="-1"></a>of its mass is above 0, since wing lengths can't be negative. So we select</span>
<span id="cb34-190"><a href="#cb34-190" aria-hidden="true" tabindex="-1"></a>$\tau_0$ such that 2 standard deviations from 1.9 &gt; 0: $\tau_0 = 0.95$.</span>
<span id="cb34-191"><a href="#cb34-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-192"><a href="#cb34-192" aria-hidden="true" tabindex="-1"></a>Our data are:</span>
<span id="cb34-193"><a href="#cb34-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-196"><a href="#cb34-196" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-197"><a href="#cb34-197" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">1.64</span>, <span class="fl">1.70</span>, <span class="fl">1.72</span>, <span class="fl">1.74</span>, <span class="fl">1.82</span>, <span class="fl">1.82</span>, <span class="fl">1.82</span>, <span class="fl">1.90</span>, <span class="fl">2.08</span>)</span>
<span id="cb34-198"><a href="#cb34-198" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(y)</span>
<span id="cb34-199"><a href="#cb34-199" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(y)</span>
<span id="cb34-200"><a href="#cb34-200" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-201"><a href="#cb34-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-202"><a href="#cb34-202" aria-hidden="true" tabindex="-1"></a>Since we are assuming for now that $\sigma^2$ is known, let's use $s^2 = \sigma^2$.</span>
<span id="cb34-203"><a href="#cb34-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-204"><a href="#cb34-204" aria-hidden="true" tabindex="-1"></a>Now calculating $\mu_n, \tau_n^2$ is simply done by plugging in the relevant</span>
<span id="cb34-205"><a href="#cb34-205" aria-hidden="true" tabindex="-1"></a>formulas:</span>
<span id="cb34-206"><a href="#cb34-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-207"><a href="#cb34-207" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-208"><a href="#cb34-208" aria-hidden="true" tabindex="-1"></a>\mu_n &amp;= \frac{1.11 (1.9) + \frac{9}{0.017} 1.804}{1.11 + \frac{9}{0.017}} = 1.805 <span class="sc">\\</span></span>
<span id="cb34-209"><a href="#cb34-209" aria-hidden="true" tabindex="-1"></a>\tau_n^2 &amp;= \frac{1}{1.11 + \frac{9}{0.017}} = 0.002</span>
<span id="cb34-210"><a href="#cb34-210" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-211"><a href="#cb34-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-214"><a href="#cb34-214" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-215"><a href="#cb34-215" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="fl">1.805</span>, <span class="fu">sqrt</span>(<span class="fl">0.002</span>))</span>
<span id="cb34-216"><a href="#cb34-216" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-217"><a href="#cb34-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-218"><a href="#cb34-218" aria-hidden="true" tabindex="-1"></a><span class="fu"># Joint inference for the mean and variance</span></span>
<span id="cb34-219"><a href="#cb34-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-220"><a href="#cb34-220" aria-hidden="true" tabindex="-1"></a>For joint inference, we wish to compute the joint probability distribution of</span>
<span id="cb34-221"><a href="#cb34-221" aria-hidden="true" tabindex="-1"></a>$(\theta, \sigma^2)$ given the data, which proceeds much like before</span>
<span id="cb34-222"><a href="#cb34-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-223"><a href="#cb34-223" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-224"><a href="#cb34-224" aria-hidden="true" tabindex="-1"></a>p(\theta, \sigma^2 \mid y_1, \dots, y_n) = \frac{p(y_1, \dots, y_n \mid \theta, \sigma^2) p(\theta, \sigma^2)}{p(y_1, \dots, y_n)}</span>
<span id="cb34-225"><a href="#cb34-225" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-226"><a href="#cb34-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-227"><a href="#cb34-227" aria-hidden="true" tabindex="-1"></a>Notice that the only real difference in this two parameter case is the joint</span>
<span id="cb34-228"><a href="#cb34-228" aria-hidden="true" tabindex="-1"></a>prior $p(\theta, \sigma^2)$, which we should select a conjugate prior</span>
<span id="cb34-229"><a href="#cb34-229" aria-hidden="true" tabindex="-1"></a>distribution for to simplify posterior calculation.</span>
<span id="cb34-230"><a href="#cb34-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-231"><a href="#cb34-231" aria-hidden="true" tabindex="-1"></a>Notice that if we split up</span>
<span id="cb34-232"><a href="#cb34-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-233"><a href="#cb34-233" aria-hidden="true" tabindex="-1"></a>$$p(\theta, \sigma^2) = p(\theta \mid \sigma^2) p(\sigma^2)$$ then, from the</span>
<span id="cb34-234"><a href="#cb34-234" aria-hidden="true" tabindex="-1"></a>previous section, we already know that the normal distribution is a conjugate</span>
<span id="cb34-235"><a href="#cb34-235" aria-hidden="true" tabindex="-1"></a>prior for the $p(\theta \mid \sigma^2)$: $\mathcal{N}(\mu_0, \tau_0^2)$. With</span>
<span id="cb34-236"><a href="#cb34-236" aria-hidden="true" tabindex="-1"></a>this selection, we have</span>
<span id="cb34-237"><a href="#cb34-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-238"><a href="#cb34-238" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-239"><a href="#cb34-239" aria-hidden="true" tabindex="-1"></a>p(\theta, \sigma^2) &amp;= p(\theta \mid \sigma^2) p(\sigma^2) <span class="sc">\\</span></span>
<span id="cb34-240"><a href="#cb34-240" aria-hidden="true" tabindex="-1"></a>&amp;= \text{dnorm}(\theta, \mu_0, \tau_0) \times p(\sigma^2) <span class="sc">\\</span></span>
<span id="cb34-241"><a href="#cb34-241" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-242"><a href="#cb34-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-243"><a href="#cb34-243" aria-hidden="true" tabindex="-1"></a>Now if we let $\tau_0$ depend on $\sigma^2$ as we explored in the previous</span>
<span id="cb34-244"><a href="#cb34-244" aria-hidden="true" tabindex="-1"></a>section, this simplifies calculations. If $\tau_0$ is not proportional to</span>
<span id="cb34-245"><a href="#cb34-245" aria-hidden="true" tabindex="-1"></a>$\sigma^2$, then there is no good closed-form solution for a posterior</span>
<span id="cb34-246"><a href="#cb34-246" aria-hidden="true" tabindex="-1"></a>distribution, which is a "semiconjugate" prior; see Chapter 6 for details.</span>
<span id="cb34-247"><a href="#cb34-247" aria-hidden="true" tabindex="-1"></a>Specifically, if we let $\tau_0^2 = \sigma^2 / \kappa_0 \implies \tau_0 = \sigma</span>
<span id="cb34-248"><a href="#cb34-248" aria-hidden="true" tabindex="-1"></a>/ \sqrt{\kappa_0}$, i.e. $\tau_0^2$ is the variance of the mean of a sample of</span>
<span id="cb34-249"><a href="#cb34-249" aria-hidden="true" tabindex="-1"></a>size $\kappa_0$ from a population with variance $\sigma^2$:</span>
<span id="cb34-250"><a href="#cb34-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-251"><a href="#cb34-251" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-252"><a href="#cb34-252" aria-hidden="true" tabindex="-1"></a>p(\theta, \sigma^2) &amp;= \text{dnorm}(\theta, \mu_0, \tau = \sigma / \sqrt{\kappa_0}) \times p(\sigma^2)</span>
<span id="cb34-253"><a href="#cb34-253" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-254"><a href="#cb34-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-255"><a href="#cb34-255" aria-hidden="true" tabindex="-1"></a>Now we need to specify $p(\sigma^2)$. We are told that the Gamma distribtion</span>
<span id="cb34-256"><a href="#cb34-256" aria-hidden="true" tabindex="-1"></a>(with support on $(0, \infty)$) is not conjugate for the normal variance, but it</span>
<span id="cb34-257"><a href="#cb34-257" aria-hidden="true" tabindex="-1"></a>*is* conjugate for the normal *precision* $1 / \sigma^2$. It's not mentioned how</span>
<span id="cb34-258"><a href="#cb34-258" aria-hidden="true" tabindex="-1"></a>this is determined, but it probably has something to do with the ease of </span>
<span id="cb34-259"><a href="#cb34-259" aria-hidden="true" tabindex="-1"></a>expressing posterior estimates in terms of precision in the previous section</span>
<span id="cb34-260"><a href="#cb34-260" aria-hidden="true" tabindex="-1"></a>where $\sigma^2$ is known.</span>
<span id="cb34-261"><a href="#cb34-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-262"><a href="#cb34-262" aria-hidden="true" tabindex="-1"></a>Let $1 / \sigma^2 \sim \text{Gamma}(a, b)$. Like we have done previously, we</span>
<span id="cb34-263"><a href="#cb34-263" aria-hidden="true" tabindex="-1"></a>would like to parameterize this distribution such that we can interpret choices</span>
<span id="cb34-264"><a href="#cb34-264" aria-hidden="true" tabindex="-1"></a>of the parameters of the prior as sensibly conveying some prior expectation</span>
<span id="cb34-265"><a href="#cb34-265" aria-hidden="true" tabindex="-1"></a>about the precision in this case. If we let</span>
<span id="cb34-266"><a href="#cb34-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-267"><a href="#cb34-267" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$a = \nu_0 / 2$</span>
<span id="cb34-268"><a href="#cb34-268" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$b = a \sigma_0^2 = \frac{\nu_0}{2}\sigma^2_0$</span>
<span id="cb34-269"><a href="#cb34-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-270"><a href="#cb34-270" aria-hidden="true" tabindex="-1"></a>We will show later that we can interpret $(\sigma^2_0, \nu_0)$ as the sample</span>
<span id="cb34-271"><a href="#cb34-271" aria-hidden="true" tabindex="-1"></a>variance and sample size of a set of prior observations.</span>
<span id="cb34-272"><a href="#cb34-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-273"><a href="#cb34-273" aria-hidden="true" tabindex="-1"></a>If $1 / \sigma^2 \sim \text{Gamma}(\nu_0 / 2, \sigma^2_0 \nu_0 / 2)$, then</span>
<span id="cb34-274"><a href="#cb34-274" aria-hidden="true" tabindex="-1"></a>notice that $\mathbb{E}(1 / \sigma^2) \neq 1 / \mathbb{E}(\sigma^2)$ since the</span>
<span id="cb34-275"><a href="#cb34-275" aria-hidden="true" tabindex="-1"></a>inverse is not a linear function. To calculate $\mathbb{E}(\sigma^2)$ requires something more complicated (law of the unconscious statistician?), or we can use the fact that $\sigma^2 \sim \text{Inverse-Gamma}(\nu_0/2, \sigma^2_0 \nu_0 / 2)$, for which</span>
<span id="cb34-276"><a href="#cb34-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-277"><a href="#cb34-277" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{E}(\sigma^2) = \frac{\sigma^2_0 \nu_0 / 2}{(\nu_0 / 2) - 1}$</span>
<span id="cb34-278"><a href="#cb34-278" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{mode}(\sigma^2) = \frac{\sigma^2_0 \nu_0 / 2}{(\nu_0 / 2) + 1}$</span>
<span id="cb34-279"><a href="#cb34-279" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{Var}(\sigma^2)$ decreases as $\nu_0$ increases.</span>
<span id="cb34-280"><a href="#cb34-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-281"><a href="#cb34-281" aria-hidden="true" tabindex="-1"></a>From this you can already intuit how $\nu_0$ is a sample size, and $\sigma^2_0$</span>
<span id="cb34-282"><a href="#cb34-282" aria-hidden="true" tabindex="-1"></a>is an initial guess of the sample variance where the expectation of $\sigma^2$</span>
<span id="cb34-283"><a href="#cb34-283" aria-hidden="true" tabindex="-1"></a>more closely approaches $\sigma^2_0$ as $\nu_0$ increases.</span>
<span id="cb34-284"><a href="#cb34-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-285"><a href="#cb34-285" aria-hidden="true" tabindex="-1"></a><span class="fu">## Posterior inference</span></span>
<span id="cb34-286"><a href="#cb34-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-287"><a href="#cb34-287" aria-hidden="true" tabindex="-1"></a>Now we have fully specified (1) our prior distributions:</span>
<span id="cb34-288"><a href="#cb34-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-289"><a href="#cb34-289" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-290"><a href="#cb34-290" aria-hidden="true" tabindex="-1"></a>1 / \sigma^2 &amp;\sim \text{Gamma}(\nu_0 / 2, \sigma^2_0 \nu_0 / 2) <span class="sc">\\</span></span>
<span id="cb34-291"><a href="#cb34-291" aria-hidden="true" tabindex="-1"></a>\theta \mid \sigma^2 &amp;\sim \mathcal{N}(\mu_0, \sigma^2 / \kappa_0), <span class="sc">\\</span></span>
<span id="cb34-292"><a href="#cb34-292" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-293"><a href="#cb34-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-294"><a href="#cb34-294" aria-hidden="true" tabindex="-1"></a>and (2) our sampling model:</span>
<span id="cb34-295"><a href="#cb34-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-296"><a href="#cb34-296" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-297"><a href="#cb34-297" aria-hidden="true" tabindex="-1"></a>Y_1, \dots, Y_n \mid \theta, \sigma^2 \sim \text{ i.i.d. } \mathcal{N}(\theta,</span>
<span id="cb34-298"><a href="#cb34-298" aria-hidden="true" tabindex="-1"></a>\sigma^2)</span>
<span id="cb34-299"><a href="#cb34-299" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-300"><a href="#cb34-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-301"><a href="#cb34-301" aria-hidden="true" tabindex="-1"></a>Now we wish to calculate</span>
<span id="cb34-302"><a href="#cb34-302" aria-hidden="true" tabindex="-1"></a>$p(\theta, \sigma^2 \mid y_1, \dots, y_n)$</span>
<span id="cb34-303"><a href="#cb34-303" aria-hidden="true" tabindex="-1"></a>which we can decompose to a product of marginal and conditional probabilities, just like the prior:</span>
<span id="cb34-304"><a href="#cb34-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-305"><a href="#cb34-305" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-306"><a href="#cb34-306" aria-hidden="true" tabindex="-1"></a>p(\theta, \sigma^2 \mid y_1, \dots, y_n) =  p(\theta \mid \sigma^2, y_1, \dots, y_n) p(\sigma^2 \mid y_1, \dots, y_n)</span>
<span id="cb34-307"><a href="#cb34-307" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-308"><a href="#cb34-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-309"><a href="#cb34-309" aria-hidden="true" tabindex="-1"></a>This is convenient because we already know $p(\theta \mid \sigma^2, y_1, \dots, y_n)$ from the one-parameter case:</span>
<span id="cb34-310"><a href="#cb34-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-311"><a href="#cb34-311" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-312"><a href="#cb34-312" aria-hidden="true" tabindex="-1"></a>\theta \mid \sigma^2, y_1, \dots y_n \sim \mathcal{N}(\mu_n, \tau_n^2)</span>
<span id="cb34-313"><a href="#cb34-313" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-314"><a href="#cb34-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-315"><a href="#cb34-315" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb34-316"><a href="#cb34-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-317"><a href="#cb34-317" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-318"><a href="#cb34-318" aria-hidden="true" tabindex="-1"></a>\mu_n &amp;= \frac{ \frac{1}{\tau_0^2} \mu_0 + \frac{n}{\sigma^2} \bar{y} }{ \frac{1}{\tau_0^2} + \frac{n}{\sigma^2} } <span class="sc">\\</span></span>
<span id="cb34-319"><a href="#cb34-319" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{ \frac{\kappa_0}{\sigma^2} \mu_0 + \frac{n}{\sigma^2} \bar{y} }{ \frac{\kappa_0}{\sigma^2} + \frac{n}{\sigma^2} } &amp; \text{Sub $\tau_0^2 = \sigma^2 / \kappa_0$} <span class="sc">\\</span></span>
<span id="cb34-320"><a href="#cb34-320" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{ \kappa_0 \mu_0 + n \bar{y} } { \kappa_0 + n } &amp; \text{$\sigma^2$s cancel} <span class="sc">\\</span></span>
<span id="cb34-321"><a href="#cb34-321" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-322"><a href="#cb34-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-323"><a href="#cb34-323" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb34-324"><a href="#cb34-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-325"><a href="#cb34-325" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-326"><a href="#cb34-326" aria-hidden="true" tabindex="-1"></a>\tau_n^2 &amp;= \frac{1}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}} <span class="sc">\\</span></span>
<span id="cb34-327"><a href="#cb34-327" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{\frac{\kappa_0}{\sigma^2} + \frac{n}{\sigma^2}} &amp; \text{Sub $\tau_0^2 = \sigma^2 / \kappa_0$} <span class="sc">\\</span></span>
<span id="cb34-328"><a href="#cb34-328" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\sigma^2}{\kappa_0 + n}.</span>
<span id="cb34-329"><a href="#cb34-329" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-330"><a href="#cb34-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-331"><a href="#cb34-331" aria-hidden="true" tabindex="-1"></a>If we let $\kappa_n = \kappa_0 + n$ (remember we will interpret $\kappa_0$ as a prior sample size, and $n$ as this sample size), then we have</span>
<span id="cb34-332"><a href="#cb34-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-333"><a href="#cb34-333" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-334"><a href="#cb34-334" aria-hidden="true" tabindex="-1"></a>\theta \mid \sigma^2, y_1, \dots y_n \sim \mathcal{N}(\mu_n, \sigma^2 / \kappa_n)</span>
<span id="cb34-335"><a href="#cb34-335" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-336"><a href="#cb34-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-337"><a href="#cb34-337" aria-hidden="true" tabindex="-1"></a>Where like before, $\mu_n$ is a weighted average of $\mu_0$ and $\bar{y}$</span>
<span id="cb34-338"><a href="#cb34-338" aria-hidden="true" tabindex="-1"></a>dependent on the "prior" sample size $\kappa_0$ and the sample size $n$, and</span>
<span id="cb34-339"><a href="#cb34-339" aria-hidden="true" tabindex="-1"></a>$\sigma^2 / \kappa_n$ is the sampling variance of the sample mean given known</span>
<span id="cb34-340"><a href="#cb34-340" aria-hidden="true" tabindex="-1"></a>variance $\sigma^2$ and our "sample size" $\kappa_n$.</span>
<span id="cb34-341"><a href="#cb34-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-342"><a href="#cb34-342" aria-hidden="true" tabindex="-1"></a>Recall our posterior distribution decomposition:</span>
<span id="cb34-343"><a href="#cb34-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-344"><a href="#cb34-344" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-345"><a href="#cb34-345" aria-hidden="true" tabindex="-1"></a>p(\theta, \sigma^2 \mid y_1, \dots, y_n) =  p(\theta \mid \sigma^2, y_1, \dots, y_n) p(\sigma^2 \mid y_1, \dots, y_n)</span>
<span id="cb34-346"><a href="#cb34-346" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb34-347"><a href="#cb34-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-348"><a href="#cb34-348" aria-hidden="true" tabindex="-1"></a>Once we calculate the second component, the posterior distribution of $\sigma^2$, we will have</span>
<span id="cb34-349"><a href="#cb34-349" aria-hidden="true" tabindex="-1"></a>fully specified the joint posterior distribution.</span>
<span id="cb34-350"><a href="#cb34-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-351"><a href="#cb34-351" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-352"><a href="#cb34-352" aria-hidden="true" tabindex="-1"></a>p(\sigma^2 \mid y_1, \dots, y_n) &amp;\propto p(\sigma^2) p(y_1, \dots, y_n \mid \sigma^2) <span class="sc">\\</span></span>
<span id="cb34-353"><a href="#cb34-353" aria-hidden="true" tabindex="-1"></a>&amp;= p(\sigma^2) \int p(y_1, \dots, y_n \mid \theta, \sigma^2) p(\theta \mid \sigma^2) \; d\theta <span class="sc">\\</span></span>
<span id="cb34-354"><a href="#cb34-354" aria-hidden="true" tabindex="-1"></a>&amp;= \text{dinverse-gamma}(\sigma^2, \nu_0 / 2, \sigma_0^2 \nu_0 / 2) \times <span class="sc">\\</span> &amp;\quad \int \left<span class="co">[</span><span class="ot"> \left( \prod_{i = 1}^{n} p(y_i \mid \theta, \sigma^2) \right) \times \text{dnorm}(\theta, \mu_0, \sigma^2 / \kappa_0)  \right</span><span class="co">]</span> \; d\theta <span class="sc">\\</span></span>
<span id="cb34-355"><a href="#cb34-355" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-356"><a href="#cb34-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-357"><a href="#cb34-357" aria-hidden="true" tabindex="-1"></a>This integral is left as an exercise (Exercise 5.3). The result is that</span>
<span id="cb34-358"><a href="#cb34-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-359"><a href="#cb34-359" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-360"><a href="#cb34-360" aria-hidden="true" tabindex="-1"></a>\sigma^2 \mid y_1, \dots, y_n &amp; \sim \text{Inverse-Gamma}(\nu_n / 2, \sigma_n^2 \nu_n / 2) <span class="sc">\\</span></span>
<span id="cb34-361"><a href="#cb34-361" aria-hidden="true" tabindex="-1"></a>1 / \sigma^2 \mid y_1, \dots, y_n &amp;\sim \text{Gamma}(\nu_n / 2, \sigma_n^2 \nu_n / 2)</span>
<span id="cb34-362"><a href="#cb34-362" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-363"><a href="#cb34-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-364"><a href="#cb34-364" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb34-365"><a href="#cb34-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-366"><a href="#cb34-366" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\nu_n = \nu_0 + n$, like $\kappa_n$</span>
<span id="cb34-367"><a href="#cb34-367" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\sigma_n^2 = \frac{1}{\nu_n} \left<span class="co">[</span><span class="ot"> \nu_0 \sigma_0^2 + (n - 1)s^2 + \frac{\kappa_0 n}{\kappa_n} (\bar{y} - \mu_0)^2 \right</span><span class="co">]</span>$</span>
<span id="cb34-368"><a href="#cb34-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-369"><a href="#cb34-369" aria-hidden="true" tabindex="-1"></a>$\nu_n$ is fairly intuitive, it acts as a sample size which is the "prior sample</span>
<span id="cb34-370"><a href="#cb34-370" aria-hidden="true" tabindex="-1"></a>size" of the variance plus the sample size $n$. $\sigma_n^2$ is a bit harder to</span>
<span id="cb34-371"><a href="#cb34-371" aria-hidden="true" tabindex="-1"></a>understand. There are three terms here. The first, $\nu_0 \sigma_0^2$, can be</span>
<span id="cb34-372"><a href="#cb34-372" aria-hidden="true" tabindex="-1"></a>thought of as a prior sum of squared observations from the sample mean ($\nu_0$</span>
<span id="cb34-373"><a href="#cb34-373" aria-hidden="true" tabindex="-1"></a>prior samples with variance $\sigma_0^2$). Similarly, $(n - 1)s^2$, where $s^2 =</span>
<span id="cb34-374"><a href="#cb34-374" aria-hidden="true" tabindex="-1"></a>\sum_{i = 1}^n (y_i - \bar{y})^2 / (n - 1)$, is literally the sum of squared</span>
<span id="cb34-375"><a href="#cb34-375" aria-hidden="true" tabindex="-1"></a>(actually observed) observations from the sample mean. Lastly, the third term</span>
<span id="cb34-376"><a href="#cb34-376" aria-hidden="true" tabindex="-1"></a>increases the posterior variance if the observed sample mean $(\bar{y})$ is</span>
<span id="cb34-377"><a href="#cb34-377" aria-hidden="true" tabindex="-1"></a>*far* away from the expected prior mean $\mu_0$, since this would suggest higher</span>
<span id="cb34-378"><a href="#cb34-378" aria-hidden="true" tabindex="-1"></a>variance. All three "sum of squares-ish" terms are combined, then divided by the</span>
<span id="cb34-379"><a href="#cb34-379" aria-hidden="true" tabindex="-1"></a>total number of "observations" $\nu_n = n + \nu_0$, as commonly done to estimate</span>
<span id="cb34-380"><a href="#cb34-380" aria-hidden="true" tabindex="-1"></a>variance from a sample.</span>
<span id="cb34-381"><a href="#cb34-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-382"><a href="#cb34-382" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary of posterior inference</span></span>
<span id="cb34-383"><a href="#cb34-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-384"><a href="#cb34-384" aria-hidden="true" tabindex="-1"></a>This is a lot to handle, since there are a lot of moving parts. In sum, for</span>
<span id="cb34-385"><a href="#cb34-385" aria-hidden="true" tabindex="-1"></a>inference with the normal model, there are four prior parameters to specify:</span>
<span id="cb34-386"><a href="#cb34-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-387"><a href="#cb34-387" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\sigma_0^2$, an initial estimate for the variance;</span>
<span id="cb34-388"><a href="#cb34-388" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\nu_0$, a "prior sample size" from which the initial estimate of the *variance* is observed;</span>
<span id="cb34-389"><a href="#cb34-389" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mu_0$, an initial estimate for the population mean;</span>
<span id="cb34-390"><a href="#cb34-390" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\kappa_0$, a "prior sample size" from which the initial estimate of the *mean* is observed</span>
<span id="cb34-391"><a href="#cb34-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-392"><a href="#cb34-392" aria-hidden="true" tabindex="-1"></a>Then we have</span>
<span id="cb34-393"><a href="#cb34-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-394"><a href="#cb34-394" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$1 / \sigma^2 \sim \text{Gamma}(\nu_0 / 2, \sigma^2_0 \nu_0 / 2)$</span>
<span id="cb34-395"><a href="#cb34-395" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>$\implies \mathbb{E}(\sigma^2) = \sigma^2_0 \frac{\nu_0 / 2}{\nu_0 / 2 - 1}$ (use expectation of inverse gamma)</span>
<span id="cb34-396"><a href="#cb34-396" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\theta \mid \sigma^2 \sim \mathcal{N}(\mu_0, \sigma^2 / \kappa_0)$</span>
<span id="cb34-397"><a href="#cb34-397" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>$\implies \mathbb{E}(\theta) = \mu_0$</span>
<span id="cb34-398"><a href="#cb34-398" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb34-399"><a href="#cb34-399" aria-hidden="true" tabindex="-1"></a>The updated parameters are</span>
<span id="cb34-400"><a href="#cb34-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-401"><a href="#cb34-401" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\nu_n = \nu_0 + n$</span>
<span id="cb34-402"><a href="#cb34-402" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\sigma_n^2 = \frac{1}{\nu_n} \left<span class="co">[</span><span class="ot"> \nu_0 \sigma_0^2 + (n - 1)s^2 + \frac{\kappa_0 n}{\kappa_n} (\bar{y} - \mu_0)^2 \right</span><span class="co">]</span>$</span>
<span id="cb34-403"><a href="#cb34-403" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mu_n = \frac{\kappa_0 \mu_0 + n\bar{y}}{\kappa_n}$</span>
<span id="cb34-404"><a href="#cb34-404" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\kappa_n = \kappa_0 + n$</span>
<span id="cb34-405"><a href="#cb34-405" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb34-406"><a href="#cb34-406" aria-hidden="true" tabindex="-1"></a>So that the posterior is finally</span>
<span id="cb34-407"><a href="#cb34-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-408"><a href="#cb34-408" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$1 / \sigma^2 \mid y_1, \dots, y_n \sim \text{Gamma}(\nu_n / 2, \sigma^2_n \nu_n / 2)$</span>
<span id="cb34-409"><a href="#cb34-409" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Where $\mathbb{E}(\sigma^2 \mid y_1, \dots, y_n) = \frac{\sigma^2_n \nu_n}{2 (\nu_n / 2 - 1)}$ (using the expectation of the inverse gamma)</span>
<span id="cb34-410"><a href="#cb34-410" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\theta \mid \sigma^2, y_1, \dots, y_n \sim \mathcal{N}(\mu_n, \sigma^2 / \kappa_n)$</span>
<span id="cb34-411"><a href="#cb34-411" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Where $\mathbb{E}(\theta \mid y_1, \dots, y_n, \sigma^2) = \mu_n = \frac{\kappa_0 \mu_0 + n \bar{y}}{\kappa_n}$</span>
<span id="cb34-412"><a href="#cb34-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-413"><a href="#cb34-413" aria-hidden="true" tabindex="-1"></a>Note how the prior sample sizes for the variance and the mean are</span>
<span id="cb34-414"><a href="#cb34-414" aria-hidden="true" tabindex="-1"></a>decoupled because they update differently. However, it's common to set $\nu_0 = </span>
<span id="cb34-415"><a href="#cb34-415" aria-hidden="true" tabindex="-1"></a>\kappa_0$.</span>
<span id="cb34-416"><a href="#cb34-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-417"><a href="#cb34-417" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example</span></span>
<span id="cb34-418"><a href="#cb34-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-419"><a href="#cb34-419" aria-hidden="true" tabindex="-1"></a>Back to midge wing length, although this time, we are leaving our estimate of </span>
<span id="cb34-420"><a href="#cb34-420" aria-hidden="true" tabindex="-1"></a>the variance of the population free as well.</span>
<span id="cb34-421"><a href="#cb34-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-422"><a href="#cb34-422" aria-hidden="true" tabindex="-1"></a>From other populations, say that we weakly believe that our prior estimates of</span>
<span id="cb34-423"><a href="#cb34-423" aria-hidden="true" tabindex="-1"></a>the population mean and variance are $\mu_0 = 1.9$ and $\sigma_0^2 = 0.01$,</span>
<span id="cb34-424"><a href="#cb34-424" aria-hidden="true" tabindex="-1"></a>respectively. Since this is a weak belief we will pick $\kappa_0 = \nu_0 = 1$. Now our prior distributions are</span>
<span id="cb34-425"><a href="#cb34-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-426"><a href="#cb34-426" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$1 / \sigma^2 \sim \text{Gamma}(0.5, 0.005)$</span>
<span id="cb34-427"><a href="#cb34-427" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\theta \mid \sigma^2 \sim \mathcal{N}(1.9, \sigma^2 / \kappa_0)$</span>
<span id="cb34-428"><a href="#cb34-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-429"><a href="#cb34-429" aria-hidden="true" tabindex="-1"></a>Recall that our data are</span>
<span id="cb34-430"><a href="#cb34-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-433"><a href="#cb34-433" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-434"><a href="#cb34-434" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">1.64</span>, <span class="fl">1.70</span>, <span class="fl">1.72</span>, <span class="fl">1.74</span>, <span class="fl">1.82</span>, <span class="fl">1.82</span>, <span class="fl">1.82</span>, <span class="fl">1.90</span>, <span class="fl">2.08</span>)</span>
<span id="cb34-435"><a href="#cb34-435" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">length</span>(y)</span>
<span id="cb34-436"><a href="#cb34-436" aria-hidden="true" tabindex="-1"></a>ybar <span class="ot">=</span> <span class="fu">mean</span>(y)</span>
<span id="cb34-437"><a href="#cb34-437" aria-hidden="true" tabindex="-1"></a>s2 <span class="ot">=</span> <span class="fu">var</span>(y)</span>
<span id="cb34-438"><a href="#cb34-438" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-439"><a href="#cb34-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-440"><a href="#cb34-440" aria-hidden="true" tabindex="-1"></a>Now we calculate the parameters of the posterior distributions</span>
<span id="cb34-441"><a href="#cb34-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-442"><a href="#cb34-442" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\kappa_n = \kappa_0 + n = 1 + 9 = 10$</span>
<span id="cb34-443"><a href="#cb34-443" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\nu_n = \nu_0 + n = 1 + 9 =10$</span>
<span id="cb34-444"><a href="#cb34-444" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mu_n = \frac{\kappa_0 \mu_0 + n\bar{y}}{\kappa_n} = \frac{1.9 + 9(1.804)}{10} = 1.814$</span>
<span id="cb34-445"><a href="#cb34-445" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>\begin{align}</span>
<span id="cb34-446"><a href="#cb34-446" aria-hidden="true" tabindex="-1"></a>\sigma_n^2 &amp;= \frac{1}{\nu_n} \left<span class="co">[</span><span class="ot"> \nu_0 \sigma_0^2 + (n - 1)s^2 + \frac{\kappa_0 n}{\kappa_n} (\bar{y} - \mu_0)^2 \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb34-447"><a href="#cb34-447" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{10} \left<span class="co">[</span><span class="ot"> 0.01 + 8(0.168) + \frac{9}{10} (1.804 - 1.9)^2 \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb34-448"><a href="#cb34-448" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{10} \left<span class="co">[</span><span class="ot"> 0.01 + 0.135 + 0.008 \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb34-449"><a href="#cb34-449" aria-hidden="true" tabindex="-1"></a>&amp;= 0.015</span>
<span id="cb34-450"><a href="#cb34-450" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-451"><a href="#cb34-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-452"><a href="#cb34-452" aria-hidden="true" tabindex="-1"></a>So our joint posterior distribution is</span>
<span id="cb34-453"><a href="#cb34-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-454"><a href="#cb34-454" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-455"><a href="#cb34-455" aria-hidden="true" tabindex="-1"></a>1 / \sigma^2 \mid y_1, \dots, y_n &amp;\sim \text{Gamma}(10/2 = 5, 10(0.015 / 2) = 0.075) <span class="sc">\\</span></span>
<span id="cb34-456"><a href="#cb34-456" aria-hidden="true" tabindex="-1"></a>\theta \mid \sigma^2, y_1, \dots, y_n &amp;\sim \mathcal{N}(1.814, \sigma^2 / 10)</span>
<span id="cb34-457"><a href="#cb34-457" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-458"><a href="#cb34-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-459"><a href="#cb34-459" aria-hidden="true" tabindex="-1"></a>Now we can plot the posterior distribution for various values of $\theta$ and $\sigma^2$.</span>
<span id="cb34-460"><a href="#cb34-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-463"><a href="#cb34-463" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-464"><a href="#cb34-464" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior</span></span>
<span id="cb34-465"><a href="#cb34-465" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="fl">1.9</span></span>
<span id="cb34-466"><a href="#cb34-466" aria-hidden="true" tabindex="-1"></a>kappa0 <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb34-467"><a href="#cb34-467" aria-hidden="true" tabindex="-1"></a>s20 <span class="ot">=</span> <span class="fl">0.01</span></span>
<span id="cb34-468"><a href="#cb34-468" aria-hidden="true" tabindex="-1"></a>nu0 <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb34-469"><a href="#cb34-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-470"><a href="#cb34-470" aria-hidden="true" tabindex="-1"></a>kappan <span class="ot">=</span> kappa0 <span class="sc">+</span> n</span>
<span id="cb34-471"><a href="#cb34-471" aria-hidden="true" tabindex="-1"></a>nun <span class="ot">=</span> nu0 <span class="sc">+</span> n</span>
<span id="cb34-472"><a href="#cb34-472" aria-hidden="true" tabindex="-1"></a>mun <span class="ot">=</span> (kappa0 <span class="sc">*</span> mu0 <span class="sc">+</span> n <span class="sc">*</span> ybar) <span class="sc">/</span> kappan</span>
<span id="cb34-473"><a href="#cb34-473" aria-hidden="true" tabindex="-1"></a>s2n <span class="ot">=</span> (<span class="dv">1</span> <span class="sc">/</span> nun) <span class="sc">*</span> (nu0 <span class="sc">*</span> s20 <span class="sc">+</span> (n <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> s2 <span class="sc">+</span> (kappa0 <span class="sc">*</span> n <span class="sc">/</span> kappan) <span class="sc">*</span> (ybar <span class="sc">-</span> mu0)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb34-474"><a href="#cb34-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-475"><a href="#cb34-475" aria-hidden="true" tabindex="-1"></a>Theta <span class="ot">=</span> <span class="fu">seq</span>(<span class="fl">1.6</span>, <span class="fl">2.0</span>, <span class="at">by =</span> <span class="fl">0.005</span>)</span>
<span id="cb34-476"><a href="#cb34-476" aria-hidden="true" tabindex="-1"></a>Sigma2 <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="fl">0.04</span>, <span class="at">by =</span> <span class="fl">0.0001</span>)</span>
<span id="cb34-477"><a href="#cb34-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-478"><a href="#cb34-478" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(invgamma)</span>
<span id="cb34-479"><a href="#cb34-479" aria-hidden="true" tabindex="-1"></a>post.func <span class="ot">=</span> <span class="cf">function</span>(theta, sigma2) {</span>
<span id="cb34-480"><a href="#cb34-480" aria-hidden="true" tabindex="-1"></a>  <span class="fu">dnorm</span>(theta, mun, <span class="fu">sqrt</span>(sigma2 <span class="sc">/</span> kappan)) <span class="sc">*</span> <span class="fu">dinvgamma</span>(sigma2, nun <span class="sc">/</span> <span class="dv">2</span>, s2n <span class="sc">*</span> nun <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb34-481"><a href="#cb34-481" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb34-482"><a href="#cb34-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-483"><a href="#cb34-483" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">outer</span>(Theta, Sigma2, post.func)</span>
<span id="cb34-484"><a href="#cb34-484" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(d) <span class="ot">=</span> Theta</span>
<span id="cb34-485"><a href="#cb34-485" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(d) <span class="ot">=</span> Sigma2</span>
<span id="cb34-486"><a href="#cb34-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-487"><a href="#cb34-487" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">melt</span>(d)</span>
<span id="cb34-488"><a href="#cb34-488" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(df) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">'theta'</span>, <span class="st">'sigma2'</span>, <span class="st">'density'</span>)</span>
<span id="cb34-489"><a href="#cb34-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-490"><a href="#cb34-490" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> sigma2, <span class="at">z =</span> density)) <span class="sc">+</span></span>
<span id="cb34-491"><a href="#cb34-491" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_contour</span>(<span class="fu">aes</span>(<span class="at">color =</span> ..level..)) <span class="sc">+</span></span>
<span id="cb34-492"><a href="#cb34-492" aria-hidden="true" tabindex="-1"></a>  <span class="fu">guides</span>(<span class="at">color =</span> <span class="cn">FALSE</span>)</span>
<span id="cb34-493"><a href="#cb34-493" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-494"><a href="#cb34-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-495"><a href="#cb34-495" aria-hidden="true" tabindex="-1"></a><span class="fu">## Monte carlo sampling</span></span>
<span id="cb34-496"><a href="#cb34-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-497"><a href="#cb34-497" aria-hidden="true" tabindex="-1"></a>We can simulate values from the posterior by first sampling $\sigma^{2(n)}$ from</span>
<span id="cb34-498"><a href="#cb34-498" aria-hidden="true" tabindex="-1"></a>its inverse gamma distribution, and $\theta^{(n)}$ from its normal distribution</span>
<span id="cb34-499"><a href="#cb34-499" aria-hidden="true" tabindex="-1"></a>conditioned on $\sigma^{2(n)}$. Then $<span class="sc">\{</span>\theta^{(n)}, \sigma^{2(n)}<span class="sc">\}</span>$ represent</span>
<span id="cb34-500"><a href="#cb34-500" aria-hidden="true" tabindex="-1"></a>samples from the joint distribution $p(\theta, \sigma^2 \mid y_1, \dots, y_n)$,</span>
<span id="cb34-501"><a href="#cb34-501" aria-hidden="true" tabindex="-1"></a>and either set of values by themselves represents samples from the full marginal</span>
<span id="cb34-502"><a href="#cb34-502" aria-hidden="true" tabindex="-1"></a>distribution. This is intuitive for $\sigma^{2(n)}$ but less so for</span>
<span id="cb34-503"><a href="#cb34-503" aria-hidden="true" tabindex="-1"></a>$\theta^{(n)}$. The key is to notice that, although $\theta^{(n)}$ is sampled</span>
<span id="cb34-504"><a href="#cb34-504" aria-hidden="true" tabindex="-1"></a>conditioned on $\sigma^{2(n)}$, multiple $\theta^{(n)}$ samples are conditioned</span>
<span id="cb34-505"><a href="#cb34-505" aria-hidden="true" tabindex="-1"></a>on multiple *different* $\sigma^{2(n)}$s, so the $\theta^{(n)}$ do indeed</span>
<span id="cb34-506"><a href="#cb34-506" aria-hidden="true" tabindex="-1"></a>represent samples from the marginal distribution.</span>
<span id="cb34-507"><a href="#cb34-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-510"><a href="#cb34-510" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-511"><a href="#cb34-511" aria-hidden="true" tabindex="-1"></a>s2.mc <span class="ot">=</span> <span class="fu">rinvgamma</span>(<span class="dv">10000</span>, nun <span class="sc">/</span> <span class="dv">2</span>, s2n <span class="sc">*</span> nun <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb34-512"><a href="#cb34-512" aria-hidden="true" tabindex="-1"></a>theta.mc <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>, mun, <span class="fu">sqrt</span>(s2.mc <span class="sc">/</span> kappan)) <span class="co"># Accepts a vector of parameters</span></span>
<span id="cb34-513"><a href="#cb34-513" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(theta.mc)</span>
<span id="cb34-514"><a href="#cb34-514" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(theta.mc, <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span>
<span id="cb34-515"><a href="#cb34-515" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-516"><a href="#cb34-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-519"><a href="#cb34-519" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-520"><a href="#cb34-520" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">sigma2 =</span> s2.mc, <span class="at">theta =</span> theta.mc)) <span class="sc">+</span></span>
<span id="cb34-521"><a href="#cb34-521" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> sigma2), <span class="at">alpha =</span> <span class="fl">0.1</span>)</span>
<span id="cb34-522"><a href="#cb34-522" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-523"><a href="#cb34-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-524"><a href="#cb34-524" aria-hidden="true" tabindex="-1"></a><span class="fu">## Improper priors</span></span>
<span id="cb34-525"><a href="#cb34-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-526"><a href="#cb34-526" aria-hidden="true" tabindex="-1"></a>What if we want to use *no* prior information? See what happens to our </span>
<span id="cb34-527"><a href="#cb34-527" aria-hidden="true" tabindex="-1"></a>posterior distribution $\kappa_0, \nu_0 \rightarrow 0$. Using the formula above,</span>
<span id="cb34-528"><a href="#cb34-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-529"><a href="#cb34-529" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\sigma_n^2 \rightarrow \frac{n - 1}{n}s^2$</span>
<span id="cb34-530"><a href="#cb34-530" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mu_n \rightarrow \bar{y}$.</span>
<span id="cb34-531"><a href="#cb34-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-532"><a href="#cb34-532" aria-hidden="true" tabindex="-1"></a>Then, the "posterior" (plugging in $\kappa_0 = \nu_0 = 0$ and the posterior</span>
<span id="cb34-533"><a href="#cb34-533" aria-hidden="true" tabindex="-1"></a>parameters $\sigma_n^2, \mu_n$ and simplifying) would be</span>
<span id="cb34-534"><a href="#cb34-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-535"><a href="#cb34-535" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-536"><a href="#cb34-536" aria-hidden="true" tabindex="-1"></a>1 / \sigma^2 \mid y_1, \dots, y_n &amp;\sim \text{Gamma}(\frac{n}{2}, \frac{1}{n} \frac{n}{2}\sum (y_i - y)^2)$ <span class="sc">\\</span></span>
<span id="cb34-537"><a href="#cb34-537" aria-hidden="true" tabindex="-1"></a>\theta \mid \sigma^2, y_1, \dots, y_n &amp;\sim \mathcal{N}(\bar{y}, \frac{\sigma^2}{n})</span>
<span id="cb34-538"><a href="#cb34-538" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-539"><a href="#cb34-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-540"><a href="#cb34-540" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- and the *full conditional* distribution of $1 / \sigma^2$ is --&gt;</span></span>
<span id="cb34-541"><a href="#cb34-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-542"><a href="#cb34-542" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \begin{align} --&gt;</span></span>
<span id="cb34-543"><a href="#cb34-543" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- p(1 / \sigma^2 \mid \theta, y_1, \dots, y_n) &amp;= \int p(1 / sigma^2 \mid) --&gt;</span></span>
<span id="cb34-544"><a href="#cb34-544" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \end{align} --&gt;</span></span>
<span id="cb34-545"><a href="#cb34-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-546"><a href="#cb34-546" aria-hidden="true" tabindex="-1"></a>With ["significant</span>
<span id="cb34-547"><a href="#cb34-547" aria-hidden="true" tabindex="-1"></a>algebra"](http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/nuisance_parameters.pdf), you can show that inference this way results in</span>
<span id="cb34-548"><a href="#cb34-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-549"><a href="#cb34-549" aria-hidden="true" tabindex="-1"></a>$$\frac{\theta - \bar{y}}{s / \sqrt{n}} \mid y_1, \dots, y_n \sim t_{n - 1}$$</span>
<span id="cb34-550"><a href="#cb34-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-551"><a href="#cb34-551" aria-hidden="true" tabindex="-1"></a>i.e. a $t$ distribution with $n - 1$ degrees of freedom. This is similar to the</span>
<span id="cb34-552"><a href="#cb34-552" aria-hidden="true" tabindex="-1"></a>sampling distribution of $t$ statistic:</span>
<span id="cb34-553"><a href="#cb34-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-554"><a href="#cb34-554" aria-hidden="true" tabindex="-1"></a>$$\frac{\bar{Y} - \theta}{s / \sqrt{n}} \mid \theta \sim t_{n - 1}$$</span>
<span id="cb34-555"><a href="#cb34-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-556"><a href="#cb34-556" aria-hidden="true" tabindex="-1"></a>but like the Bayesian vs Frequentist</span>
<span id="cb34-557"><a href="#cb34-557" aria-hidden="true" tabindex="-1"></a>confidence intervals discussion in Chapter 3, they are philosophically</span>
<span id="cb34-558"><a href="#cb34-558" aria-hidden="true" tabindex="-1"></a>different. The first describes uncertainty about the true mean conditional on</span>
<span id="cb34-559"><a href="#cb34-559" aria-hidden="true" tabindex="-1"></a>the data, while the second describes uncertainty about the observed sample mean</span>
<span id="cb34-560"><a href="#cb34-560" aria-hidden="true" tabindex="-1"></a>given the true population mean.</span>
<span id="cb34-561"><a href="#cb34-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-562"><a href="#cb34-562" aria-hidden="true" tabindex="-1"></a><span class="fu"># Bias, variance, and mean squared error</span></span>
<span id="cb34-563"><a href="#cb34-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-564"><a href="#cb34-564" aria-hidden="true" tabindex="-1"></a>Now we are diving into the properties of estimators for posterior parameters.</span>
<span id="cb34-565"><a href="#cb34-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-566"><a href="#cb34-566" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; A *point estimator* of an unknown parameter $\theta$ is a function that converts your data into a single element of the parameter space $\Theta$. Good point estimators should hopefully approximate (and *reliably* approximate) the true value of $\theta$; we can formalize these properties as the bias and mean squared error of estimators.</span></span>
<span id="cb34-567"><a href="#cb34-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-568"><a href="#cb34-568" aria-hidden="true" tabindex="-1"></a>In Bayesian analysis, point estimators are usually functions of the posterior </span>
<span id="cb34-569"><a href="#cb34-569" aria-hidden="true" tabindex="-1"></a>distribution of the parameter, such as the expectation.</span>
<span id="cb34-570"><a href="#cb34-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-571"><a href="#cb34-571" aria-hidden="true" tabindex="-1"></a>The point estimator for the posterior of our normal sampling model and a normal</span>
<span id="cb34-572"><a href="#cb34-572" aria-hidden="true" tabindex="-1"></a>prior is (call it $\hat{\theta_b}$)</span>
<span id="cb34-573"><a href="#cb34-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-574"><a href="#cb34-574" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-575"><a href="#cb34-575" aria-hidden="true" tabindex="-1"></a>\hat{\theta_b} &amp;= \frac{\kappa_0 \mu_0 + n\bar{y}}{\kappa_n} <span class="sc">\\</span></span>
<span id="cb34-576"><a href="#cb34-576" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\kappa_0}{\kappa_0 + n} \mu_0 + \frac{n}{\kappa_0 + n}\bar{y}</span>
<span id="cb34-577"><a href="#cb34-577" aria-hidden="true" tabindex="-1"></a>&amp;= w \bar{y} + (1 - w) \mu_0</span>
<span id="cb34-578"><a href="#cb34-578" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-579"><a href="#cb34-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-580"><a href="#cb34-580" aria-hidden="true" tabindex="-1"></a>where $w = \frac{n}{\kappa_0 + n}$.</span>
<span id="cb34-581"><a href="#cb34-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-582"><a href="#cb34-582" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The **Bias** of an estimator $\hat{\theta}$ is $$\text{Bias}(\hat{\theta}) = \mathbb{E}(\hat{\theta}) - \theta$$. If $\text{Bias}(\hat{\theta}) = 0$ we say that $\hat{\theta}$ is an *unbiased* estimator; otherwise we say it is biased.</span></span>
<span id="cb34-583"><a href="#cb34-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-584"><a href="#cb34-584" aria-hidden="true" tabindex="-1"></a>Consider the Bayesian $\hat{\theta_b}$ above versus the standard maximum</span>
<span id="cb34-585"><a href="#cb34-585" aria-hidden="true" tabindex="-1"></a>likelihood estimator, $\hat{\theta_e} = \bar{y}$.</span>
<span id="cb34-586"><a href="#cb34-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-587"><a href="#cb34-587" aria-hidden="true" tabindex="-1"></a>Notice that</span>
<span id="cb34-588"><a href="#cb34-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-589"><a href="#cb34-589" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{Bias}(\hat{\theta_e}) = \mathbb{E}(\hat{\theta_e}) - \theta = 0$, so $\hat{\theta_b}$ is unbiased;</span>
<span id="cb34-590"><a href="#cb34-590" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{Bias}(\hat{\theta_b}) = \mathbb{E}(\hat{\theta_b}) - \theta = w\theta + (1 - w)\mu_0 - \theta$. Notice that the first two terms add up to $\theta$ only if $\mu_0 = \theta$. For all $\mu \neq \theta$, $\hat{\theta_b}$ is biased!</span>
<span id="cb34-591"><a href="#cb34-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-592"><a href="#cb34-592" aria-hidden="true" tabindex="-1"></a>A biased estimator seems undesirable, but can actually be useful in this</span>
<span id="cb34-593"><a href="#cb34-593" aria-hidden="true" tabindex="-1"></a>setting. Imagine "biasing" the estimator *towards the true mean* to obtain a</span>
<span id="cb34-594"><a href="#cb34-594" aria-hidden="true" tabindex="-1"></a>more accurate estimate. Thus it is useful to recall using the Mean Squared Error</span>
<span id="cb34-595"><a href="#cb34-595" aria-hidden="true" tabindex="-1"></a>as another measure of estimator performance, which measures how close an</span>
<span id="cb34-596"><a href="#cb34-596" aria-hidden="true" tabindex="-1"></a>estimator $\hat{\theta}$ will be to the true population parameter $\theta$, on</span>
<span id="cb34-597"><a href="#cb34-597" aria-hidden="true" tabindex="-1"></a>average:</span>
<span id="cb34-598"><a href="#cb34-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-599"><a href="#cb34-599" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The **Mean Squared Error** (MSE) of an estimator $\hat{\theta}$ is $$\text{MSE}(\hat{\theta}) = \text{Var}(\hat{\theta}) + \text{Bias}^2(\hat{\theta})$$</span></span>
<span id="cb34-600"><a href="#cb34-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-601"><a href="#cb34-601" aria-hidden="true" tabindex="-1"></a>Let's compare the MSE of $\hat{\theta_b}$ and $\hat{\theta_e}$:</span>
<span id="cb34-602"><a href="#cb34-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-603"><a href="#cb34-603" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{Var}(\hat{\theta_e}) = \text{Var}(\bar{y}) = \frac{\sigma^2}{n}$</span>
<span id="cb34-604"><a href="#cb34-604" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{Var}(\hat{\theta_b}) = \text{Var}(w \theta_0 + (1 - w)\mu_0) = \text{Var}(w \theta) = w^2 \frac{\sigma^2}{n}$</span>
<span id="cb34-605"><a href="#cb34-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-606"><a href="#cb34-606" aria-hidden="true" tabindex="-1"></a>So</span>
<span id="cb34-607"><a href="#cb34-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-608"><a href="#cb34-608" aria-hidden="true" tabindex="-1"></a>$$\text{MSE}(\hat{\theta_e}) = \text{Var}(\hat{\theta_e}) + \text{Bias}^2(\hat{\theta_e}) = \frac{\sigma^2}{n} + 0$$</span>
<span id="cb34-609"><a href="#cb34-609" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-610"><a href="#cb34-610" aria-hidden="true" tabindex="-1"></a>\text{MSE}(\hat{\theta_b}) &amp;= \text{Var}(\hat{\theta_b}) + \text{Bias}^2(\hat{\theta_b}) <span class="sc">\\</span></span>
<span id="cb34-611"><a href="#cb34-611" aria-hidden="true" tabindex="-1"></a>&amp;= w^2 \frac{\sigma^2}{n} + \left<span class="co">[</span><span class="ot"> w\theta + (1 - w)\mu_0 - \theta \right</span><span class="co">]</span>^2 <span class="sc">\\</span></span>
<span id="cb34-612"><a href="#cb34-612" aria-hidden="true" tabindex="-1"></a>&amp;= w^2 \frac{\sigma^2}{n} + \left<span class="co">[</span><span class="ot"> (1 - w)\mu_0 - (1 - w)\theta \right</span><span class="co">]</span>^2 <span class="sc">\\</span></span>
<span id="cb34-613"><a href="#cb34-613" aria-hidden="true" tabindex="-1"></a>&amp;= w^2 \frac{\sigma^2}{n} + (1 - w)^2(\mu_0 - \theta)^2 <span class="sc">\\</span></span>
<span id="cb34-614"><a href="#cb34-614" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-615"><a href="#cb34-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-616"><a href="#cb34-616" aria-hidden="true" tabindex="-1"></a>Notice that</span>
<span id="cb34-617"><a href="#cb34-617" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-618"><a href="#cb34-618" aria-hidden="true" tabindex="-1"></a>&amp; \text{MSE}(\hat{\theta_b}) &lt; \text{MSE}(\hat{\theta_e}) <span class="sc">\\</span></span>
<span id="cb34-619"><a href="#cb34-619" aria-hidden="true" tabindex="-1"></a>\implies&amp; w^2 \frac{\sigma^2}{n} + (1 - w)^2 (\mu_0 - \theta)^2 &lt; \frac{\sigma^2}{n} <span class="sc">\\</span></span>
<span id="cb34-620"><a href="#cb34-620" aria-hidden="true" tabindex="-1"></a>\implies&amp; (1-w)^2(\mu_0 - \theta)^2 &lt; (1 - w^2) \frac{\sigma^2}{n} <span class="sc">\\</span></span>
<span id="cb34-621"><a href="#cb34-621" aria-hidden="true" tabindex="-1"></a>\implies&amp; (\mu_0 - \theta)^2 &lt; \frac{\sigma^2}{n} \frac{1-w^2}{(1 - w)^2} <span class="sc">\\</span></span>
<span id="cb34-622"><a href="#cb34-622" aria-hidden="true" tabindex="-1"></a>\implies&amp; (\mu_0 - \theta)^2 &lt; \frac{\sigma^2}{n} \frac{(1 - w)(1 + w)}{(1 - w)^2} &amp; \text{Difference of squares} <span class="sc">\\</span></span>
<span id="cb34-623"><a href="#cb34-623" aria-hidden="true" tabindex="-1"></a>\implies&amp; (\mu_0 - \theta)^2 &lt; \frac{\sigma^2}{n} \frac{1 + w}{1 - w} <span class="sc">\\</span></span>
<span id="cb34-624"><a href="#cb34-624" aria-hidden="true" tabindex="-1"></a>\implies&amp; (\mu_0 - \theta)^2 &lt; \frac{\sigma^2}{n} \frac{1 + \frac{n}{\kappa_0 + n}}{1 - \frac{n}{\kappa_0 + n}} &amp; \text{Def. of $w$} <span class="sc">\\</span></span>
<span id="cb34-625"><a href="#cb34-625" aria-hidden="true" tabindex="-1"></a>\implies&amp; (\mu_0 - \theta)^2 &lt; \frac{\sigma^2}{n} \frac{\frac{\kappa_0 + 2n}{\kappa_0 + n}}{\frac{\kappa_0}{\kappa_0 + n}} <span class="sc">\\</span></span>
<span id="cb34-626"><a href="#cb34-626" aria-hidden="true" tabindex="-1"></a>\implies&amp; (\mu_0 - \theta)^2 &lt; \frac{\sigma^2}{n} \frac{\kappa_0 + 2n}{\kappa_0 + n}\frac{\kappa_0 + n}{\kappa_0} <span class="sc">\\</span></span>
<span id="cb34-627"><a href="#cb34-627" aria-hidden="true" tabindex="-1"></a>\implies&amp; (\mu_0 - \theta)^2 &lt; \frac{\sigma^2}{n} \frac{\kappa_0 + 2n}{\kappa_0} <span class="sc">\\</span></span>
<span id="cb34-628"><a href="#cb34-628" aria-hidden="true" tabindex="-1"></a>\implies&amp; (\mu_0 - \theta)^2 &lt; \sigma^2 \frac{\kappa_0 + 2n}{n\kappa_0} <span class="sc">\\</span></span>
<span id="cb34-629"><a href="#cb34-629" aria-hidden="true" tabindex="-1"></a>\implies&amp; (\mu_0 - \theta)^2 &lt; \sigma^2 \left( \frac{\kappa_0}{n\kappa_0} + \frac{2n}{n\kappa_0} \right) <span class="sc">\\</span></span>
<span id="cb34-630"><a href="#cb34-630" aria-hidden="true" tabindex="-1"></a>\implies&amp; (\mu_0 - \theta)^2 &lt; \sigma^2 \left( \frac{1}{n} + \frac{2}{\kappa_0} \right) <span class="sc">\\</span></span>
<span id="cb34-631"><a href="#cb34-631" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-632"><a href="#cb34-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-633"><a href="#cb34-633" aria-hidden="true" tabindex="-1"></a>So the Bayesian estimator has lower mean squared error than the ML estimate as</span>
<span id="cb34-634"><a href="#cb34-634" aria-hidden="true" tabindex="-1"></a>long as values of $\mu_0$ and $\kappa_0$ are picked such that this inequality</span>
<span id="cb34-635"><a href="#cb34-635" aria-hidden="true" tabindex="-1"></a>holds - intutively, if your "guess" about the prior is not far from the truth.</span>
<span id="cb34-636"><a href="#cb34-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-637"><a href="#cb34-637" aria-hidden="true" tabindex="-1"></a><span class="fu"># Prior specification based on expectations</span></span>
<span id="cb34-638"><a href="#cb34-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-639"><a href="#cb34-639" aria-hidden="true" tabindex="-1"></a>The normal model can be shown to be an 2-dimensional exponential model.</span>
<span id="cb34-640"><a href="#cb34-640" aria-hidden="true" tabindex="-1"></a>A $p$-dimensional exponential family model has densities of the form</span>
<span id="cb34-641"><a href="#cb34-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-642"><a href="#cb34-642" aria-hidden="true" tabindex="-1"></a>$$p(y \mid \boldsymbol{\phi}) = h(y) c(\boldsymbol{\phi}) \text{exp}\left( \boldsymbol{\phi}^T \mathbf{t}(y) \right)$$</span>
<span id="cb34-643"><a href="#cb34-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-644"><a href="#cb34-644" aria-hidden="true" tabindex="-1"></a>Recall the normal density:</span>
<span id="cb34-645"><a href="#cb34-645" aria-hidden="true" tabindex="-1"></a>$$p(y \mid \theta, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \text{exp} \left( - \frac{(y - \theta)^2}{2\sigma^2} \right)$$</span>
<span id="cb34-646"><a href="#cb34-646" aria-hidden="true" tabindex="-1"></a>For clarity later, let's expand the quadratic term in the exponential:</span>
<span id="cb34-647"><a href="#cb34-647" aria-hidden="true" tabindex="-1"></a>$$p(y \mid \theta, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \text{exp} \left( - \frac{y^2 - 2\theta y + \theta^2}{2\sigma^2} \right)$$</span>
<span id="cb34-648"><a href="#cb34-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-649"><a href="#cb34-649" aria-hidden="true" tabindex="-1"></a>Given the exponential family parameters</span>
<span id="cb34-650"><a href="#cb34-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-651"><a href="#cb34-651" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{t}(y) = (y, y^2)$</span>
<span id="cb34-652"><a href="#cb34-652" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\boldsymbol{\phi} = (\theta / \sigma^2, -(2\sigma^2)^{-1})$</span>
<span id="cb34-653"><a href="#cb34-653" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$c(\boldsymbol{\phi}) = | \phi_2 |^{1 / 2} \text{exp}\left(\phi_1^2 / (2\phi_2) \right)$</span>
<span id="cb34-654"><a href="#cb34-654" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$h(y) = 1 / \sqrt{\pi}$</span>
<span id="cb34-655"><a href="#cb34-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-656"><a href="#cb34-656" aria-hidden="true" tabindex="-1"></a>You can reconstruct the normal density:</span>
<span id="cb34-657"><a href="#cb34-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-658"><a href="#cb34-658" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-659"><a href="#cb34-659" aria-hidden="true" tabindex="-1"></a>p(y \mid \boldsymbol{\phi}) &amp;= \frac{1}{\sqrt{\pi}} | \phi_2 |^{1/2} \text{exp}\left( \frac{\phi_1^2}{2\phi_2} \right) \text{exp} \left(\begin{pmatrix} y &amp; y^2 \end{pmatrix}  \begin{pmatrix} \theta/\sigma^2 <span class="sc">\\</span> -(2\sigma^2)^{-1} \end{pmatrix}  \right) <span class="sc">\\</span></span>
<span id="cb34-660"><a href="#cb34-660" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{\sqrt{\pi}} (2\sigma^2)^{-1/2} \text{exp}\left( \frac{(\theta / \sigma^2)^2}{-2(2\sigma^2)^{-1}} \right) \text{exp} \left(\begin{pmatrix} y &amp; y^2 \end{pmatrix}  \begin{pmatrix} \theta/\sigma^2 <span class="sc">\\</span> -(2\sigma^2)^{-1} \end{pmatrix} \right) <span class="sc">\\</span></span>
<span id="cb34-661"><a href="#cb34-661" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-662"><a href="#cb34-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-663"><a href="#cb34-663" aria-hidden="true" tabindex="-1"></a>I am not going to do the exact algebra here, but notice that once you combine</span>
<span id="cb34-664"><a href="#cb34-664" aria-hidden="true" tabindex="-1"></a>the exponential terms (and the matrix multiplication in the second exp), there</span>
<span id="cb34-665"><a href="#cb34-665" aria-hidden="true" tabindex="-1"></a>are three separate terms added together. With a common factor of $1 /</span>
<span id="cb34-666"><a href="#cb34-666" aria-hidden="true" tabindex="-1"></a>-2\sigma^2$, those three terms are the   $y^2$, $2 \theta y$, and $\theta^2$ of</span>
<span id="cb34-667"><a href="#cb34-667" aria-hidden="true" tabindex="-1"></a>the expanded normal density above.</span>
<span id="cb34-668"><a href="#cb34-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-669"><a href="#cb34-669" aria-hidden="true" tabindex="-1"></a>With exponential family models, we can now "read off" conjugate priors; for the</span>
<span id="cb34-670"><a href="#cb34-670" aria-hidden="true" tabindex="-1"></a>$p$-dimensional case, the prior is $p(\boldsymbol{\phi} \mid n_0,</span>
<span id="cb34-671"><a href="#cb34-671" aria-hidden="true" tabindex="-1"></a>\mathbf{t}_0) \propto c(\boldsymbol{\phi})^{n_0} \text{exp}(n_0 \mathbf{t}_0^T</span>
<span id="cb34-672"><a href="#cb34-672" aria-hidden="true" tabindex="-1"></a>\boldsymbol{\phi})$. Using the change of variables formula (which seems very</span>
<span id="cb34-673"><a href="#cb34-673" aria-hidden="true" tabindex="-1"></a>complicated), you can reparamaterize the corresponding prior in terms of</span>
<span id="cb34-674"><a href="#cb34-674" aria-hidden="true" tabindex="-1"></a>$\theta$ and $\sigma^2$, which gives a prior that is the product of two priors</span>
<span id="cb34-675"><a href="#cb34-675" aria-hidden="true" tabindex="-1"></a>we had determined previously to be conjugate: the normal and inverse-gamma densities.</span>
<span id="cb34-676"><a href="#cb34-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-677"><a href="#cb34-677" aria-hidden="true" tabindex="-1"></a>There are some more details on the significance of specifying $n_0$ and</span>
<span id="cb34-678"><a href="#cb34-678" aria-hidden="true" tabindex="-1"></a>$\mathbf{t}_0$ that I am skipping, since it essentially mirrors the prior</span>
<span id="cb34-679"><a href="#cb34-679" aria-hidden="true" tabindex="-1"></a>specification advice in the previous sections.</span>
<span id="cb34-680"><a href="#cb34-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-681"><a href="#cb34-681" aria-hidden="true" tabindex="-1"></a><span class="fu"># Normal model for non-normal data</span></span>
<span id="cb34-682"><a href="#cb34-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-683"><a href="#cb34-683" aria-hidden="true" tabindex="-1"></a>Because of the central limit theorem etc., we often use the normal model for</span>
<span id="cb34-684"><a href="#cb34-684" aria-hidden="true" tabindex="-1"></a>non-normal data. This is especially applicable when 1) we are measuring summary</span>
<span id="cb34-685"><a href="#cb34-685" aria-hidden="true" tabindex="-1"></a>statistics of a population, such as the mean, and 2) when we are measuring</span>
<span id="cb34-686"><a href="#cb34-686" aria-hidden="true" tabindex="-1"></a>variables that might be the additive result of many underlying factors, which</span>
<span id="cb34-687"><a href="#cb34-687" aria-hidden="true" tabindex="-1"></a>results in an approximately normal variable.</span>
<span id="cb34-688"><a href="#cb34-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-689"><a href="#cb34-689" aria-hidden="true" tabindex="-1"></a><span class="fu"># Exercises</span></span>
<span id="cb34-690"><a href="#cb34-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-691"><a href="#cb34-691" aria-hidden="true" tabindex="-1"></a><span class="fu">## 5.1</span></span>
<span id="cb34-692"><a href="#cb34-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-695"><a href="#cb34-695" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-696"><a href="#cb34-696" aria-hidden="true" tabindex="-1"></a>school1 <span class="ot">=</span> <span class="fu">scan</span>(<span class="st">'Exercises/school1.dat'</span>)</span>
<span id="cb34-697"><a href="#cb34-697" aria-hidden="true" tabindex="-1"></a>school2 <span class="ot">=</span> <span class="fu">scan</span>(<span class="st">'Exercises/school2.dat'</span>)</span>
<span id="cb34-698"><a href="#cb34-698" aria-hidden="true" tabindex="-1"></a>school3 <span class="ot">=</span> <span class="fu">scan</span>(<span class="st">'Exercises/school3.dat'</span>)</span>
<span id="cb34-699"><a href="#cb34-699" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-700"><a href="#cb34-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-701"><a href="#cb34-701" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb34-702"><a href="#cb34-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-705"><a href="#cb34-705" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-706"><a href="#cb34-706" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb34-707"><a href="#cb34-707" aria-hidden="true" tabindex="-1"></a>s20 <span class="ot">=</span> <span class="dv">4</span></span>
<span id="cb34-708"><a href="#cb34-708" aria-hidden="true" tabindex="-1"></a>k0 <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb34-709"><a href="#cb34-709" aria-hidden="true" tabindex="-1"></a>nu0 <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb34-710"><a href="#cb34-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-711"><a href="#cb34-711" aria-hidden="true" tabindex="-1"></a>params <span class="ot">=</span> <span class="fu">lapply</span>(<span class="fu">list</span>(school1, school2, school3), <span class="cf">function</span>(sdata) {</span>
<span id="cb34-712"><a href="#cb34-712" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Statistics of data</span></span>
<span id="cb34-713"><a href="#cb34-713" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="fu">length</span>(sdata)</span>
<span id="cb34-714"><a href="#cb34-714" aria-hidden="true" tabindex="-1"></a>  ybar <span class="ot">=</span> <span class="fu">mean</span>(sdata)</span>
<span id="cb34-715"><a href="#cb34-715" aria-hidden="true" tabindex="-1"></a>  s2 <span class="ot">=</span> <span class="fu">var</span>(sdata)</span>
<span id="cb34-716"><a href="#cb34-716" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb34-717"><a href="#cb34-717" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Compute posterior values, mun, s2n, kappan, nun</span></span>
<span id="cb34-718"><a href="#cb34-718" aria-hidden="true" tabindex="-1"></a>  kn <span class="ot">=</span> k0 <span class="sc">+</span> n</span>
<span id="cb34-719"><a href="#cb34-719" aria-hidden="true" tabindex="-1"></a>  nun <span class="ot">=</span> nu0 <span class="sc">+</span> n</span>
<span id="cb34-720"><a href="#cb34-720" aria-hidden="true" tabindex="-1"></a>  mun <span class="ot">=</span> (k0 <span class="sc">*</span> mu0 <span class="sc">+</span> n <span class="sc">*</span> ybar) <span class="sc">/</span> kn</span>
<span id="cb34-721"><a href="#cb34-721" aria-hidden="true" tabindex="-1"></a>  s2n <span class="ot">=</span> (<span class="dv">1</span> <span class="sc">/</span> nun) <span class="sc">*</span> (nu0 <span class="sc">*</span> s20 <span class="sc">+</span> (n <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> s2 <span class="sc">+</span> ((k0 <span class="sc">*</span> n) <span class="sc">/</span> kn) <span class="sc">*</span> (ybar <span class="sc">-</span> mu0)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb34-722"><a href="#cb34-722" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb34-723"><a href="#cb34-723" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="st">'mun'</span> <span class="ot">=</span> mun, <span class="st">'s2n'</span> <span class="ot">=</span> s2n, <span class="st">'kn'</span> <span class="ot">=</span> kn, <span class="st">'nun'</span> <span class="ot">=</span> nun)</span>
<span id="cb34-724"><a href="#cb34-724" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb34-725"><a href="#cb34-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-726"><a href="#cb34-726" aria-hidden="true" tabindex="-1"></a>params.df <span class="ot">=</span> <span class="fu">as.data.frame</span>(<span class="fu">rbind</span>(params[[<span class="dv">1</span>]], params[[<span class="dv">2</span>]], params[[<span class="dv">3</span>]]))</span>
<span id="cb34-727"><a href="#cb34-727" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(params.df) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">'school1'</span>, <span class="st">'school2'</span>, <span class="st">'school3'</span>)</span>
<span id="cb34-728"><a href="#cb34-728" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-729"><a href="#cb34-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-732"><a href="#cb34-732" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-733"><a href="#cb34-733" aria-hidden="true" tabindex="-1"></a><span class="co"># 5000 monte carlo samples. Need to estimate \sigma^2 before \theta.</span></span>
<span id="cb34-734"><a href="#cb34-734" aria-hidden="true" tabindex="-1"></a><span class="co"># I can easily do means and confidence intervals of the \sigma^2, but for</span></span>
<span id="cb34-735"><a href="#cb34-735" aria-hidden="true" tabindex="-1"></a><span class="co"># brevity, I will output only \theta</span></span>
<span id="cb34-736"><a href="#cb34-736" aria-hidden="true" tabindex="-1"></a>school1.s2.mc <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">rgamma</span>(<span class="dv">5000</span>, params.df[<span class="dv">1</span>, ]<span class="sc">$</span>nun <span class="sc">/</span> <span class="dv">2</span>, params.df[<span class="dv">1</span>, ]<span class="sc">$</span>s2n <span class="sc">*</span> params.df[<span class="dv">1</span>, ]<span class="sc">$</span>nun <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb34-737"><a href="#cb34-737" aria-hidden="true" tabindex="-1"></a>school1.theta.mc <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">5000</span>, params.df[<span class="dv">1</span>, ]<span class="sc">$</span>mun, <span class="fu">sqrt</span>(school1.s2.mc <span class="sc">/</span> params.df[<span class="dv">1</span>, ]<span class="sc">$</span>kn))</span>
<span id="cb34-738"><a href="#cb34-738" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(school1.theta.mc, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))</span>
<span id="cb34-739"><a href="#cb34-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-740"><a href="#cb34-740" aria-hidden="true" tabindex="-1"></a>school2.s2.mc <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">rgamma</span>(<span class="dv">5000</span>, params.df[<span class="dv">2</span>, ]<span class="sc">$</span>nun <span class="sc">/</span> <span class="dv">2</span>, params.df[<span class="dv">2</span>, ]<span class="sc">$</span>s2n <span class="sc">*</span> params.df[<span class="dv">2</span>, ]<span class="sc">$</span>nun <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb34-741"><a href="#cb34-741" aria-hidden="true" tabindex="-1"></a>school2.theta.mc <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">5000</span>, params.df[<span class="dv">2</span>, ]<span class="sc">$</span>mun, <span class="fu">sqrt</span>(school2.s2.mc <span class="sc">/</span> params.df[<span class="dv">2</span>, ]<span class="sc">$</span>kn))</span>
<span id="cb34-742"><a href="#cb34-742" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(school2.theta.mc, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))</span>
<span id="cb34-743"><a href="#cb34-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-744"><a href="#cb34-744" aria-hidden="true" tabindex="-1"></a>school3.s2.mc <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">rgamma</span>(<span class="dv">5000</span>, params.df[<span class="dv">3</span>, ]<span class="sc">$</span>nun <span class="sc">/</span> <span class="dv">3</span>, params.df[<span class="dv">3</span>, ]<span class="sc">$</span>s2n <span class="sc">*</span> params.df[<span class="dv">3</span>, ]<span class="sc">$</span>nun <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb34-745"><a href="#cb34-745" aria-hidden="true" tabindex="-1"></a>school3.theta.mc <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">5000</span>, params.df[<span class="dv">3</span>, ]<span class="sc">$</span>mun, <span class="fu">sqrt</span>(school3.s2.mc <span class="sc">/</span> params.df[<span class="dv">3</span>, ]<span class="sc">$</span>kn))</span>
<span id="cb34-746"><a href="#cb34-746" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(school3.theta.mc, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))</span>
<span id="cb34-747"><a href="#cb34-747" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-748"><a href="#cb34-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-749"><a href="#cb34-749" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb34-750"><a href="#cb34-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-753"><a href="#cb34-753" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-754"><a href="#cb34-754" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(combinat)</span>
<span id="cb34-755"><a href="#cb34-755" aria-hidden="true" tabindex="-1"></a>school.theta.mc <span class="ot">=</span> <span class="fu">list</span>(school1.theta.mc, school2.theta.mc, school3.theta.mc)</span>
<span id="cb34-756"><a href="#cb34-756" aria-hidden="true" tabindex="-1"></a>perms <span class="ot">=</span> <span class="fu">permn</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>)</span>
<span id="cb34-757"><a href="#cb34-757" aria-hidden="true" tabindex="-1"></a>theta.lt.probs <span class="ot">=</span> <span class="fu">lapply</span>(perms, <span class="cf">function</span>(perm) {</span>
<span id="cb34-758"><a href="#cb34-758" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This is a vector e.g. c(1, 3, 2)</span></span>
<span id="cb34-759"><a href="#cb34-759" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(school.theta.mc[[perm[<span class="dv">1</span>]]] <span class="sc">&lt;</span> school.theta.mc[[perm[<span class="dv">2</span>]]] <span class="sc">&amp;</span></span>
<span id="cb34-760"><a href="#cb34-760" aria-hidden="true" tabindex="-1"></a>         school.theta.mc[[perm[<span class="dv">2</span>]]] <span class="sc">&lt;</span> school.theta.mc[[perm[<span class="dv">3</span>]]])</span>
<span id="cb34-761"><a href="#cb34-761" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb34-762"><a href="#cb34-762" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(theta.lt.probs) <span class="ot">=</span> <span class="fu">sapply</span>(perms, <span class="cf">function</span>(v) <span class="fu">paste</span>(v, <span class="at">collapse =</span><span class="st">' &lt; '</span>))</span>
<span id="cb34-763"><a href="#cb34-763" aria-hidden="true" tabindex="-1"></a>theta.lt.probs.stacked <span class="ot">=</span> <span class="fu">stack</span>(theta.lt.probs)[, <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>)] <span class="co"># Reverse stack order</span></span>
<span id="cb34-764"><a href="#cb34-764" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(theta.lt.probs.stacked, <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">'inequality'</span>, <span class="st">'prob'</span>))</span>
<span id="cb34-765"><a href="#cb34-765" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-766"><a href="#cb34-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-767"><a href="#cb34-767" aria-hidden="true" tabindex="-1"></a><span class="fu">### c</span></span>
<span id="cb34-768"><a href="#cb34-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-771"><a href="#cb34-771" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-772"><a href="#cb34-772" aria-hidden="true" tabindex="-1"></a>school.s2.mc <span class="ot">=</span> <span class="fu">list</span>(school1.s2.mc, school2.s2.mc, school3.s2.mc)</span>
<span id="cb34-773"><a href="#cb34-773" aria-hidden="true" tabindex="-1"></a>school.y.mc <span class="ot">=</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="cf">function</span>(i) {</span>
<span id="cb34-774"><a href="#cb34-774" aria-hidden="true" tabindex="-1"></a>  this.s2 <span class="ot">=</span> school.s2.mc[[i]]</span>
<span id="cb34-775"><a href="#cb34-775" aria-hidden="true" tabindex="-1"></a>  this.theta <span class="ot">=</span> school.theta.mc[[i]]</span>
<span id="cb34-776"><a href="#cb34-776" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rnorm</span>(<span class="dv">5000</span>, this.theta, <span class="fu">sqrt</span>(this.s2))</span>
<span id="cb34-777"><a href="#cb34-777" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb34-778"><a href="#cb34-778" aria-hidden="true" tabindex="-1"></a>y.lt.probs <span class="ot">=</span> <span class="fu">lapply</span>(perms, <span class="cf">function</span>(perm) {</span>
<span id="cb34-779"><a href="#cb34-779" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This is a vector e.g. c(1, 3, 2)</span></span>
<span id="cb34-780"><a href="#cb34-780" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(school.y.mc[[perm[<span class="dv">1</span>]]] <span class="sc">&lt;</span> school.y.mc[[perm[<span class="dv">2</span>]]] <span class="sc">&amp;</span></span>
<span id="cb34-781"><a href="#cb34-781" aria-hidden="true" tabindex="-1"></a>         school.y.mc[[perm[<span class="dv">2</span>]]] <span class="sc">&lt;</span> school.y.mc[[perm[<span class="dv">3</span>]]])</span>
<span id="cb34-782"><a href="#cb34-782" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb34-783"><a href="#cb34-783" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(y.lt.probs) <span class="ot">=</span> <span class="fu">sapply</span>(perms, <span class="cf">function</span>(v) <span class="fu">paste</span>(v, <span class="at">collapse =</span><span class="st">' &lt; '</span>))</span>
<span id="cb34-784"><a href="#cb34-784" aria-hidden="true" tabindex="-1"></a>y.lt.probs.stacked <span class="ot">=</span> <span class="fu">stack</span>(y.lt.probs)[, <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>)] <span class="co"># Reverse stack order</span></span>
<span id="cb34-785"><a href="#cb34-785" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(y.lt.probs.stacked, <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">'inequality'</span>, <span class="st">'prob'</span>))</span>
<span id="cb34-786"><a href="#cb34-786" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-787"><a href="#cb34-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-788"><a href="#cb34-788" aria-hidden="true" tabindex="-1"></a><span class="fu">### d</span></span>
<span id="cb34-789"><a href="#cb34-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-792"><a href="#cb34-792" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-793"><a href="#cb34-793" aria-hidden="true" tabindex="-1"></a>theta1.big.prob <span class="ot">=</span> <span class="fu">mean</span>(school.theta.mc[[<span class="dv">1</span>]] <span class="sc">&gt;</span> school.theta.mc[[<span class="dv">2</span>]] <span class="sc">&amp;</span> school.theta.mc[[<span class="dv">1</span>]] <span class="sc">&gt;</span> school.theta.mc[[<span class="dv">3</span>]])</span>
<span id="cb34-794"><a href="#cb34-794" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(theta1.big.prob)</span>
<span id="cb34-795"><a href="#cb34-795" aria-hidden="true" tabindex="-1"></a>y1.big.prob <span class="ot">=</span> <span class="fu">mean</span>(school.y.mc[[<span class="dv">1</span>]] <span class="sc">&gt;</span> school.y.mc[[<span class="dv">2</span>]] <span class="sc">&amp;</span> school.y.mc[[<span class="dv">1</span>]] <span class="sc">&gt;</span> school.y.mc[[<span class="dv">3</span>]])</span>
<span id="cb34-796"><a href="#cb34-796" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(y1.big.prob)</span>
<span id="cb34-797"><a href="#cb34-797" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-798"><a href="#cb34-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-799"><a href="#cb34-799" aria-hidden="true" tabindex="-1"></a><span class="fu">## 5.2</span></span>
<span id="cb34-800"><a href="#cb34-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-803"><a href="#cb34-803" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb34-804"><a href="#cb34-804" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="dv">75</span></span>
<span id="cb34-805"><a href="#cb34-805" aria-hidden="true" tabindex="-1"></a>s20 <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb34-806"><a href="#cb34-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-807"><a href="#cb34-807" aria-hidden="true" tabindex="-1"></a>n.a <span class="ot">=</span> n.b <span class="ot">=</span> <span class="dv">16</span></span>
<span id="cb34-808"><a href="#cb34-808" aria-hidden="true" tabindex="-1"></a>ybar.a <span class="ot">=</span> <span class="fl">75.2</span></span>
<span id="cb34-809"><a href="#cb34-809" aria-hidden="true" tabindex="-1"></a>s2.a <span class="ot">=</span> <span class="fl">7.3</span><span class="sc">^</span><span class="dv">2</span></span>
<span id="cb34-810"><a href="#cb34-810" aria-hidden="true" tabindex="-1"></a>ybar.b <span class="ot">=</span> <span class="fl">77.5</span></span>
<span id="cb34-811"><a href="#cb34-811" aria-hidden="true" tabindex="-1"></a>s2.b <span class="ot">=</span> <span class="fl">8.1</span><span class="sc">^</span><span class="dv">2</span></span>
<span id="cb34-812"><a href="#cb34-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-813"><a href="#cb34-813" aria-hidden="true" tabindex="-1"></a>k0nu0 <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>)</span>
<span id="cb34-814"><a href="#cb34-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-815"><a href="#cb34-815" aria-hidden="true" tabindex="-1"></a>prob <span class="ot">=</span> <span class="fu">sapply</span>(k0nu0, <span class="cf">function</span>(p) {</span>
<span id="cb34-816"><a href="#cb34-816" aria-hidden="true" tabindex="-1"></a>  <span class="co"># p is the common parameter for k0 and nu0</span></span>
<span id="cb34-817"><a href="#cb34-817" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb34-818"><a href="#cb34-818" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate posterior parameters</span></span>
<span id="cb34-819"><a href="#cb34-819" aria-hidden="true" tabindex="-1"></a>  kn.a <span class="ot">=</span> p <span class="sc">+</span> n.a</span>
<span id="cb34-820"><a href="#cb34-820" aria-hidden="true" tabindex="-1"></a>  nun.a <span class="ot">=</span> p <span class="sc">+</span> n.a</span>
<span id="cb34-821"><a href="#cb34-821" aria-hidden="true" tabindex="-1"></a>  mun.a <span class="ot">=</span> (p <span class="sc">*</span> mu0 <span class="sc">+</span> n.a <span class="sc">*</span> ybar.a) <span class="sc">/</span> kn.a</span>
<span id="cb34-822"><a href="#cb34-822" aria-hidden="true" tabindex="-1"></a>  s2n.a <span class="ot">=</span> (<span class="dv">1</span> <span class="sc">/</span> nun.a) <span class="sc">*</span> (p <span class="sc">*</span> s20 <span class="sc">+</span> (n.a <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> s2.a <span class="sc">+</span> ((p <span class="sc">*</span> n.a) <span class="sc">/</span> kn.a) <span class="sc">*</span> (ybar.a <span class="sc">-</span> mu0)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb34-823"><a href="#cb34-823" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb34-824"><a href="#cb34-824" aria-hidden="true" tabindex="-1"></a>  s2.a.mc <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">rgamma</span>(<span class="dv">10000</span>, nun.a <span class="sc">/</span> <span class="dv">2</span>, s2n.a <span class="sc">*</span> nun.a <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb34-825"><a href="#cb34-825" aria-hidden="true" tabindex="-1"></a>  theta.a.mc <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>, mun.a, <span class="fu">sqrt</span>(s2.a.mc<span class="sc">/</span>kn.a))</span>
<span id="cb34-826"><a href="#cb34-826" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb34-827"><a href="#cb34-827" aria-hidden="true" tabindex="-1"></a>  kn.b <span class="ot">=</span> p <span class="sc">+</span> n.b</span>
<span id="cb34-828"><a href="#cb34-828" aria-hidden="true" tabindex="-1"></a>  nun.b <span class="ot">=</span> p <span class="sc">+</span> n.b</span>
<span id="cb34-829"><a href="#cb34-829" aria-hidden="true" tabindex="-1"></a>  mun.b <span class="ot">=</span> (p <span class="sc">*</span> mu0 <span class="sc">+</span> n.b <span class="sc">*</span> ybar.b) <span class="sc">/</span> kn.b</span>
<span id="cb34-830"><a href="#cb34-830" aria-hidden="true" tabindex="-1"></a>  s2n.b <span class="ot">=</span> (<span class="dv">1</span> <span class="sc">/</span> nun.b) <span class="sc">*</span> (p <span class="sc">*</span> s20 <span class="sc">+</span> (n.b <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> s2.b <span class="sc">+</span> ((p <span class="sc">*</span> n.b) <span class="sc">/</span> kn.b) <span class="sc">*</span> (ybar.b <span class="sc">-</span> mu0)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb34-831"><a href="#cb34-831" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb34-832"><a href="#cb34-832" aria-hidden="true" tabindex="-1"></a>  s2.b.mc <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">rgamma</span>(<span class="dv">10000</span>, nun.b <span class="sc">/</span> <span class="dv">2</span>, s2n.b <span class="sc">*</span> nun.b <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb34-833"><a href="#cb34-833" aria-hidden="true" tabindex="-1"></a>  theta.b.mc <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">10000</span>, mun.b, <span class="fu">sqrt</span>(s2.b.mc<span class="sc">/</span>kn.b))</span>
<span id="cb34-834"><a href="#cb34-834" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb34-835"><a href="#cb34-835" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(theta.a.mc <span class="sc">&lt;</span> theta.b.mc)</span>
<span id="cb34-836"><a href="#cb34-836" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb34-837"><a href="#cb34-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-838"><a href="#cb34-838" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(k0nu0, prob, <span class="at">geom =</span> <span class="fu">c</span>(<span class="st">'line'</span>, <span class="st">'point'</span>))</span>
<span id="cb34-839"><a href="#cb34-839" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb34-840"><a href="#cb34-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-841"><a href="#cb34-841" aria-hidden="true" tabindex="-1"></a>In general, there is weak evidence that $\theta_A &lt; \theta_B$. Depending on the</span>
<span id="cb34-842"><a href="#cb34-842" aria-hidden="true" tabindex="-1"></a>strength of a person's confidence in the prior, as quantified by a "prior sample</span>
<span id="cb34-843"><a href="#cb34-843" aria-hidden="true" tabindex="-1"></a>size" $\nu_0 = \kappa_0$, the posterior probability starts at ~0.58 and declines</span>
<span id="cb34-844"><a href="#cb34-844" aria-hidden="true" tabindex="-1"></a>as strength increases. However, it takes a *very* strong prior belief for the</span>
<span id="cb34-845"><a href="#cb34-845" aria-hidden="true" tabindex="-1"></a>probability to dip below 0.50.</span>
<span id="cb34-846"><a href="#cb34-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-847"><a href="#cb34-847" aria-hidden="true" tabindex="-1"></a><span class="fu">## 5.3</span></span>
<span id="cb34-848"><a href="#cb34-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-849"><a href="#cb34-849" aria-hidden="true" tabindex="-1"></a>I'll derive $p(\sigma^2 \mid y_1, \dots, y_n)$:</span>
<span id="cb34-850"><a href="#cb34-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-851"><a href="#cb34-851" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-852"><a href="#cb34-852" aria-hidden="true" tabindex="-1"></a>p(\sigma^2 \mid y_1, \dots, y_n) &amp;\propto p(\sigma^2)p(y_1, \dots, y_n \mid \sigma^2) <span class="sc">\\</span></span>
<span id="cb34-853"><a href="#cb34-853" aria-hidden="true" tabindex="-1"></a>&amp;= p(\sigma^2) \times \int p(y_1, \dots, y_n \mid \theta, \sigma^2) p(\theta \mid \sigma^2) \; d\theta <span class="sc">\\</span></span>
<span id="cb34-854"><a href="#cb34-854" aria-hidden="true" tabindex="-1"></a>&amp;= \text{dinvgamma}(\sigma^2, \nu_0 / 2, \nu_0 \sigma_0^2 / 2) \int \left<span class="co">[</span><span class="ot"> \prod_{i=1}^n \text{dnorm}(y_i, \theta, \sigma^2) \right</span><span class="co">]</span> \times \text{dnorm}(\theta, \mu_0, \sigma^2 / \kappa_0)\; d\theta <span class="sc">\\</span></span>
<span id="cb34-855"><a href="#cb34-855" aria-hidden="true" tabindex="-1"></a>&amp;= \text{dinvgamma}(\sigma^2, \nu_0 / 2, \nu_0 \sigma_0^2 / 2) \; \times <span class="sc">\\</span> &amp;\quad \int (2\pi</span>
<span id="cb34-856"><a href="#cb34-856" aria-hidden="true" tabindex="-1"></a>\sigma^2)^{-n/2} \text{exp}\left( -\frac{1}{2} \sum \frac{(y_i -</span>
<span id="cb34-857"><a href="#cb34-857" aria-hidden="true" tabindex="-1"></a>\theta)^2}{\sigma^2} \right) \times \frac{1}{\sqrt{2\pi\sigma^2 / \kappa_0}}</span>
<span id="cb34-858"><a href="#cb34-858" aria-hidden="true" tabindex="-1"></a>\text{exp}\left(-\frac{1}{2} \frac{(\theta - \mu_0)^2}{\sigma^2 / \kappa_0}\right) \; d\theta <span class="sc">\\</span></span>
<span id="cb34-859"><a href="#cb34-859" aria-hidden="true" tabindex="-1"></a>&amp;\propto \text{dinvgamma}(\sigma^2, \nu_0 / 2, \nu_0 \sigma_0^2 / 2) \; \times <span class="sc">\\</span> &amp;\quad \sigma^{2(-(n + 1)/2)} \int \text{exp}\left( -\frac{1}{2} \sum \frac{(y_i -</span>
<span id="cb34-860"><a href="#cb34-860" aria-hidden="true" tabindex="-1"></a>\theta)^2}{\sigma^2} \right) \times </span>
<span id="cb34-861"><a href="#cb34-861" aria-hidden="true" tabindex="-1"></a>\text{exp}\left(-\frac{1}{2} \frac{(\theta - \mu_0)^2}{\sigma^2 / \kappa_0}\right) \; d\theta <span class="sc">\\</span></span>
<span id="cb34-862"><a href="#cb34-862" aria-hidden="true" tabindex="-1"></a>&amp;\propto \dots</span>
<span id="cb34-863"><a href="#cb34-863" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-864"><a href="#cb34-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-865"><a href="#cb34-865" aria-hidden="true" tabindex="-1"></a>This is apparently non-trivial, and requires expanding the quadratic terms in the $\text{exp}$ terms. I'll skip this for now.</span>
<span id="cb34-866"><a href="#cb34-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-867"><a href="#cb34-867" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--</span></span>
<span id="cb34-868"><a href="#cb34-868" aria-hidden="true" tabindex="-1"></a><span class="co">The marginal posterior distribution on $\theta^2$ can be calculated using Bayes'</span></span>
<span id="cb34-869"><a href="#cb34-869" aria-hidden="true" tabindex="-1"></a><span class="co">rule above, or by simply plugging in the above posterior into</span></span>
<span id="cb34-870"><a href="#cb34-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-871"><a href="#cb34-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-872"><a href="#cb34-872" aria-hidden="true" tabindex="-1"></a><span class="co">\begin{align}</span></span>
<span id="cb34-873"><a href="#cb34-873" aria-hidden="true" tabindex="-1"></a><span class="co">p(\theta \mid y_1, \dots, y_n) &amp;= \int p(\theta, \sigma^2 \mid y_1, \dots, y_n) \; d\sigma \\</span></span>
<span id="cb34-874"><a href="#cb34-874" aria-hidden="true" tabindex="-1"></a><span class="co">&amp;= \int \text{dnorm}(\theta, )</span></span>
<span id="cb34-875"><a href="#cb34-875" aria-hidden="true" tabindex="-1"></a><span class="co">&amp;= \int p(\theta \mid \sigma^2, y_1, \dots, y_n) \times p(\sigma^2 \mid y_1, \dots, y_n) \; d\sigma \\</span></span>
<span id="cb34-876"><a href="#cb34-876" aria-hidden="true" tabindex="-1"></a><span class="co">&amp;= \int \left[ \frac{1}{\sqrt{2\pi \tau_n^2</span><span class="re">}}}</span><span class="co"> \text{exp}(-\frac{1}{2} \frac{(\theta - \mu_n)^2}{\tau_n^2})  \right] \times \left[ \right] \\</span></span>
<span id="cb34-877"><a href="#cb34-877" aria-hidden="true" tabindex="-1"></a><span class="co">\end{align}</span></span>
<span id="cb34-878"><a href="#cb34-878" aria-hidden="true" tabindex="-1"></a><span class="co">--&gt;</span></span>
<span id="cb34-879"><a href="#cb34-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-880"><a href="#cb34-880" aria-hidden="true" tabindex="-1"></a><span class="fu">## 5.4</span></span>
<span id="cb34-881"><a href="#cb34-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-882"><a href="#cb34-882" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb34-883"><a href="#cb34-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-884"><a href="#cb34-884" aria-hidden="true" tabindex="-1"></a>The log-likelihood function $\ell$ is</span>
<span id="cb34-885"><a href="#cb34-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-886"><a href="#cb34-886" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-887"><a href="#cb34-887" aria-hidden="true" tabindex="-1"></a>\ell(Y \mid \theta, \sigma^2) &amp;= \log \left<span class="co">[</span><span class="ot"> \frac{1}{\sqrt{2\pi\sigma^2}} \text{exp}\left(-\frac{1}{2}\sum_{i=1}^n \frac{(y_i - \theta)^2}{\sigma^2} \right) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb34-888"><a href="#cb34-888" aria-hidden="true" tabindex="-1"></a>&amp;= -\frac{n}{2} \log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \theta)^2 <span class="sc">\\</span></span>
<span id="cb34-889"><a href="#cb34-889" aria-hidden="true" tabindex="-1"></a>&amp;= -\frac{n}{2} \log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n y_i^2 -2\theta y_i + \theta^2 <span class="sc">\\</span></span>
<span id="cb34-890"><a href="#cb34-890" aria-hidden="true" tabindex="-1"></a>&amp;= -\frac{n}{2} \log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2} \left(  \sum y_i^2 - 2\theta \sum y_i + n \theta^2 \right)</span>
<span id="cb34-891"><a href="#cb34-891" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-892"><a href="#cb34-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-893"><a href="#cb34-893" aria-hidden="true" tabindex="-1"></a>The first derivatives are</span>
<span id="cb34-894"><a href="#cb34-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-895"><a href="#cb34-895" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-896"><a href="#cb34-896" aria-hidden="true" tabindex="-1"></a>\ell_\theta(Y \mid \theta, \sigma) &amp;= \frac{\sum y_i - n\theta}{\sigma^2} <span class="sc">\\</span></span>
<span id="cb34-897"><a href="#cb34-897" aria-hidden="true" tabindex="-1"></a>\ell_{\sigma^2}(Y \mid \theta, \sigma) &amp;= -\frac{n}{2\sigma^2} + \frac{\sum y_i^2}{2(\sigma^2)^2} - \frac{2\theta \sum y_i}{2(\sigma^2)^2} + \frac{n\theta^2}{2(\sigma^2)^2} <span class="sc">\\</span></span>
<span id="cb34-898"><a href="#cb34-898" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-899"><a href="#cb34-899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-900"><a href="#cb34-900" aria-hidden="true" tabindex="-1"></a>So the $2 \times 2$ matrix $I(\theta, \sigma^2)$ is (expectations wrt $Y$)</span>
<span id="cb34-901"><a href="#cb34-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-902"><a href="#cb34-902" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-903"><a href="#cb34-903" aria-hidden="true" tabindex="-1"></a>I(\theta, \sigma^2) &amp;=</span>
<span id="cb34-904"><a href="#cb34-904" aria-hidden="true" tabindex="-1"></a>\begin{bmatrix}</span>
<span id="cb34-905"><a href="#cb34-905" aria-hidden="true" tabindex="-1"></a>-\mathbb{E}\left( \ell_{\theta\theta}(Y \mid \theta, \sigma) \right) &amp; -\mathbb{E}\left( \ell_{\theta\sigma^2}(Y \mid \theta, \sigma) \right) <span class="sc">\\</span></span>
<span id="cb34-906"><a href="#cb34-906" aria-hidden="true" tabindex="-1"></a>-\mathbb{E}\left( \ell_{\sigma^2\theta}(Y \mid \theta, \sigma) \right) &amp; -\mathbb{E}\left( \ell_{\sigma^2\sigma^2}(Y \mid \theta, \sigma) \right)<span class="sc">\\</span></span>
<span id="cb34-907"><a href="#cb34-907" aria-hidden="true" tabindex="-1"></a>\end{bmatrix} <span class="sc">\\</span></span>
<span id="cb34-908"><a href="#cb34-908" aria-hidden="true" tabindex="-1"></a>&amp;= \begin{bmatrix}</span>
<span id="cb34-909"><a href="#cb34-909" aria-hidden="true" tabindex="-1"></a>-\mathbb{E}\left( -\frac{n}{\sigma^2} \right) &amp;</span>
<span id="cb34-910"><a href="#cb34-910" aria-hidden="true" tabindex="-1"></a>-\mathbb{E}\left( -\frac{\sum y_i - n\theta}{(\sigma^2)^2}\right) <span class="sc">\\</span></span>
<span id="cb34-911"><a href="#cb34-911" aria-hidden="true" tabindex="-1"></a>-\mathbb{E}\left( -\frac{\sum y_i - n\theta}{(\sigma^2)^2}\right) &amp;</span>
<span id="cb34-912"><a href="#cb34-912" aria-hidden="true" tabindex="-1"></a>-\mathbb{E}\left( \frac{n}{2(\sigma^2)^2} - \frac{\sum y_i^2 - 2\theta\sum y_i + n\theta^2}{(\sigma^2)^3}\right) <span class="sc">\\</span></span>
<span id="cb34-913"><a href="#cb34-913" aria-hidden="true" tabindex="-1"></a>\end{bmatrix} <span class="sc">\\</span></span>
<span id="cb34-914"><a href="#cb34-914" aria-hidden="true" tabindex="-1"></a>&amp;= \begin{bmatrix}</span>
<span id="cb34-915"><a href="#cb34-915" aria-hidden="true" tabindex="-1"></a>\frac{n}{\sigma^2} &amp;</span>
<span id="cb34-916"><a href="#cb34-916" aria-hidden="true" tabindex="-1"></a>-\left( -\frac{n\theta - n\theta}{(\sigma^2)^2} \right) <span class="sc">\\</span></span>
<span id="cb34-917"><a href="#cb34-917" aria-hidden="true" tabindex="-1"></a>-\left( -\frac{n\theta - n\theta}{(\sigma^2)^2} \right) &amp;</span>
<span id="cb34-918"><a href="#cb34-918" aria-hidden="true" tabindex="-1"></a>-\frac{n}{2(\sigma^2)^2} + \frac{\mathbb{E}(\sum y_i^2 - 2\theta\sum y_i + n\theta^2)}{(\sigma^2)^3} <span class="sc">\\</span></span>
<span id="cb34-919"><a href="#cb34-919" aria-hidden="true" tabindex="-1"></a>\end{bmatrix} <span class="sc">\\</span></span>
<span id="cb34-920"><a href="#cb34-920" aria-hidden="true" tabindex="-1"></a>&amp;= \begin{bmatrix}</span>
<span id="cb34-921"><a href="#cb34-921" aria-hidden="true" tabindex="-1"></a>\frac{n}{\sigma^2} &amp;</span>
<span id="cb34-922"><a href="#cb34-922" aria-hidden="true" tabindex="-1"></a>0 <span class="sc">\\</span></span>
<span id="cb34-923"><a href="#cb34-923" aria-hidden="true" tabindex="-1"></a>0 &amp;</span>
<span id="cb34-924"><a href="#cb34-924" aria-hidden="true" tabindex="-1"></a>\frac{n}{2(\sigma^2)^2} <span class="sc">\\</span></span>
<span id="cb34-925"><a href="#cb34-925" aria-hidden="true" tabindex="-1"></a>\end{bmatrix}</span>
<span id="cb34-926"><a href="#cb34-926" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-927"><a href="#cb34-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-928"><a href="#cb34-928" aria-hidden="true" tabindex="-1"></a>The last one is derived as follows:</span>
<span id="cb34-929"><a href="#cb34-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-930"><a href="#cb34-930" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-931"><a href="#cb34-931" aria-hidden="true" tabindex="-1"></a>-\frac{n}{2(\sigma^2)^2} + \frac{\mathbb{E}(\sum y_i^2 - 2\theta\sum y_i + n\theta^2)}{(\sigma^2)^3} &amp;= -\frac{n}{2(\sigma^2)^2} + \frac{n(\theta^2 + \sigma^2) - 2\theta(n\theta) + n\theta^2}{(\sigma^2)^3} <span class="sc">\\</span></span>
<span id="cb34-932"><a href="#cb34-932" aria-hidden="true" tabindex="-1"></a>&amp;= -\frac{n}{2(\sigma^2)^2} + \frac{n\sigma^2}{(\sigma^2)^3} <span class="sc">\\</span></span>
<span id="cb34-933"><a href="#cb34-933" aria-hidden="true" tabindex="-1"></a>&amp;= -\frac{n}{2(\sigma^2)^2} + \frac{2n}{2(\sigma^2)^2} <span class="sc">\\</span></span>
<span id="cb34-934"><a href="#cb34-934" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{n}{2(\sigma^2)^2}</span>
<span id="cb34-935"><a href="#cb34-935" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-936"><a href="#cb34-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-937"><a href="#cb34-937" aria-hidden="true" tabindex="-1"></a>So Jeffrey's prior is</span>
<span id="cb34-938"><a href="#cb34-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-939"><a href="#cb34-939" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-940"><a href="#cb34-940" aria-hidden="true" tabindex="-1"></a>p_J(\theta, \sigma^2) &amp;\propto \sqrt{|I(\theta, \sigma^2)|} <span class="sc">\\</span></span>
<span id="cb34-941"><a href="#cb34-941" aria-hidden="true" tabindex="-1"></a>&amp;= \sqrt{\frac{n^2}{2(\sigma^2)^3}} <span class="sc">\\</span></span>
<span id="cb34-942"><a href="#cb34-942" aria-hidden="true" tabindex="-1"></a>&amp;\propto \sqrt{\frac{n^2}{2}} \sqrt{\frac{1}{(\sigma^2)^3}} <span class="sc">\\</span></span>
<span id="cb34-943"><a href="#cb34-943" aria-hidden="true" tabindex="-1"></a>&amp;\propto (\sigma^2)^{-3/2}.</span>
<span id="cb34-944"><a href="#cb34-944" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-945"><a href="#cb34-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-946"><a href="#cb34-946" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb34-947"><a href="#cb34-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-948"><a href="#cb34-948" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb34-949"><a href="#cb34-949" aria-hidden="true" tabindex="-1"></a>p_J(\theta, \sigma^2 \mid \mathbf{y}) &amp;\propto p_J(\theta, \sigma^2) p(\mathbf{y} \mid \theta, \sigma^2) <span class="sc">\\</span></span>
<span id="cb34-950"><a href="#cb34-950" aria-hidden="true" tabindex="-1"></a>&amp;\propto (\sigma^2)^{-3/2} \times (\sigma^2)^{-n/2} \text{exp}\left(-\frac{1}{2\sigma^2} \sum_{i = 1}^n (y_i - \theta)^2 \right) <span class="sc">\\</span></span>
<span id="cb34-951"><a href="#cb34-951" aria-hidden="true" tabindex="-1"></a>&amp;= (\sigma^2)^{-(3 + n)/2} \text{exp}\left(-\frac{1}{2\sigma^2} \sum_{i = 1}^n (y_i - \theta)^2 \right)</span>
<span id="cb34-952"><a href="#cb34-952" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb34-953"><a href="#cb34-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-954"><a href="#cb34-954" aria-hidden="true" tabindex="-1"></a>Yes, since there is some normalizing constant that results in the integral over</span>
<span id="cb34-955"><a href="#cb34-955" aria-hidden="true" tabindex="-1"></a>$p_J$ being equal to 1.</span>
<span id="cb34-956"><a href="#cb34-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-957"><a href="#cb34-957" aria-hidden="true" tabindex="-1"></a>After some poking around, this is a [normal-inverse-chi-squared </span>
<span id="cb34-958"><a href="#cb34-958" aria-hidden="true" tabindex="-1"></a>distribution](https://www2.stat.duke.edu/courses/Fall10/sta114/notes15.pdf).</span>
<span id="cb34-959"><a href="#cb34-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-960"><a href="#cb34-960" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--</span></span>
<span id="cb34-961"><a href="#cb34-961" aria-hidden="true" tabindex="-1"></a><span class="co">For random variables $W, V$, the density function is</span></span>
<span id="cb34-962"><a href="#cb34-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-963"><a href="#cb34-963" aria-hidden="true" tabindex="-1"></a><span class="co">\begin{align}</span></span>
<span id="cb34-964"><a href="#cb34-964" aria-hidden="true" tabindex="-1"></a><span class="co">f(w, v; m, k, r, s) &amp;\propto v^{-(r + 3)/2} \text{exp}\left(-\frac{k(w-m)^2 +</span></span>
<span id="cb34-965"><a href="#cb34-965" aria-hidden="true" tabindex="-1"></a><span class="co">rs}{2v} \right)</span></span>
<span id="cb34-966"><a href="#cb34-966" aria-hidden="true" tabindex="-1"></a><span class="co">\end{align}</span></span>
<span id="cb34-967"><a href="#cb34-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-968"><a href="#cb34-968" aria-hidden="true" tabindex="-1"></a><span class="co">For the random variables $\theta, \sigma^2$, $r = n$ is obvious from $p_J$. The</span></span>
<span id="cb34-969"><a href="#cb34-969" aria-hidden="true" tabindex="-1"></a><span class="co">other parameters can be seen by expanding the exponential, where I borrow the</span></span>
<span id="cb34-970"><a href="#cb34-970" aria-hidden="true" tabindex="-1"></a><span class="co">normal likelihood reparameterization trick from</span></span>
<span id="cb34-971"><a href="#cb34-971" aria-hidden="true" tabindex="-1"></a><span class="co">[here](http://faculty.washington.edu/ezivot/econ583/mleLectures.pdf) (page 3):</span></span>
<span id="cb34-972"><a href="#cb34-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-973"><a href="#cb34-973" aria-hidden="true" tabindex="-1"></a><span class="co">\begin{align}</span></span>
<span id="cb34-974"><a href="#cb34-974" aria-hidden="true" tabindex="-1"></a><span class="co">\text{exp}\left(-\frac{1}{2\sigma^2} \sum_{i = 1}^n (y_i - \theta)^2 \right) &amp;=</span></span>
<span id="cb34-975"><a href="#cb34-975" aria-hidden="true" tabindex="-1"></a><span class="co">\text{exp}\left(-\frac{1}{2\sigma^2} \sum_{i = 1}^n \left(y_i - \bar{y} + \bar{y} - \theta \right) \right) \\</span></span>
<span id="cb34-976"><a href="#cb34-976" aria-hidden="true" tabindex="-1"></a><span class="co">&amp;= \text{exp}\left(-\frac{1}{2\sigma^2} \sum_{i = 1}^n \left((y_i -\bar{y})^2 + 2(y_i - \bar{y})(\bar{y} - \theta) + (\bar{y} - \theta)^2 \right) \right)\\</span></span>
<span id="cb34-977"><a href="#cb34-977" aria-hidden="true" tabindex="-1"></a><span class="co">&amp;= \text{exp}\left(-\frac{1}{2\sigma^2} \sum_{i = 1}^n \left((y_i - \bar{y})^2 + n(\bar{y} - \theta)^2 \right) \right) \\</span></span>
<span id="cb34-978"><a href="#cb34-978" aria-hidden="true" tabindex="-1"></a><span class="co">&amp;= \text{exp}\left(-\frac{1}{2\sigma^2} \sum_{i = 1}^n \left((y_i - \bar{y})^2 + n(\bar{y} - \theta)^2 \right) \right) \\</span></span>
<span id="cb34-979"><a href="#cb34-979" aria-hidden="true" tabindex="-1"></a><span class="co">&amp;= \text{exp}\left(-\frac{1}{2\sigma^2} \sum_{i = 1}^n \left(n(\theta - \bar{y})^2 + (y_i - \bar{y})^2 \right) \right)  \\</span></span>
<span id="cb34-980"><a href="#cb34-980" aria-hidden="true" tabindex="-1"></a><span class="co">\end{align}</span></span>
<span id="cb34-981"><a href="#cb34-981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-982"><a href="#cb34-982" aria-hidden="true" tabindex="-1"></a><span class="co">So:</span></span>
<span id="cb34-983"><a href="#cb34-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-984"><a href="#cb34-984" aria-hidden="true" tabindex="-1"></a><span class="co">\begin{align}</span></span>
<span id="cb34-985"><a href="#cb34-985" aria-hidden="true" tabindex="-1"></a><span class="co">\left( \{\theta, \sigma^2\} \; \middle| \; \mathbf{y} \right) &amp;\sim \mathcal{N}_{\chi^{-2}}(\bar{y}, n, n, \frac{1}{n}\sum (y_i - \bar{y})^2)$$</span></span>
<span id="cb34-986"><a href="#cb34-986" aria-hidden="true" tabindex="-1"></a><span class="co">\end{align}</span></span>
<span id="cb34-987"><a href="#cb34-987" aria-hidden="true" tabindex="-1"></a><span class="co">--&gt;</span></span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>