<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jesse Mu">
<meta name="dcterms.date" content="2016-11-10">

<title>Chapter 7: The multivariate normal model – Hoff Bayesian Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-68c8bffd90dad8f2b55c52d7b6410dc0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-26500bfc55c7891837a911d6d50a6255.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Hoff Bayesian Statistics</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-chapters" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Chapters</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-chapters">    
        <li>
    <a class="dropdown-item" href="./1.html">
 <span class="dropdown-text">Chapter 1: Introduction and examples</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./2.html">
 <span class="dropdown-text">Chapter 2: Belief, probability, and exchangeability</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./3.html">
 <span class="dropdown-text">Chapter 3: One-parameter models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./4.html">
 <span class="dropdown-text">Chapter 4: Monte Carlo approximation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./5.html">
 <span class="dropdown-text">Chapter 5: The Normal Model</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./6.html">
 <span class="dropdown-text">Chapter 6: Posterior approximation with the Gibbs sampler</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./7.html">
 <span class="dropdown-text">Chapter 7: The multivariate normal model</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./8.html">
 <span class="dropdown-text">Chapter 8: Group comparisons and hierarchical modeling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./9.html">
 <span class="dropdown-text">Chapter 9: Linear regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./10.html">
 <span class="dropdown-text">Chapter 10: Nonconjugate priors and Metropolis-Hastings algorithms</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./irm.html"> 
<span class="menu-text">Infinite Relational Model</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-multivariate-normal-density" id="toc-the-multivariate-normal-density" class="nav-link active" data-scroll-target="#the-multivariate-normal-density">The multivariate normal density</a>
  <ul class="collapse">
  <li><a href="#example-reading-comprehension" id="toc-example-reading-comprehension" class="nav-link" data-scroll-target="#example-reading-comprehension">Example: reading comprehension</a></li>
  <li><a href="#multivariate-normal-density" id="toc-multivariate-normal-density" class="nav-link" data-scroll-target="#multivariate-normal-density">Multivariate normal density</a></li>
  </ul></li>
  <li><a href="#a-semiconjugate-prior-distribution-for-the-mean" id="toc-a-semiconjugate-prior-distribution-for-the-mean" class="nav-link" data-scroll-target="#a-semiconjugate-prior-distribution-for-the-mean">A semiconjugate prior distribution for the mean</a></li>
  <li><a href="#the-inverse-wishart-distribution" id="toc-the-inverse-wishart-distribution" class="nav-link" data-scroll-target="#the-inverse-wishart-distribution">The inverse-Wishart distribution</a>
  <ul class="collapse">
  <li><a href="#specifying-parameters" id="toc-specifying-parameters" class="nav-link" data-scroll-target="#specifying-parameters">Specifying parameters</a></li>
  <li><a href="#full-conditional-distribution-of-sigma-mid-boldsymboly_1-dots-boldsymboly_n-boldsymboltheta" id="toc-full-conditional-distribution-of-sigma-mid-boldsymboly_1-dots-boldsymboly_n-boldsymboltheta" class="nav-link" data-scroll-target="#full-conditional-distribution-of-sigma-mid-boldsymboly_1-dots-boldsymboly_n-boldsymboltheta">Full conditional distribution of <span class="math inline">\(\Sigma \mid \boldsymbol{y}_1, \dots, \boldsymbol{y}_n, \boldsymbol{\theta}\)</span></a></li>
  </ul></li>
  <li><a href="#summary-of-inference-with-the-multivariate-normal" id="toc-summary-of-inference-with-the-multivariate-normal" class="nav-link" data-scroll-target="#summary-of-inference-with-the-multivariate-normal">Summary of inference with the multivariate normal</a>
  <ul class="collapse">
  <li><a href="#semiconjugate-prior" id="toc-semiconjugate-prior" class="nav-link" data-scroll-target="#semiconjugate-prior">(Semiconjugate) prior</a></li>
  <li><a href="#posterior" id="toc-posterior" class="nav-link" data-scroll-target="#posterior">Posterior</a></li>
  </ul></li>
  <li><a href="#gibbs-sampling-of-the-mean-and-covariance" id="toc-gibbs-sampling-of-the-mean-and-covariance" class="nav-link" data-scroll-target="#gibbs-sampling-of-the-mean-and-covariance">Gibbs sampling of the mean and covariance</a>
  <ul class="collapse">
  <li><a href="#example-reading-comprehension-1" id="toc-example-reading-comprehension-1" class="nav-link" data-scroll-target="#example-reading-comprehension-1">Example: reading comprehension</a>
  <ul class="collapse">
  <li><a href="#specifying-prior" id="toc-specifying-prior" class="nav-link" data-scroll-target="#specifying-prior">Specifying prior</a></li>
  <li><a href="#code" id="toc-code" class="nav-link" data-scroll-target="#code">Code</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#missing-data-and-imputation" id="toc-missing-data-and-imputation" class="nav-link" data-scroll-target="#missing-data-and-imputation">Missing data and imputation</a>
  <ul class="collapse">
  <li><a href="#gibbs-sampling-with-missing-data" id="toc-gibbs-sampling-with-missing-data" class="nav-link" data-scroll-target="#gibbs-sampling-with-missing-data">Gibbs sampling with missing data</a></li>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a>
  <ul class="collapse">
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section">7.1</a>
  <ul class="collapse">
  <li><a href="#a" id="toc-a" class="nav-link" data-scroll-target="#a">a</a></li>
  <li><a href="#b" id="toc-b" class="nav-link" data-scroll-target="#b">b</a></li>
  </ul></li>
  <li><a href="#section-1" id="toc-section-1" class="nav-link" data-scroll-target="#section-1">7.3</a>
  <ul class="collapse">
  <li><a href="#a-1" id="toc-a-1" class="nav-link" data-scroll-target="#a-1">a</a></li>
  <li><a href="#b-1" id="toc-b-1" class="nav-link" data-scroll-target="#b-1">b</a></li>
  <li><a href="#c" id="toc-c" class="nav-link" data-scroll-target="#c">c</a></li>
  </ul></li>
  <li><a href="#section-2" id="toc-section-2" class="nav-link" data-scroll-target="#section-2">7.4</a>
  <ul class="collapse">
  <li><a href="#a-2" id="toc-a-2" class="nav-link" data-scroll-target="#a-2">a</a></li>
  <li><a href="#b-2" id="toc-b-2" class="nav-link" data-scroll-target="#b-2">b</a></li>
  <li><a href="#c-1" id="toc-c-1" class="nav-link" data-scroll-target="#c-1">c</a></li>
  <li><a href="#d" id="toc-d" class="nav-link" data-scroll-target="#d">d</a></li>
  <li><a href="#e" id="toc-e" class="nav-link" data-scroll-target="#e">e</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Chapter 7: The multivariate normal model</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Jesse Mu </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 10, 2016</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<!-- Setup -->
<!-- Begin writing -->
<section id="the-multivariate-normal-density" class="level1">
<h1>The multivariate normal density</h1>
<section id="example-reading-comprehension" class="level2">
<h2 class="anchored" data-anchor-id="example-reading-comprehension">Example: reading comprehension</h2>
<p>We make the step to two variables by example. Consider a sample (<span class="math inline">\(n = 22\)</span>) of children who are given reading comprehension tests before and after receiving a particular instructional method. So each student has a before and after test score. We can denote these <em>two</em> variables for student <span class="math inline">\(i\)</span> as a vector <span class="math inline">\(\mathbf{Y}_i\)</span>, where <span class="math inline">\(Y_{i, 1}\)</span> is the before score, and <span class="math inline">\(Y_{i, 2}\)</span> is the after score:</p>
<p><span class="math display">\[\begin{align}
\mathbf{Y}_i = \begin{pmatrix}
Y_{i, 1} \\
Y_{i, 2} \\
\end{pmatrix}
\end{align}\]</span></p>
<p>Recall <span class="math display">\[\begin{align}
\text{Cor}(X, Y) &amp;= \frac{\text{Cov}(X, Y)}{\text{SD}(X)\text{SD}(Y)} \\
&amp;= \frac{\sigma_{1, 2}}{\sigma_1 \sigma_2} \\
&amp;= \frac{\sigma_{1, 2}}{\sqrt{\sigma_1^2 \sigma_2^2}}
\end{align}\]</span></p>
</section>
<section id="multivariate-normal-density" class="level2">
<h2 class="anchored" data-anchor-id="multivariate-normal-density">Multivariate normal density</h2>
<p>If <span class="math display">\[\begin{align}
\mathbf{y} &amp;= \begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_p
\end{pmatrix} \sim \mathcal{N}(\boldsymbol{\theta}, \Sigma)
\end{align}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{align}
\boldsymbol{\theta} &amp;= \begin{pmatrix}
\theta_1 \\
\theta_2 \\
\vdots \\
\theta_p
\end{pmatrix}
\end{align}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align}
\Sigma &amp;= \begin{pmatrix}
\sigma_1^2 &amp; \sigma_{1, 2} &amp; \cdots &amp; \sigma_{1, p} \\
\sigma_{1, 2} &amp; \sigma_2^2 &amp; \cdots &amp; \sigma_{2, p} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
\sigma_{1, p} &amp; \cdots &amp; \cdots &amp; \sigma_p^2
\end{pmatrix}
\end{align}\]</span></p>
<p>then</p>
<p><span class="math display">\[\begin{align}
p(\mathbf{y} \mid \boldsymbol{\theta}, \Sigma) = (2\pi)^{-p / 2} | \Sigma |^{-1/2} \text{exp}\left( \frac{1}{2} -(\mathbf{y} - \boldsymbol{\theta})^T \Sigma^{-1} (\mathbf{y} - \boldsymbol{\theta}) \right)
\end{align}\]</span></p>
<p>It’s useful to compare this to the single variable case. The first two terms represent <span class="math inline">\(1/\sqrt{2\pi\sigma^2}\)</span> in the univariate case, generalized to an arbitrary dimension: there is no longer the square root of the variance for a single variable, but rather a <span class="math inline">\(p\)</span>-dimensional covariance matrix raised to the <span class="math inline">\(1/2\)</span> power. Furthermore, the quadratic term in the exponential <span class="math inline">\((y -
\theta)^2\)</span> can be seen in the exponential here, but also included is the matrix multiplication with the inverse of the covariance matrix.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output cell-output-stderr">
<pre><code>Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by
the caller; using TRUE
Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by
the caller; using TRUE
Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by
the caller; using TRUE
Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by
the caller; using TRUE
Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by
the caller; using TRUE
Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by
the caller; using TRUE</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use "none" instead as
of ggplot2 3.3.4.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: The dot-dot notation (`..level..`) was deprecated in ggplot2 3.4.0.
ℹ Please use `after_stat(level)` instead.</code></pre>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="7_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="a-semiconjugate-prior-distribution-for-the-mean" class="level1">
<h1>A semiconjugate prior distribution for the mean</h1>
<p>At this point we have given up on computing a full closed form solution for the posterior, as it’s too complicated. Rather, if we compute conjugate priors and posteriors for the full conditional distributions of the <span class="math inline">\(\boldsymbol{\theta}\)</span> and <span class="math inline">\(\Sigma\)</span> separately, we can use Gibbs sampling to easily estimate the joint posterior distribution. Let’s start by calculating <span class="math inline">\(\boldsymbol{\theta}\)</span> first:</p>
<p>Analogous the univariate case, for the multivariate normal distribution, a conjugate prior for the population mean is a multivariate normal.</p>
<p>Let <span class="math inline">\(\boldsymbol{\mu}_0\)</span> be the prior mean, and <span class="math inline">\(\Lambda_0\)</span> be the covariance matrix of <span class="math inline">\(\boldsymbol{\mu}_0\)</span>. Then let <span class="math inline">\(\boldsymbol{\theta} \sim \mathcal{N}(\boldsymbol{\mu}_0, \Lambda_0)\)</span>. What does this prior look like?</p>
<p><span class="math display">\[\begin{align}
p(\boldsymbol{\theta} \mid \Sigma) &amp;\propto \text{exp}\left[ -\frac{1}{2} (\boldsymbol{\theta} - \boldsymbol{\mu}_0)^T \Lambda_0^{-1} (\boldsymbol{\theta} - \boldsymbol{\mu}_0) \right] &amp; \text{Drop constants} \\
&amp;= \text{exp}\left[-\frac{1}{2} (\boldsymbol{\theta}^T - \boldsymbol{\mu}_0^T) (\Lambda_0^{-1}\boldsymbol{\theta} - \Lambda_0^{-1}\boldsymbol{\mu}_0 ) \right] \\
&amp;= \text{exp}\left[-\frac{1}{2} (\boldsymbol{\theta}^T \Lambda_0^{-1} \boldsymbol{\theta} - \boldsymbol{\mu}_0^T \Lambda_0^{-1} \boldsymbol{\theta} - \boldsymbol{\theta}^T \Lambda_0^{-1} \boldsymbol{\mu}_0 + \boldsymbol{\mu}_0^T \Lambda_0^{-1}\boldsymbol{\mu}_0  )\right] \\
&amp;= \text{exp} \left[ -\frac{1}{2} (\boldsymbol{\theta}^T \Lambda_0^{-1} \boldsymbol{\theta} - 2\boldsymbol{\theta}^T \Lambda_0^{-1} \boldsymbol{\mu}_0 + \boldsymbol{\mu}_0^T \Lambda_0^{-1}\boldsymbol{\mu}_0 ) \right] &amp; \text{Middle terms combine} \\
&amp;\propto \text{exp} \left[ -\frac{1}{2}\boldsymbol{\theta}^T \Lambda_0^{-1} \boldsymbol{\theta} + \boldsymbol{\theta}^T \Lambda_0^{-1} \boldsymbol{\mu}_0 \right] &amp; \text{Discard last term} \\

\end{align}\]</span></p>
<p>If we let <span class="math inline">\(\mathbf{A}_0 = \Lambda_0^{-1}\)</span>, i.e.&nbsp;the precision matrix (which echoes the univariate case) and <span class="math inline">\(\mathbf{b}_0 = \Lambda_0^{-1}
\boldsymbol{\mu}_0\)</span>, this simplifies to</p>
<p><span class="math display">\[\begin{align}
p(\boldsymbol{\theta}) &amp;= \text{exp}\left( -\frac{1}{2}\boldsymbol{\theta}^T
\mathbf{A}_0 \boldsymbol{\theta} + \boldsymbol{\theta}^T \mathbf{b}_0 \right) \\
\end{align}\]</span></p>
<p>We will see this simplified form show up when working with the sampling models and posterior distribution as well.</p>
<p>Let’s first look at the sampling model. Doing similar simplifications, we get</p>
<p><span class="math display">\[\begin{align}
p(\mathbf{y}_1, \dots, \mathbf{y}_n \mid \boldsymbol{\theta}, \Sigma) \propto
\text{exp}(-\frac{1}{2}\boldsymbol{\theta}^T \mathbf{A}_1 \boldsymbol{\theta} +
\boldsymbol{\theta}^T \mathbf{b}_1)
\end{align}\]</span></p>
<p>where this time <span class="math inline">\(\mathbf{A}_1 = n\Sigma^{-1}\)</span> and <span class="math inline">\(\mathbf{b}_1 = n
\Sigma^{-1}\bar{\mathbf{y}}\)</span>. <span class="math inline">\(\bar{\mathbf{y}}\)</span> is the vector of sample averages for each variable.</p>
<p>So finally, we can calculate the posterior for <span class="math inline">\(\boldsymbol{\theta}\)</span>:</p>
<p><span class="math display">\[\begin{align}
p(\boldsymbol{\theta} \mid \mathbf{y}_1, \dots, \mathbf{y}_n, \Sigma) &amp;=
p(\boldsymbol{\theta} \mid \Sigma) p(\mathbf{y}_1, \dots, \mathbf{y}_n \mid
\boldsymbol{\theta}, \Sigma) / p(\mathbf{y}_1, \dots, \mathbf{y}_n \mid \Sigma)
\\
&amp;\propto p(\boldsymbol{\theta} \mid \Sigma) p(\mathbf{y}_1, \dots, \mathbf{y}_n \mid
\boldsymbol{\theta}, \Sigma) \\
&amp;\propto
\text{exp}\left( -\frac{1}{2}\boldsymbol{\theta}^T
\mathbf{A}_0 \boldsymbol{\theta} + \boldsymbol{\theta}^T \mathbf{b}_0 \right)
\times \text{exp}(-\frac{1}{2}\boldsymbol{\theta}^T \mathbf{A}_1 \boldsymbol{\theta} +
\boldsymbol{\theta}^T \mathbf{b}_1) \\
&amp;= \text{exp}\left(-\frac{1}{2} \boldsymbol{\theta}^T \mathbf{A}_n \boldsymbol{\theta} + \boldsymbol{\theta}^T \mathbf{b}_n \right)
\end{align}\]</span></p>
<p>where we have combined terms such that <span class="math inline">\(\mathbf{A}_n = \mathbf{A}_0 + \mathbf{A}_1 = \Lambda_0^{-1} + n\Sigma^{-1}\)</span> and <span class="math inline">\(\mathbf{b}_n = \mathbf{b}_0 + \mathbf{b}_1 = \Lambda_0^{-1} \boldsymbol{\mu}_0 + n\Sigma^{-1} \bar{\mathbf{y}}\)</span>.</p>
<p>So if <span class="math inline">\(\boldsymbol{\theta} \mid \Sigma \sim \mathcal{N}(\boldsymbol{\mu}_0,
\Lambda_0)\)</span>, then <span class="math inline">\(\boldsymbol{\theta} \mid \mathbf{y}_1, \dots, \mathbf{y}_n, \Sigma
\sim \mathcal{N}(\boldsymbol{\mu}_n, \Lambda_n)\)</span> where the parameters are defined as above.</p>
<p>Then notice that, like the univariate case:</p>
<ul>
<li><span class="math inline">\(\text{Cov}(\boldsymbol{\theta} \mid \mathbf{y}_1, \dots, \mathbf{y}_n,
\Sigma) = \Lambda_n = (A_0^{-1} + n\Sigma^{-1})^{-1}\)</span>, a combination of prior and posterior precision, and</li>
<li><span class="math inline">\(\mathbb{E}(\boldsymbol{\theta} \mid \mathbf{y}_1, \dots, \mathbf{y}_n,
\Sigma) = \boldsymbol{\mu}_n = (A_0^{-1} + n\Sigma^{-1})^{-1} (\Lambda_0^{-1} \boldsymbol{\mu}_0 + n\Sigma^{-1}\bar{\boldsymbol{y}})\)</span>, weighted average of the prior estimate of the mean and the sample mean.</li>
</ul>
</section>
<section id="the-inverse-wishart-distribution" class="level1">
<h1>The inverse-Wishart distribution</h1>
<p>We’ve seen the (semi)conjugate prior and posterior distribution for the mean. Now to the covariance matrix <span class="math inline">\(\Sigma\)</span>. Remember that for the univariate normal distribution, a semi-conjugate prior distribution for the variance was the inverse-Gamma distribution. Similarly, for the multivariate case, a semi-conjugate prior distribution for the covariance matrix is the inverse of the multivariate analog of the Gamma distribution, known as a Wishart distribution.</p>
<p>Generating samples from a Wishart distribution is analogous to sampling a set of variables from a multivariate normal distribution and calculating the empirical covriance matrix of the samples. More specifically, with parameters <span class="math inline">\(\nu_0 \in
\mathbb{Z}^+\)</span> and <span class="math inline">\(\Phi_0\)</span> (a <span class="math inline">\(p \times p\)</span> covariance matrix),</p>
<ol type="1">
<li>Sample <span class="math inline">\(z_1, \dots, z_{\nu_0} \sim \mathcal{N}(\boldsymbol{0}, \Phi_0)\)</span></li>
<li>Calculate <span class="math inline">\(\boldsymbol{Z}^T \boldsymbol{Z} = \sum_{i = 1}^{\nu_0} z_i z_i^T\)</span></li>
</ol>
<p>Then <span class="math inline">\(\boldsymbol{Z}_1^T \boldsymbol{Z}_1, \dots, \boldsymbol{Z}_S^T \boldsymbol{Z} \sim \text{Wishart}(\nu_0, \Phi_0)\)</span>.</p>
<p>Some properties of samples from the Wishart which happen to stem from calculating empirical covariance matrices in the first place</p>
<ul>
<li>If <span class="math inline">\(\nu_0 &gt; p\)</span> then <span class="math inline">\(\boldsymbol{Z}^\boldsymbol{Z}\)</span> is positive definite</li>
<li><span class="math inline">\(\boldsymbol{Z}^\boldsymbol{Z}\)</span> is symmetric</li>
<li><span class="math inline">\(\mathbb{E}(\boldsymbol{Z}^T\boldsymbol{Z}) = \nu_0 \Phi_0\)</span></li>
</ul>
<p>Like how the Gamma is not the conjugate prior of the variance for the Normal, the Wishart is not the conjugate prior of the variance for the multivariate Normal; rather, the inverse-Wishart is. Sampling from an inverse-Wishart just involves taking <span class="math inline">\(\Sigma^{(s)} = (\boldsymbol{Z}^{(s)T} \boldsymbol{Z}^{(s)})^{-1}\)</span></p>
<p>If we’re sampling from an inverse-Wishart we rename <span class="math inline">\(\Phi_0\)</span> to <span class="math inline">\(\mathbf{S}_0^{-1}\)</span> such that</p>
<ul>
<li><span class="math inline">\(\mathbb{E}(\Sigma^{-1}) = \nu_0 \mathbf{S}_0^{-1}\)</span> (instead of <span class="math inline">\(\Phi_0\)</span>)</li>
<li><span class="math inline">\(\mathbb{E}(\Sigma) = \frac{1}{\nu_0 - p - 1} \mathbf{S}_0\)</span> (so not exactly the inverse of <span class="math inline">\(\mathbf{S}_0^{-1}\)</span>)</li>
</ul>
<section id="specifying-parameters" class="level2">
<h2 class="anchored" data-anchor-id="specifying-parameters">Specifying parameters</h2>
<p>Specifying parameters is somewhat interesting for the inverse-Wishart becase there are many of them - we need to specify the entire covariance matrix. If we have a prior expectation of a covariance matrix <span class="math inline">\(\Sigma_0\)</span>, then we can center our prior around it in two suggested ways:</p>
<ul>
<li>Set <span class="math inline">\(\nu_0\)</span> large and set <span class="math inline">\(\mathbf{S}_0 = (\nu_0 - p - 1) \Sigma_0\)</span>, such that <span class="math inline">\(\mathbb{E}(\Sigma) = \frac{\nu_0 - p - 1}{\nu_0 - p - 1}\Sigma_0 = \Sigma_0\)</span> and (due to large <span class="math inline">\(\nu_0\)</span>) the prior is fairly concentrated around <span class="math inline">\(\Sigma_0\)</span>;</li>
<li>Set <span class="math inline">\(\nu_0 = p + 2\)</span> and let <span class="math inline">\(\mathbf{S}_0 = \Sigma_0\)</span>, such that <span class="math inline">\(\mathbb{E}(\Sigma) = \frac{1}{p + 2 - p - 1}\Sigma_0 = \Sigma_0\)</span> but only loosely centered around <span class="math inline">\(\Sigma_0\)</span> (due to fairly small <span class="math inline">\(\nu_0\)</span>)</li>
</ul>
<p>For an “empirical Bayes” approach, we can center our prior around the empirical covariance matrix of our sample.</p>
</section>
<section id="full-conditional-distribution-of-sigma-mid-boldsymboly_1-dots-boldsymboly_n-boldsymboltheta" class="level2">
<h2 class="anchored" data-anchor-id="full-conditional-distribution-of-sigma-mid-boldsymboly_1-dots-boldsymboly_n-boldsymboltheta">Full conditional distribution of <span class="math inline">\(\Sigma \mid \boldsymbol{y}_1, \dots, \boldsymbol{y}_n, \boldsymbol{\theta}\)</span></h2>
<p>There is an intimidating normalizing constant for the inverse-Wishart. All we need to know is if <span class="math inline">\(\Sigma \sim \text{inverse-Wishart}(\nu_0, \mathbf{S}_0^{-1})\)</span>,</p>
<p><span class="math display">\[\begin{align}
p(\Sigma) \propto | \Sigma |^{-(\nu_0 + p + 1) / 2} \times \exp\left(-\text{tr}(\mathbf{S}_0 \Sigma^{-1})  / 2\right)
\end{align}\]</span></p>
<p>Recall the sampling model from earlier. We skipped some simplifications above (substituting <span class="math inline">\(\mathbf{A}_1\)</span> and <span class="math inline">\(\boldsymbol{b}_1\)</span>) so take it for granted that a less-simplified form is</p>
<p><span class="math display">\[\begin{align}
p(\mathbf{y}_1, \dots, \mathbf{y}_n \mid \boldsymbol{\theta}, \Sigma) &amp;= (2\pi)^{-np/2} | \Sigma |^{-n/2} \exp \left( - \sum_{i = 1}^n (\boldsymbol{y}_i - \boldsymbol{\theta})^T \Sigma^{-1} (\boldsymbol{y}_i - \boldsymbol{\theta}) / 2 \right)
\end{align}\]</span></p>
<p>Using some linear algebra,</p>
<p><span class="math display">\[\begin{align}
\sum_{i = 1}^n (\boldsymbol{y}_i - \boldsymbol{\theta})^T \Sigma^{-1} (\boldsymbol{y}_i - \boldsymbol{\theta}) &amp;= \text{tr}\left( \left(\sum_{i = 1}^n (\boldsymbol{y}_i - \boldsymbol{\theta}) (\boldsymbol{y}_i - \boldsymbol{\theta})^T \right) \Sigma^{-1} \right)
\text{tr}\left( \mathbf{S}_{\theta} \Sigma^{-1} \right)
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf{S}_{\theta} = \sum_{i = 1}^n (\boldsymbol{y}_i -
\boldsymbol{\theta}) (\boldsymbol{y}_i - \boldsymbol{\theta})^T\)</span> is the <em>residual sum of squares matrix</em> for the vectors <span class="math inline">\(\boldsymbol{y}_1, \dots,
\boldsymbol{y}_n\)</span>. (To obtain the residual sum of squares matrix, you calculate the sum of squares for the residual vectors <span class="math inline">\(\boldsymbol{y}_i -
\boldsymbol{\theta}\)</span>)</p>
<p>So</p>
<p><span class="math display">\[\begin{align}
p(\mathbf{y}_1, \dots, \mathbf{y}_n \mid \boldsymbol{\theta}, \Sigma) &amp;= (2\pi)^{-np/2} | \Sigma |^{-n/2} \exp \left( - \text{tr}(\mathbf{S}_{\theta} \Sigma^{-1})  / 2 \right)
\end{align}\]</span></p>
<p>Now we can calculate the full conditional distribution of <span class="math inline">\(\Sigma\)</span>:</p>
<p><span class="math display">\[\begin{align}
p(\Sigma \mid \boldsymbol{y}_1, \dots, \boldsymbol{y}_n, \boldsymbol{\theta}) &amp;\propto p(\Sigma) \times p(\boldsymbol{y}_1, \dots, \boldsymbol{y}_n \mid \boldsymbol{\theta}, \Sigma) \\
&amp;\propto \left[ | \Sigma |^{-(\nu_0 + p + 1) / 2} \times \exp\left(-\text{tr}(\mathbf{S}_0 \Sigma^{-1})  / 2\right) \right] \times
\left[ | \Sigma |^{-n/2} \exp \left( - \text{tr}(\mathbf{S}_{\theta} \Sigma^{-1})  / 2 \right) \right] \\
&amp;=  | \Sigma |^{-(\nu_0 + n + p + 1) / 2} \exp \left( -\text{tr}(\left( \mathbf{S}_0 + \mathbf{S}_{\theta} \right) \Sigma^{-1}) / 2 \right) \\
&amp;\propto \text{dinverse-Wishart}\left(\nu_0 + n, \left[\mathbf{S}_0 + \mathbf{S}_{\theta} \right]^{-1} \right) \\
&amp;= \text{dinverse-Wishart}\left(\nu_n, \mathbf{S}_n^{-1} \right)
\end{align}\]</span></p>
<p>where <span class="math inline">\(\nu_n = \nu_0 + n\)</span> and <span class="math inline">\(\mathbf{S}_n = \mathbf{S}_0 + \mathbf{S}_{\theta}\)</span>.</p>
<p>Like the univariate case, the conditional distribution on <span class="math inline">\(\Sigma\)</span> is dependent on <span class="math inline">\(\nu_0 + n\)</span>, a sum of the prior sample size and the data sample size, <span class="math inline">\(\textbf{S}_0 + \textbf{S}_\theta\)</span>, the sum of the “prior” residual sum of squares and the empirical sum of squares.</p>
<p>Recall that, since inverse-Wishart matrices involve sampling from a normal distribution with mean <span class="math inline">\(\mathbf{0}\)</span>, indeed <span class="math inline">\(\mathbf{S}_0\)</span> can be treated as a <em>residual</em> covariance matrix, given that <span class="math inline">\(\mathbb{E}(\boldsymbol{y}_i -
\boldsymbol{\theta}) = \mathbf{0}\)</span>.</p>
<p>Finally, notice that the conditional expectation of the covariance matrix is is a weighted average of the prior expectation <span class="math inline">\(\frac{1}{\nu_0 - p -
1}\mathbf{S}_0\)</span> and the unbiased estimator <span class="math inline">\(\frac{1}{n} \mathbf{S}_{\theta}\)</span>:</p>
<p><span class="math display">\[\begin{align}
\mathbb{E}(\Sigma \mid \boldsymbol{y}_1, \dots, \boldsymbol{y}_n, \boldsymbol{\theta}) &amp;= \frac{1}{\nu_0 + n - p - 1} (\mathbf{S}_0 + \mathbf{S}_{\theta}) \\
&amp;= \frac{\nu_0 - p - 1}{\nu_0 + n - p - 1} \frac{1}{\nu_0 - p - 1} \mathbf{S}_0 + \frac{n}{\nu_0 + n - p - 1}\frac{1}{n} \mathbf{S}_{\theta}.
\end{align}\]</span></p>
<p>So the Bayesian estimator is a biased estimator, but is still demonstrably consistent as <span class="math inline">\(n \to \infty\)</span>. As mentioned in Chapter 5, we hope that the estimator is biased more towards the true mean, as long as the prior is mildly informative.</p>
</section>
</section>
<section id="summary-of-inference-with-the-multivariate-normal" class="level1">
<h1>Summary of inference with the multivariate normal</h1>
<p>Like in Chapter 5, here we summarize the moving parts of inference with the multivariate normal sampling model. There are four prior parameters (note some are matrices):</p>
<section id="semiconjugate-prior" class="level2">
<h2 class="anchored" data-anchor-id="semiconjugate-prior">(Semiconjugate) prior</h2>
<ul>
<li><span class="math inline">\(\mathbf{S}_0\)</span> for the inverse-Wishart
<ul>
<li><em>related to</em> the prior estimate of the covariance between the variables</li>
<li>Only <em>related</em> to because as mentioned above, there are some guidelines for what to use for <span class="math inline">\(\nu_0\)</span> and <span class="math inline">\(\mathbf{S}_0\)</span> such that the prior distribution is centered around <span class="math inline">\(\Sigma_0\)</span>, the <em>true</em> prior estimate of the covariance matrix you are looking for.</li>
</ul></li>
<li><span class="math inline">\(\nu_0\)</span> for the inverse-Wishart
<ul>
<li>a “prior sample size” from which the initial estimate of the <em>variance</em> is observed</li>
</ul></li>
<li><span class="math inline">\(\boldsymbol{mu}_0\)</span> for the multivariate normal
<ul>
<li>an initial estimate for the population mean</li>
</ul></li>
<li><span class="math inline">\(\Lambda_0\)</span> for the multivariate normal
<ul>
<li>the covariance (i.e.&nbsp;uncertainty) of the initial estimate for the population mean</li>
</ul></li>
</ul>
<p>Somewhat similar to the univariate case, the estimate of the covariance matrix for the inverse-Wishart prior is decoupled from the estimate of the covariance of the mean vector in the multivariate normal prior, although it’s common to set these the same (but again, see rules for determining <span class="math inline">\(\nu_0\)</span>).</p>
<p>Note that this is somewhat <em>different</em> than the univariate case; since there were no covariances to worry about, what was decoupled was “prior sample sizes” from which the prior variance and prior mean are observed. Like here, it was also common to set these the same.</p>
</section>
<section id="posterior" class="level2">
<h2 class="anchored" data-anchor-id="posterior">Posterior</h2>
<p>The updated parameters are</p>
<ul>
<li><span class="math inline">\(\mathbf{S}_n = \mathbf{S}_0 + \mathbf{S}_{\theta}\)</span>, where <span class="math inline">\(\mathbf{S}_{\theta}\)</span> is the residual sum of squares matrix</li>
<li><span class="math inline">\(\nu_n = \nu_0 + n\)</span></li>
<li><span class="math inline">\(\mu_n = (\Lambda_0^{-1} + n\Sigma^{-1})^{-1} (\Lambda_0 \boldsymbol{\mu}_0 + n\Sigma^{-1}\bar{\boldsymbol{y}}) = \Lambda_n (\Lambda_0^{-1}\boldsymbol{\mu}_0 + n\Sigma^{-1}\bar{\boldsymbol{y}})\)</span></li>
<li><span class="math inline">\(\Lambda_n = (\Lambda_0^{-1} + n\Sigma^{-1})^{-1}\)</span></li>
</ul>
</section>
</section>
<section id="gibbs-sampling-of-the-mean-and-covariance" class="level1">
<h1>Gibbs sampling of the mean and covariance</h1>
<p>Knowing these values, we can now perform Gibbs sampling to sample from <span class="math inline">\(p(\boldsymbol{\theta}, \Sigma \mid \boldsymbol{y}_1, \dots, \boldsymbol{y_n})\)</span>. Specifically, we start with an estimate of one of the two values - <span class="math inline">\(\Sigma^{(0)}\)</span> for simplicity - and use the following algorithm:</p>
<ol type="1">
<li>Sample <span class="math inline">\(\boldsymbol{\theta}^{(s + 1)} \sim \mathcal{N}(\boldsymbol{\mu}_n,
\Lambda_n)\)</span> where the parameters are calculated as above. This depends on the inverse of the previous <span class="math inline">\(\Sigma^{(s)}\)</span>.</li>
<li>Sample <span class="math inline">\(\Sigma^{(s + 1)} \sim \text{inverse-Wishart}(\nu_n, \mathbf{S}_n^{-1})\)</span>, where the parameters depend on <span class="math inline">\(\boldsymbol{\theta}^{(s + 1)}\)</span>.</li>
</ol>
<section id="example-reading-comprehension-1" class="level2">
<h2 class="anchored" data-anchor-id="example-reading-comprehension-1">Example: reading comprehension</h2>
<p>We return to the example of two reading comprehension exams, given pre- and post-training.</p>
<section id="specifying-prior" class="level3">
<h3 class="anchored" data-anchor-id="specifying-prior">Specifying prior</h3>
<section id="mean" class="level4">
<h4 class="anchored" data-anchor-id="mean">Mean</h4>
<p>Assume the tests are designed such that people generally score 50 out of 100. Thus, our prior mean is <span class="math inline">\(\boldsymbol{\mu}_0 = (50, 50)^T\)</span>. Note that this implicitly assumes there is no effect of the training - the prior expectation of the post-training score is the same as the pre-training score.</p>
<p>Next, we need to specify the covariance of the prior expectation of the mean. Specifically, since the scores are bounded between 0 and 100, we should put little probability outside the <span class="math inline">\([0, 100]\)</span> range (so a bell curve centered on 50 with 2 standard deviations <span class="math inline">\(\in [0, 100]\)</span>). Then 1 standard deviation is 25 and the variance is thus 625.</p>
<p>Since the scores are both testing reading ability the scores are probably correlated. So if we want a prior correlation between <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> of 0.5, we need to solve the correlation equation</p>
<p><span class="math display">\[\begin{align}
&amp; 0.5 = \frac{\sigma_{1, 2}}{\sqrt{\sigma_1^2 \sigma_2^2}} \\
\implies&amp; \sigma_{1, 2} = 0.5 \sqrt{625^2} = 312.5 \\
\end{align}\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
\Lambda_0 = \begin{bmatrix} 625 &amp; 312.5 \\ 312.5 &amp; 625 \end{bmatrix}
\]</span></p>
</section>
<section id="variance" class="level4">
<h4 class="anchored" data-anchor-id="variance">Variance</h4>
<p>We will use the guideline mentioned earlier for loosely centering our covariance matrix prior on <span class="math inline">\(\Lambda_0\)</span>. We’ll set <span class="math inline">\(\mathbf{S}_0^{-1} = \Lambda_0\)</span> and <span class="math inline">\(\nu_0 = p + 2 = 4\)</span>.</p>
</section>
</section>
<section id="code" class="level3">
<h3 class="anchored" data-anchor-id="code">Code</h3>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior specification</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>Mu_0 <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">50</span>, <span class="dv">50</span>)  <span class="co"># If coerced, will be treated as column</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>Lambda_0 <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">625</span>, <span class="fl">312.5</span>), <span class="fu">c</span>(<span class="fl">312.5</span>, <span class="dv">625</span>))</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>nu_0 <span class="ot">=</span> <span class="dv">4</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>S_0 <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(Lambda_0), <span class="at">nrow =</span> <span class="dv">2</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that this defines rmvnorm and rwish, but default</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># implementations or packages are used here for real-world applications.</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Specifically MASS::mvrnorm (Modern Applied Statistics with S) is needed, and</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># wishart comes default in newer R distributions with rWishart</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"Inline/chapter7.R"</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Try plotting</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> Y.reading</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(Y.reading)) <span class="sc">+</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> pretest, <span class="at">y =</span> posttest))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="7_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gibbs sampling</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(Y) <span class="co"># Number of observations</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>Sigma_0 <span class="ot">=</span> <span class="fu">cov</span>(Y) <span class="co"># Calculate covariance matrix; initial Sigma sample</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: This initial Sigma sample doesn't end up in the gibbs sample - we throw</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># it away and start from scratch where sample 1 is the Theta based on Sigma_0, and</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># the Sigma based on that new Theta</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">=</span> Sigma_0</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>ybar <span class="ot">=</span> <span class="fu">colMeans</span>(Y) <span class="co"># Faster way of calculating column means</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">TODO</span><span class="co">: Preallocate space for these instead of setting as NULL</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>THETA <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>SIGMA <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Also, inv = solve to make it more readable</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>inv <span class="ot">=</span> solve</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5000</span>) {</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update theta</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 1a. Compute params: Mu_n and Lambda_n from y_1, \dots, y_n and \Sigma^{(s)}</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># &gt; Compute Lambda_n according to equation 7.4</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Note: could cache inverse of priors</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>  Lambda_n <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">inv</span>(Lambda_0) <span class="sc">+</span> n <span class="sc">*</span> <span class="fu">inv</span>(Sigma))</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># &gt; Compute Mu_n according to 7.5. Use Matrix mult</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Note that he first term in 7.5 is Lambda_n</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>  Mu_n <span class="ot">=</span> Lambda_n <span class="sc">%*%</span> (<span class="fu">inv</span>(Lambda_0) <span class="sc">%*%</span> Mu_0 <span class="sc">+</span> n <span class="sc">*</span> <span class="fu">inv</span>(Sigma) <span class="sc">%*%</span> ybar)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 1b. Sample \theta^{(s + 1)} \sim multivariate normal mu_n, lambda_n</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Now we know Lambda_n, Mu_n as implied by the known Sigma and data (p. 108).</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Sample theta from multivariate normal (7.6, implied by 7.3)</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>  Theta <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">1</span>, Mu_n, Lambda_n)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Known Theta. Now sample a new Sigma. </span><span class="al">NOTE</span><span class="co">: Old sigma gets thrown away!</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 2a) Compute params for Sigma: Compute S_n from data and \theta^{(s + 1)}</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>  <span class="co"># i.e. Given the data and this Theta, we need to calculate the parameters that</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>  <span class="co"># define the full conditional distribution of \Sigma^{(s + 1)}</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>  <span class="co"># S_n according to p.112 first paragraph - not defined, but could be in 7.9</span></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>  <span class="co"># S_\theta is the residual sum of squares, defined in unlabeled equation after</span></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 7.8</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Use vectorized to avoid summation</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate residuals, then do sum of squares</span></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Q: why t(Y)? just how elementwise works I guess</span></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>  resid <span class="ot">=</span> <span class="fu">t</span>(Y) <span class="sc">-</span> <span class="fu">c</span>(Theta)</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>  S_theta <span class="ot">=</span> resid <span class="sc">%*%</span> <span class="fu">t</span>(resid)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>  S_n <span class="ot">=</span> S_0 <span class="sc">+</span> S_theta</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 2b) Knowing the parameters for the full conditional distribution on Sigma</span></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>  <span class="co"># (inverse wishart with nu_0 and S_n known), sample</span></span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>  <span class="co"># df = number of samples (degrees of freedom)</span></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Don't forget to invert it afterwards (inverse wishart)</span></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Weird thing is rWishart returns a weird list of arrays</span></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">rWishart</span>(<span class="dv">1</span>, nu_0 <span class="sc">+</span> n, <span class="fu">inv</span>(S_n))[, , <span class="dv">1</span>])</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>  THETA <span class="ot">=</span> <span class="fu">rbind</span>(THETA, Theta)</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Flatten sigma??</span></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>  SIGMA <span class="ot">=</span> <span class="fu">rbind</span>(SIGMA, <span class="fu">c</span>(Sigma))</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Here are some associated calculations with this sample</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Confidence interval for difference between post and pre test</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(THETA[, <span class="dv">2</span>] <span class="sc">-</span> THETA[, <span class="dv">1</span>], <span class="at">prob =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>     2.5%       50%     97.5% 
 1.454197  6.614914 11.663732 </code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Likelihood that the mean for the second test is greater than the mean for the</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 1st test</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(THETA[, <span class="dv">2</span>] <span class="sc">&gt;</span> THETA[, <span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9924</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Confidence interval for the correlation</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>CORR <span class="ot">=</span> <span class="fu">apply</span>(SIGMA, <span class="at">MARGIN =</span> <span class="dv">1</span>, <span class="at">FUN =</span> <span class="cf">function</span>(row) {</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># indices 1 and 4 are correlation of dims 1 and 2</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># indices 2 is equal to index 3 is equal to covariance</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  row[<span class="dv">2</span>] <span class="sc">/</span> <span class="fu">sqrt</span>(row[<span class="dv">1</span>] <span class="sc">*</span> row[<span class="dv">4</span>])</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Obviously there is a correlation</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(CORR, <span class="at">prob =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>     2.5%       50%     97.5% 
0.4180072 0.6871101 0.8476477 </code></pre>
</div>
</div>
<p>We can also work with the posterior predictive distribution by sampling new pairs <span class="math inline">\((y_1, y_2)^{T(s)}\)</span> from our samples of <span class="math inline">\(\boldsymbol{\theta}^{(s)}\)</span> and <span class="math inline">\(\Sigma^{(s)}\)</span>:</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="dv">5000</span>, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5000</span>) {</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  Y[s, ] <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="dv">1</span>, THETA[s, ], <span class="fu">matrix</span>(SIGMA[s, ], <span class="at">nrow =</span> <span class="dv">2</span>))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability that the post-test score of a randomly selected person is greater</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># than the pre-test score</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(Y[, <span class="dv">2</span>] <span class="sc">&gt;</span> Y[, <span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.7032</code></pre>
</div>
</div>
<p>Importantly, the probability of an individual having a higher post-test score than pre-test is much lower than the near-1 probability of the difference in mean test scores. It’s important to have clarified the difference between population means and individuals when drawing conclusions about your data.</p>
</section>
</section>
</section>
<section id="missing-data-and-imputation" class="level1">
<h1>Missing data and imputation</h1>
<p>Another useful application of Bayesian analysis and the multivariate normal model is the imputation of missing data. There are various naive ways to deal with missing data in datasets:</p>
<ul>
<li>listwise deletion: just discard points with missing data. But this wastes valuable data!</li>
<li>replace missing variables for points with the mean of that variable for the entire dataset. But this results in inaccurate estimates, since a data point’s other variables may contain information about the data point’s</li>
</ul>
<p>The Bayesian approach offers a neat solution around this. The idea is that the likelihood of a datapoint with missing values is the likelihood of the observed values while integrating over the missing values.</p>
<p>Specifically, let’s assume in a dataset <span class="math inline">\(\boldsymbol{Y}\)</span> with missing values, we have a corresponding matrix <span class="math inline">\(\boldsymbol{O}\)</span> which contains a 1 if the corresponding element in <span class="math inline">\(\boldsymbol{Y}\)</span> exists, else 0. (This is just to help us mathematically indicate which variables are missing)</p>
<p>Then consider a single observation <span class="math inline">\(\boldsymbol{y}_i\)</span>. We have</p>
<p><span class="math display">\[\begin{align}
p(\boldsymbol{o}_i, \{ y_{i, j} \; : \; o_{i, j} = 1 \} \mid \boldsymbol{\theta}, \Sigma) &amp;=
p(\boldsymbol{o}_i) \times \int \left[ p(\boldsymbol{y}_i \mid \boldsymbol{\theta}, \Sigma) \prod_{y_{i, j} \; : \; o_{i, j} = 0} dy_{i, j} \right]
\end{align}\]</span></p>
<p>i.e.&nbsp;we are integrating over all possible “full” observations of datapoints <span class="math inline">\(\boldsymbol{y}\)</span> with respect to the variables that we don’t have. The variables we have observed stay constant.</p>
<section id="gibbs-sampling-with-missing-data" class="level2">
<h2 class="anchored" data-anchor-id="gibbs-sampling-with-missing-data">Gibbs sampling with missing data</h2>
<p>Normally we use Gibbs sampling to estimate the posterior <span class="math inline">\(p(\boldsymbol{\theta},
\Sigma \mid \mathbf{Y})\)</span> Here, however, we don’t have a full dataset <span class="math inline">\(\mathbf{Y}\)</span>; rather, we have an observed dataset <span class="math inline">\(\mathbf{Y}_{\text{obs}}\)</span> and missing values <span class="math inline">\(\mathbf{Y}_{\text{miss}}\)</span>. The key idea is to <em>also</em> estimate the posterior distribution on <span class="math inline">\(\mathbf{Y}_{\text{miss}}\)</span>, which will also help us make more accurate estimates on <span class="math inline">\(\boldsymbol{\theta}\)</span> and <span class="math inline">\(\Sigma\)</span>. Using Gibbs sampling, when we have sample values <span class="math inline">\(\boldsymbol{\theta}^{(s)}\)</span> and <span class="math inline">\(\Sigma^{(s)}\)</span>, we can sample from</p>
<p><span class="math display">\[
\mathbf{Y}_{\text{miss}}^{(s)} \sim p(\mathbf{Y}_{\text{miss}} \mid \mathbf{Y}_{\text{obs}}, \boldsymbol{\theta}^{(s)}, \Sigma^{(s)})
\]</span></p>
<p>Specifically, to sample from the above, we simply sample the missing values of each data point independently. For a data point <span class="math inline">\(\boldsymbol{y}\)</span> with missing values, let <span class="math inline">\(a\)</span> be the indices of the observed values and <span class="math inline">\(b\)</span> be the indices of the missing values. Then it is shown that sampling <span class="math inline">\(\boldsymbol{y}_{[b]}\)</span> given known observed variables and the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> and <span class="math inline">\(\Sigma\)</span> also follows a multivariate normal distribution, but with mean and covariance matrices with dimension <span class="math inline">\(| b |\)</span> that take into account the existing variables:</p>
<p><span class="math display">\[\begin{align}
\boldsymbol{y}_{[b]} \mid \boldsymbol{y}_{[a]}, \boldsymbol{\theta}, \Sigma \sim \mathcal{N}(\boldsymbol{\theta}_{b \mid a}, \Sigma_{b \mid a})
\end{align}\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\boldsymbol{\theta}_{b \mid a} = \boldsymbol{\theta}_b + \Sigma_{[b, a]}(\Sigma_{[a, a]})^{-1} (\boldsymbol{y}_{[a]} - \boldsymbol{\theta}_{[a]})\)</span>;
<ul>
<li>Intuitively, the mean of the multivariate normal distribution on the missing values <em>given</em> some observed values starts with the unconditional mean of the observed values, plus or minus some offset that depends on the observed values and the correlations between the observed and missing values. For example, if it is known that a datapoint’s observed values are quite high relative to the mean <span class="math inline">\((\boldsymbol{y}_a - \boldsymbol{\theta}_a)\)</span>, and that there is a positive correlation between observed values and missing values, we would expect the missing values to generally be higher as well.</li>
</ul></li>
<li><span class="math inline">\(\Sigma_{b \mid a} = \Sigma_{[b, b]} - \Sigma_{[b, a]} (\Sigma_{[a, a]})^{-1} \Sigma_{[a, b]}\)</span>
<ul>
<li>Intuitively, the covariance matrix of the conditional distribution on the missing values starts with the unconditional covariance, but notice the minus sign; since the covariance matrix is positive definite, knowing about some observed variables will decrease our uncertainty about the missing values.</li>
</ul></li>
</ul>
<p>Once we’ve sampled a set of missing values, notice that we now have a full “dataset” if we combine our observed values with the newly sampled missing values. This means that we can sample from the full conditional distributions of <span class="math inline">\(\boldsymbol{\theta}\)</span> and <span class="math inline">\(\Sigma\)</span> normally, and from there, once again sample a new set of <span class="math inline">\(\mathbf{Y}_{\text{miss}}\)</span>.</p>
<p>To summarize Gibbs sampling with missing data: assume starting values <span class="math inline">\(\Sigma^{(0)}\)</span> and <span class="math inline">\(\mathbf{Y}_{\text{miss}}^{(0)}\)</span> - perhaps the empirical covariance matrix and the unconditional means of the observed sample. Then the algorithm has just one more step:</p>
<ol type="1">
<li>Sample <span class="math inline">\(\boldsymbol{\theta}^{(s + 1)}\)</span> from <span class="math inline">\(p(\boldsymbol{\theta} \mid \mathbf{Y}_{\text{obs}}, \mathbf{Y}_{\text{miss}}^{(s)}, \Sigma^{(s)})\)</span></li>
<li>Sample <span class="math inline">\(\Sigma^{(s + 1)}\)</span> from <span class="math inline">\(p(\Sigma \mid \mathbf{Y}_{\text{obs}}, \mathbf{Y}_{\text{miss}}^{(s)}, \boldsymbol{\theta}^{(s)})\)</span></li>
<li>Sample <span class="math inline">\(\mathbf{Y}_{\text{miss}}^{(s + 1)}\)</span> from <span class="math inline">\(p(\mathbf{Y}_{\text{miss}} \mid \mathbf{Y}_{\text{obs}}, \boldsymbol{\theta}^{(s)}, \Sigma^{(s)})\)</span></li>
</ol>
<p>For steps 1 and 2, you simply combine the sampled missing data and the observed data for a full dataset <span class="math inline">\(\mathbf{Y}\)</span> and sample from the full conditional distributions like normal.</p>
</section>
<section id="example" class="level2">
<h2 class="anchored" data-anchor-id="example">Example</h2>
<p>Let’s take a look at an example using a dataset with four health-related measurements on 200 women near Phoenix, Arizona. Notice that this dataset has missing values. We’ll assume that the data is <em>missing at random</em>, which is necessary for this analysis.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> Y.pima.miss</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Y) <span class="co"># Notice missing data</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>  glu bp skin  bmi
1  86 68   28 30.2
2 195 70   33   NA
3  77 82   NA 35.8
4  NA 76   43 47.9
5 107 60   NA   NA
6  97 76   27   NA</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(GGally)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressWarnings</span>(<span class="fu">ggpairs</span>(Y))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="7_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Gibbs sampling according to the 3-step scheme described above is implemented below. For priors, we set <span class="math inline">\(\boldsymbol{\mu}_0 = (120, 64, 26, 26)\)</span> assuming we know these are the national averages of the health measurements. We then (waving our hands a little) select prior variances that keep these measurements mostly around zero and only lightly centered around our estimates.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gibbs sampling</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(Y)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">ncol</span>(Y)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">120</span>, <span class="dv">64</span>, <span class="dv">26</span>, <span class="dv">26</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>sd0 <span class="ot">=</span> mu0 <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Setting prior on covariance</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>L0 <span class="ot">=</span> <span class="fu">matrix</span>(.<span class="dv">1</span>, p, p)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="fu">diag</span>(L0) <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>L0 <span class="ot">=</span> L0 <span class="sc">*</span> <span class="fu">outer</span>(sd0, sd0)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(L0)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1]   [,2]  [,3]  [,4]
[1,] 3600  192.0  78.0  78.0
[2,]  192 1024.0  41.6  41.6
[3,]   78   41.6 169.0  16.9
[4,]   78   41.6  16.9 169.0</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Variance prior lightly concentrated, where nu0 &gt; p is nu0 = p + 2</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>nu0 <span class="ot">=</span> p <span class="sc">+</span> <span class="dv">2</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale matrix - set to be the same as L0</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>S0 <span class="ot">=</span> L0</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set starting values</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">=</span> S0</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>Y.full <span class="ot">=</span> Y</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>O <span class="ot">=</span> <span class="dv">1</span> <span class="sc">*</span> (<span class="sc">!</span><span class="fu">is.na</span>(Y))  <span class="co"># Get NOT NAs, coerce to int via * 1</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>p) { <span class="co"># Looping through columns</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># For missing values in column, set to mean of the observed values</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>  mean.wo.na <span class="ot">=</span> <span class="fu">mean</span>(Y.full[, j], <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Rows: all of those that are NA, and the jth column, set it to this mean</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>  Y.full[<span class="fu">is.na</span>(Y.full[, j]), j] <span class="ot">=</span> mean.wo.na</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Gibbs</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>THETA <span class="ot">=</span> SIGMA <span class="ot">=</span> Y.MISS <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>) {</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update theta: step 1 of p.117 which is the same as previous</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>  ybar <span class="ot">=</span> <span class="fu">colMeans</span>(Y.full)</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>  Ln <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">inv</span>(L0) <span class="sc">+</span> n <span class="sc">*</span> <span class="fu">inv</span>(Sigma))</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>  mun <span class="ot">=</span> Ln <span class="sc">%*%</span> (<span class="fu">inv</span>(L0) <span class="sc">%*%</span> mu0 <span class="sc">+</span> n <span class="sc">*</span> <span class="fu">inv</span>(Sigma) <span class="sc">%*%</span> ybar)</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>  theta <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="dv">1</span>, mun, Ln)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update sigma: step 2 of p.117, same as previous</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>  resid <span class="ot">=</span> <span class="fu">t</span>(Y.full) <span class="sc">-</span> <span class="fu">c</span>(theta)</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>  Stheta <span class="ot">=</span> resid <span class="sc">%*%</span> <span class="fu">t</span>(resid)</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>  Sn <span class="ot">=</span> S0 <span class="sc">+</span> Stheta</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">rWishart</span>(<span class="dv">1</span>, nu0 <span class="sc">+</span> n, <span class="fu">inv</span>(Sn))[, , <span class="dv">1</span>])</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update ymiss: step 3 of p.117, requires eqs 7.10, 7.11</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Loop through rows of data (takes longer!!), need to sample rows individually</span></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>  <span class="co"># (independent) top of p.118</span></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Skip if we already have it</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">all</span>(O[i, ] <span class="sc">==</span> <span class="dv">1</span>)) {</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>      <span class="cf">next</span></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Partition b = NA rows, a = present rows</span></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Still works of a, b are empty, I presume</span></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>    oi <span class="ot">=</span> O[i, ]</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>    a <span class="ot">=</span> oi <span class="sc">==</span> <span class="dv">1</span></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>    b <span class="ot">=</span> oi <span class="sc">==</span> <span class="dv">0</span></span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Now we want to sample yb | ya, Sigma, Theta</span></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># \Sigma_{[a, a]}^{-1}, used in eqs 7.10 AND 7.11 (so calc once)</span></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>    iSa <span class="ot">=</span> <span class="fu">inv</span>(Sigma[a, a])</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calcualte \Sigma_{[b, a]}(\Sigma_{[a, a]})^{-1} used in 7.10 AND 7.11</span></span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>    beta.j <span class="ot">=</span> Sigma[b, a] <span class="sc">%*%</span> iSa</span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate Sigma.j (7.11). Start with covariacne matrix for the missing</span></span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># vars, then influence by beta (decrease variance)</span></span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>    Sigma.j <span class="ot">=</span> Sigma[b, b] <span class="sc">-</span> beta.j <span class="sc">%*%</span> Sigma[a, b]</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate theta.j (7.10). Start with standard theta, then change based on</span></span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># residuals of other vals and what we wknow about covariances</span></span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>    yi <span class="ot">=</span> Y.full[i, ]</span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>    theta.j <span class="ot">=</span> theta[b] <span class="sc">+</span> beta.j <span class="sc">%*%</span> <span class="fu">t</span>(yi[a] <span class="sc">-</span> theta[a])</span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Now we have samples for subset of b. Preserver order, now sample</span></span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>    Y.full[i, b] <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="dv">1</span>, theta.j, Sigma.j)</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Concat</span></span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>  THETA <span class="ot">=</span> <span class="fu">rbind</span>(THETA, theta)</span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a>  SIGMA <span class="ot">=</span> <span class="fu">rbind</span>(SIGMA, <span class="fu">c</span>(Sigma))</span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a>  Y.MISS <span class="ot">=</span> <span class="fu">rbind</span>(Y.MISS, Y.full[O <span class="sc">==</span> <span class="dv">0</span>])</span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Means and confidence intervals for posterior means</span></span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(Y)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "glu"  "bp"   "skin" "bmi" </code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(THETA)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 123.51682  71.04978  29.38495  32.18491</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apply</span>(THETA, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> <span class="cf">function</span>(d) <span class="fu">quantile</span>(d, <span class="at">prob =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]     [,2]     [,3]     [,4]
2.5%  118.7039 69.41573 27.62746 31.33561
50%   123.5534 71.05000 29.39057 32.18047
97.5% 127.8140 72.63692 31.05039 33.04161</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample correlation matrices</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>COR <span class="ot">=</span> <span class="fu">array</span>(<span class="at">dim =</span> <span class="fu">c</span>(p, p, <span class="dv">1000</span>))</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(SIGMA)) {</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># It's in rows right now, refold into matrix</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>  Sig <span class="ot">=</span> <span class="fu">matrix</span>(SIGMA[s, ], <span class="at">nrow =</span> p, <span class="at">ncol =</span> p)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate correlation matrix: (</span><span class="al">TODO</span><span class="co">: figure out how this works)</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>  COR[, , s] <span class="ot">=</span> Sig <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(Sig) <span class="sc">%o%</span> <span class="fu">diag</span>(Sig))</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Mean correlations from the correlation matrix samples. Can also calculate</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="co"># confidence intervals with quantile</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="fu">apply</span>(COR, <span class="at">MARGIN =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">FUN =</span> mean)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]      [,2]      [,3]      [,4]
[1,] 1.0000000 0.2236531 0.2475843 0.1888097
[2,] 0.2236531 1.0000000 0.2478914 0.2349531
[3,] 0.2475843 0.2478914 1.0000000 0.6510103
[4,] 0.1888097 0.2349531 0.6510103 1.0000000</code></pre>
</div>
</div>
</section>
</section>
<section id="exercises" class="level1">
<h1>Exercises</h1>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section">7.1</h2>
<section id="a" class="level3">
<h3 class="anchored" data-anchor-id="a">a</h3>
<p>Since the density is uniform with respect to <span class="math inline">\(\boldsymbol{\theta}\)</span>, the integral over the support of this function is infinite and cannot be 1.</p>
</section>
<section id="b" class="level3">
<h3 class="anchored" data-anchor-id="b">b</h3>
<p><span class="math display">\[\begin{align}
p_J(\boldsymbol{\theta}, \Sigma \mid \boldsymbol{y}_1, \dots, \boldsymbol{y}_n) &amp;\propto p(\boldsymbol{\theta}, \Sigma) \times p(\boldsymbol{y}_1, \dots, \boldsymbol{y}_n \mid \boldsymbol{\theta}, \Sigma) \\
&amp;\propto \left[ | \Sigma |^{-(p + 2) / 2} \right] \times \left[ | \Sigma |^{-n / 2} \exp\left( -\text{tr}(\mathbf{S}_\theta \Sigma^{-1}) \right) \right] \\
&amp;\propto | \Sigma |^{-(p + n + 2) / 2} \exp \left( -\text{tr}(\mathbf{S}_\theta \Sigma^{-1}) / 2 \right)
\end{align}\]</span></p>
<p>To obtain the full conditionals of a parameter, we treat the other parameters as constant, so</p>
<p><span class="math display">\[\begin{align}
p_J(\boldsymbol{\theta} \mid \boldsymbol{y}_1, \dots, \boldsymbol{y}_n, \Sigma) &amp;\propto \exp(- \text{tr}(\mathbf{S}_\theta \Sigma^{-1}) / 2) \\
&amp;= \exp(-\sum_{i = 1}^n (\boldsymbol{y}_i - \theta)^T \Sigma^{-1} (\boldsymbol{y}_i - \theta) / 2 ) &amp; \text{Expand back} \\
&amp;= \exp(- n (\bar{\boldsymbol{y}} - \theta)^T \Sigma^{-1} (\bar{\boldsymbol{y}} - \theta) / 2 ) \\
&amp;= \text{dnormal}(\bar{\boldsymbol{y}}, \Sigma / n)
\end{align}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align}
p_J(\Sigma \mid \boldsymbol{y}_1, \dots, \boldsymbol{y}_n, \boldsymbol{\theta}) &amp;\propto | \Sigma | ^{-(p + n + 2) / 2 } \exp(- \text{tr}(\mathbf{S}_\theta \Sigma^{-1}) / 2) \\
&amp;\propto \text{dinverse-wishart}\left(n + 1, \mathbf{S}_\theta^{-1} \right)
\end{align}\]</span></p>
</section>
</section>
<section id="section-1" class="level2">
<h2 class="anchored" data-anchor-id="section-1">7.3</h2>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>bluecrab <span class="ot">=</span> <span class="fu">as.matrix</span>(<span class="fu">read.table</span>(<span class="st">'Exercises/bluecrab.dat'</span>))</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>orangecrab <span class="ot">=</span> <span class="fu">as.matrix</span>(<span class="fu">read.table</span>(<span class="st">'Exercises/orangecrab.dat'</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="a-1" class="level3">
<h3 class="anchored" data-anchor-id="a-1">a</h3>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>crab.mcmc <span class="ot">=</span> <span class="fu">lapply</span>(<span class="fu">list</span>(<span class="st">'bluecrab'</span> <span class="ot">=</span> bluecrab, <span class="st">'orangecrab'</span> <span class="ot">=</span> orangecrab), <span class="cf">function</span>(crab) {</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="fu">ncol</span>(crab)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="fu">nrow</span>(crab)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>  ybar <span class="ot">=</span> <span class="fu">colMeans</span>(crab)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Prior parameters</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>  mu0 <span class="ot">=</span> ybar</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>  lambda0 <span class="ot">=</span> s0 <span class="ot">=</span> <span class="fu">cov</span>(crab)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>  nu0 <span class="ot">=</span> <span class="dv">4</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>  S <span class="ot">=</span> <span class="dv">10000</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>  THETA <span class="ot">=</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> S, <span class="at">ncol =</span> p)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>  SIGMA <span class="ot">=</span> <span class="fu">array</span>(<span class="at">dim =</span> <span class="fu">c</span>(p, p, S))</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Start with sigma sample</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>  sigma <span class="ot">=</span> s0</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Gibbs sampling</span></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(MASS)</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Also, inv = solve to make it more readable</span></span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>  inv <span class="ot">=</span> solve</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>S) {</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update theta</span></span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>    lambdan <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">inv</span>(lambda0) <span class="sc">+</span> n <span class="sc">*</span> <span class="fu">inv</span>(sigma))</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>    mun <span class="ot">=</span> lambdan <span class="sc">%*%</span> (<span class="fu">inv</span>(lambda0) <span class="sc">%*%</span> mu0 <span class="sc">+</span> n <span class="sc">*</span> <span class="fu">inv</span>(sigma) <span class="sc">%*%</span> ybar)</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>    theta <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">1</span>, mun, lambdan)</span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update sigma</span></span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>    resid <span class="ot">=</span> <span class="fu">t</span>(crab) <span class="sc">-</span> <span class="fu">c</span>(theta)</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>    stheta <span class="ot">=</span> resid <span class="sc">%*%</span> <span class="fu">t</span>(resid)</span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>    sn <span class="ot">=</span> s0 <span class="sc">+</span> stheta</span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>    sigma <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">rWishart</span>(<span class="dv">1</span>, nu0 <span class="sc">+</span> n, <span class="fu">inv</span>(sn))[, , <span class="dv">1</span>])</span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a>    THETA[s, ] <span class="ot">=</span> theta</span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a>    SIGMA[, , s] <span class="ot">=</span> sigma</span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">theta =</span> THETA, <span class="at">sigma =</span> SIGMA)</span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a>})</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="b-1" class="level3">
<h3 class="anchored" data-anchor-id="b-1">b</h3>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="7_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>There is strong evidence that orange crabs tend to be larger in both measurements than blue crabs:</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(orangecrab.df<span class="sc">$</span>theta1 <span class="sc">&gt;</span> bluecrab.df<span class="sc">$</span>theta1)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9013</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(orangecrab.df<span class="sc">$</span>theta2 <span class="sc">&gt;</span> bluecrab.df<span class="sc">$</span>theta2)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9979</code></pre>
</div>
</div>
</section>
<section id="c" class="level3">
<h3 class="anchored" data-anchor-id="c">c</h3>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>bluecrab.cor <span class="ot">=</span> <span class="fu">apply</span>(crab.mcmc<span class="sc">$</span>bluecrab<span class="sc">$</span>sigma, <span class="at">MARGIN =</span> <span class="dv">3</span>, <span class="at">FUN =</span> <span class="cf">function</span>(covmat) {</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>  covmat[<span class="dv">1</span>, <span class="dv">2</span>] <span class="sc">/</span> (<span class="fu">sqrt</span>(covmat[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">*</span> covmat[<span class="dv">2</span>, <span class="dv">2</span>]))</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>orangecrab.cor <span class="ot">=</span> <span class="fu">apply</span>(crab.mcmc<span class="sc">$</span>orangecrab<span class="sc">$</span>sigma, <span class="at">MARGIN =</span> <span class="dv">3</span>, <span class="at">FUN =</span> <span class="cf">function</span>(covmat) {</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>  covmat[<span class="dv">1</span>, <span class="dv">2</span>] <span class="sc">/</span> (<span class="fu">sqrt</span>(covmat[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">*</span> covmat[<span class="dv">2</span>, <span class="dv">2</span>]))</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>cor.df <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">species =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">'blue'</span>, <span class="fu">length</span>(bluecrab.cor)), <span class="fu">rep</span>(<span class="st">'orange'</span>, <span class="fu">length</span>(orangecrab.cor))),</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>                    <span class="at">cor =</span> <span class="fu">c</span>(bluecrab.cor, orangecrab.cor))</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(cor.df, <span class="fu">aes</span>(<span class="at">x =</span> cor, <span class="at">fill =</span> species)) <span class="sc">+</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">'blue'</span>, <span class="st">'orange'</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="7_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(bluecrab.cor <span class="sc">&lt;</span> orangecrab.cor)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.9899</code></pre>
</div>
</div>
<p>The orange crab species appears to have a much higher correlation between its two measurements than the blue crab species.</p>
</section>
</section>
<section id="section-2" class="level2">
<h2 class="anchored" data-anchor-id="section-2">7.4</h2>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>agehw <span class="ot">=</span> <span class="fu">as.matrix</span>(<span class="fu">read.table</span>(<span class="st">'Exercises/agehw.dat'</span>))</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(agehw) <span class="ot">=</span> agehw[<span class="dv">1</span>, ]</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>agehw <span class="ot">=</span> agehw[<span class="sc">-</span><span class="dv">1</span>, ]</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>agehw <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">as.numeric</span>(agehw), <span class="at">nrow =</span> <span class="dv">100</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="a-2" class="level3">
<h3 class="anchored" data-anchor-id="a-2">a</h3>
<p>With typical intuitions about life expectancy and age of marriage, I suspect that the ages of most of the married couples will fall between 25 and 80. There may be slight age differences among men and women, but not enough to warrant encoding in the prior. Thus <span class="math inline">\(\boldsymbol{\mu}_0 = ((25 + 80) / 2, (25 + 80) / 2) = (52.5, 52.5)^T\)</span> is set.</p>
<p>A semiconjugate prior distribution is chosen in this case, as it seems intuitive that there are fewer married couples at ages 25 and 80 than there are married couples around age 50, thus justifying a bell curve centered around <span class="math inline">\(52.5\)</span> with variance <span class="math inline">\(13.75^2 \approx 189\)</span> such that approximately 95% of the prior falls within the range <span class="math inline">\((25, 80)\)</span>.</p>
<p>The ages of the couples are expected to be quite tightly correlated, so knowing the above variance, a prior correlation of <span class="math inline">\(0.75\)</span> is targeted. Solving the correlation equation gives</p>
<p><span class="math display">\[\begin{align}
&amp; 0.75 = \frac{\sigma_{1, 2}}{189} \\
\implies&amp; \sigma_{1, 2} = 141.75
\end{align}\]</span></p>
<p>Thus <span class="math display">\[\Lambda_0 = \begin{bmatrix} 189 &amp; 141.75 \\ 141.75 &amp; 189 \end{bmatrix} \]</span> is set.</p>
<p>Like previous problems, for the variance, <span class="math inline">\(\mathbf{S}_0^{-1} =
\Lambda_0\)</span> and <span class="math inline">\(\nu_0 = p + 2 = 4\)</span> are set. Note that this only loosely centers the covariance matrix prior on <span class="math inline">\(\Lambda_0\)</span>, representing a conservative approach in terms of belief in the variance and the correlation between the ages.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> agehw</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">ncol</span>(agehw)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(agehw)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>ybar <span class="ot">=</span> <span class="fu">colMeans</span>(agehw)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="fu">rep</span>(<span class="fl">52.5</span>, p)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>lambda0 <span class="ot">=</span> s0 <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">189</span>, <span class="fl">141.75</span>), <span class="fu">c</span>(<span class="fl">141.75</span>, <span class="dv">189</span>))</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="co"># nu0 = p + 2</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>nu0 <span class="ot">=</span> p <span class="sc">+</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">10</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="b-2" class="level3">
<h3 class="anchored" data-anchor-id="b-2">b</h3>
<p>The wording of the question is interesting - the assumption is to sample a fixed <span class="math inline">\(\boldsymbol{\theta}, \Sigma\)</span> and from there sample <span class="math inline">\(100\)</span> points all with the same parameters. An alternative approach would be to sample a new data point for each sample of <span class="math inline">\(\boldsymbol{\theta}, \Sigma\)</span>.</p>
<p>In fact, because of that wording, <span class="math inline">\(\nu_0 = p + 2 = 4\)</span> was originally set to loosely center the prior. But given that this variance will often produce uncorrelated prior predictive datasets, <span class="math inline">\(\nu_0\)</span> is increased slightly.</p>
<p>After increasing <span class="math inline">\(\nu_0\)</span>, the posterior predictive datasets appear satisfactory.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>S <span class="ot">=</span> <span class="dv">12</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>Y_preds <span class="ot">=</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span>S, <span class="cf">function</span>(s) {</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Sample THETA according to prior</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>  theta <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">1</span>, mu0, lambda0)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>  sigma <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">rWishart</span>(<span class="dv">1</span>, nu0, <span class="fu">inv</span>(s0))[, , <span class="dv">1</span>])</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>  Y_s <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, theta, sigma)</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">Y1 =</span> Y_s[, <span class="dv">1</span>], <span class="at">Y2 =</span> Y_s[, <span class="dv">2</span>], <span class="at">dataset =</span> s)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>Y_comb <span class="ot">=</span> <span class="fu">do.call</span>(rbind, Y_preds)</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(Y_comb, <span class="fu">aes</span>(<span class="at">x =</span> Y1, <span class="at">y =</span> Y2)) <span class="sc">+</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> dataset)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="7_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="c-1" class="level3">
<h3 class="anchored" data-anchor-id="c-1">c</h3>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>S <span class="ot">=</span> <span class="dv">10000</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Reuse this since we'll need to specify different priors</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>do_mcmc <span class="ot">=</span> <span class="cf">function</span>(Y, mu0, lambda0, s0, nu0) {</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>  ybar <span class="ot">=</span> <span class="fu">colMeans</span>(Y)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="fu">ncol</span>(Y)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="fu">nrow</span>(Y)</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>  THETA <span class="ot">=</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> S, <span class="at">ncol =</span> p)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>  SIGMA <span class="ot">=</span> <span class="fu">array</span>(<span class="at">dim =</span> <span class="fu">c</span>(p, p, S))</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Start with sigma sample</span></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>  sigma <span class="ot">=</span> <span class="fu">cov</span>(Y)</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Gibbs sampling</span></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Also, inv = solve to make it more readable</span></span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>  inv <span class="ot">=</span> solve</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>S) {</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update theta</span></span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>    lambdan <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">inv</span>(lambda0) <span class="sc">+</span> n <span class="sc">*</span> <span class="fu">inv</span>(sigma))</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>    mun <span class="ot">=</span> lambdan <span class="sc">%*%</span> (<span class="fu">inv</span>(lambda0) <span class="sc">%*%</span> mu0 <span class="sc">+</span> n <span class="sc">*</span> <span class="fu">inv</span>(sigma) <span class="sc">%*%</span> ybar)</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>    theta <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">1</span>, mun, lambdan)</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update sigma</span></span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a>    resid <span class="ot">=</span> <span class="fu">t</span>(Y) <span class="sc">-</span> <span class="fu">c</span>(theta)</span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>    stheta <span class="ot">=</span> resid <span class="sc">%*%</span> <span class="fu">t</span>(resid)</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>    sn <span class="ot">=</span> s0 <span class="sc">+</span> stheta</span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>    sigma <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">rWishart</span>(<span class="dv">1</span>, nu0 <span class="sc">+</span> n, <span class="fu">inv</span>(sn))[, , <span class="dv">1</span>])</span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a>    THETA[s, ] <span class="ot">=</span> theta</span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a>    SIGMA[, , s] <span class="ot">=</span> sigma</span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">theta =</span> THETA, <span class="at">sigma =</span> SIGMA)</span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-39"><a href="#cb40-39" aria-hidden="true" tabindex="-1"></a>my_prior_mcmc <span class="ot">=</span> <span class="fu">do_mcmc</span>(agehw, mu0, lambda0, s0, nu0)</span>
<span id="cb40-40"><a href="#cb40-40" aria-hidden="true" tabindex="-1"></a>THETA <span class="ot">=</span> my_prior_mcmc<span class="sc">$</span>theta</span>
<span id="cb40-41"><a href="#cb40-41" aria-hidden="true" tabindex="-1"></a>SIGMA <span class="ot">=</span> my_prior_mcmc<span class="sc">$</span>sigma</span>
<span id="cb40-42"><a href="#cb40-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-43"><a href="#cb40-43" aria-hidden="true" tabindex="-1"></a><span class="co"># For reuse later</span></span>
<span id="cb40-44"><a href="#cb40-44" aria-hidden="true" tabindex="-1"></a>print_quantiles <span class="ot">=</span> <span class="cf">function</span>(THETA, SIGMA) {</span>
<span id="cb40-45"><a href="#cb40-45" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="st">"Husband"</span>)</span>
<span id="cb40-46"><a href="#cb40-46" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">quantile</span>(THETA[, <span class="dv">1</span>], <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))) <span class="co"># Husband</span></span>
<span id="cb40-47"><a href="#cb40-47" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="st">"Wife"</span>)</span>
<span id="cb40-48"><a href="#cb40-48" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">quantile</span>(THETA[, <span class="dv">2</span>], <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))) <span class="co"># Wife</span></span>
<span id="cb40-49"><a href="#cb40-49" aria-hidden="true" tabindex="-1"></a>  cors <span class="ot">=</span> <span class="fu">apply</span>(SIGMA, <span class="at">MARGIN =</span> <span class="dv">3</span>, <span class="at">FUN =</span> <span class="cf">function</span>(covmat) {</span>
<span id="cb40-50"><a href="#cb40-50" aria-hidden="true" tabindex="-1"></a>    covmat[<span class="dv">1</span>, <span class="dv">2</span>] <span class="sc">/</span> (<span class="fu">sqrt</span>(covmat[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">*</span> covmat[<span class="dv">2</span>, <span class="dv">2</span>]))</span>
<span id="cb40-51"><a href="#cb40-51" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb40-52"><a href="#cb40-52" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="st">"Correlation"</span>)</span>
<span id="cb40-53"><a href="#cb40-53" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">quantile</span>(cors, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>)))</span>
<span id="cb40-54"><a href="#cb40-54" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb40-55"><a href="#cb40-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-56"><a href="#cb40-56" aria-hidden="true" tabindex="-1"></a><span class="fu">print_quantiles</span>(THETA, SIGMA)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Husband"
    2.5%      50%    97.5% 
42.01685 44.52494 47.03880 
[1] "Wife"
    2.5%      50%    97.5% 
38.60299 40.99994 43.42255 
[1] "Correlation"
     2.5%       50%     97.5% 
0.8615416 0.9028502 0.9321153 </code></pre>
</div>
</div>
</section>
<section id="d" class="level3">
<h3 class="anchored" data-anchor-id="d">d</h3>
<p>Exercise 7.2 has not been completed, but doing Jeffreys’ prior and a “diffuse prior” below will still be helpful to see what effect prior information has.</p>
<section id="i" class="level4">
<h4 class="anchored" data-anchor-id="i">i</h4>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>THETA <span class="ot">=</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> S, <span class="at">ncol =</span> p)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>SIGMA <span class="ot">=</span> <span class="fu">array</span>(<span class="at">dim =</span> <span class="fu">c</span>(p, p, S))</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Start with sigma sample</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">=</span> <span class="fu">cov</span>(Y)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Gibbs sampling</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Also, inv = solve to make it more readable</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>inv <span class="ot">=</span> solve</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>S) {</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update theta</span></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>  theta <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">1</span>, ybar, sigma <span class="sc">/</span> n)</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update sigma</span></span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>  resid <span class="ot">=</span> <span class="fu">t</span>(Y) <span class="sc">-</span> <span class="fu">c</span>(theta)</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>  stheta <span class="ot">=</span> resid <span class="sc">%*%</span> <span class="fu">t</span>(resid)</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>  sigma <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">rWishart</span>(<span class="dv">1</span>, n <span class="sc">+</span> <span class="dv">1</span>, <span class="fu">inv</span>(stheta))[, , <span class="dv">1</span>])</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>  THETA[s, ] <span class="ot">=</span> theta</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>  SIGMA[, , s] <span class="ot">=</span> sigma</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a><span class="fu">print_quantiles</span>(THETA, SIGMA)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Husband"
    2.5%      50%    97.5% 
41.73803 44.42889 47.15085 
[1] "Wife"
    2.5%      50%    97.5% 
38.37376 40.91439 43.48397 
[1] "Correlation"
     2.5%       50%     97.5% 
0.8608382 0.9038100 0.9347940 </code></pre>
</div>
</div>
</section>
<section id="ii" class="level4">
<h4 class="anchored" data-anchor-id="ii">ii</h4>
<p>Unit information prior (skipping)</p>
</section>
<section id="iii" class="level4">
<h4 class="anchored" data-anchor-id="iii">iii</h4>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, p)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>lambda0 <span class="ot">=</span> <span class="dv">10</span><span class="sc">^</span><span class="dv">5</span> <span class="sc">*</span> <span class="fu">diag</span>(p)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>s0 <span class="ot">=</span> <span class="dv">1000</span> <span class="sc">*</span> <span class="fu">diag</span>(p)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>nu0 <span class="ot">=</span> <span class="dv">3</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>diffuse_mcmc <span class="ot">=</span> <span class="fu">do_mcmc</span>(agehw, mu0, lambda0, s0, nu0)</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print_quantiles</span>(diffuse_mcmc<span class="sc">$</span>theta, diffuse_mcmc<span class="sc">$</span>sigma)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Husband"
    2.5%      50%    97.5% 
41.67960 44.41405 47.18645 
[1] "Wife"
    2.5%      50%    97.5% 
38.32275 40.88109 43.51451 
[1] "Correlation"
     2.5%       50%     97.5% 
0.7931776 0.8552845 0.8996432 </code></pre>
</div>
</div>
</section>
</section>
<section id="e" class="level3">
<h3 class="anchored" data-anchor-id="e">e</h3>
<p>It doesn’t really seem like the prior information matters because the sample size is so large. Regardless of whether the prior is informative, the quantiles and correlations end up quite similar. Maybe the diffuse prior is slightly different, but it’s not a big difference.</p>
<p>If a smaller sample size is used, this may be different. The following shows what happens when the dataset is truncated to the first 25 variables and rerun with the informative prior and the diffuse prior:</p>
<section id="informative-prior" class="level4">
<h4 class="anchored" data-anchor-id="informative-prior">Informative prior</h4>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="fu">rep</span>(<span class="fl">52.5</span>, p)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>lambda0 <span class="ot">=</span> s0 <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">189</span>, <span class="fl">141.75</span>), <span class="fu">c</span>(<span class="fl">141.75</span>, <span class="dv">189</span>))</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="co"># nu0 = p + 2</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>nu0 <span class="ot">=</span> p <span class="sc">+</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">10</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>my_prior_mcmc_short <span class="ot">=</span> <span class="fu">do_mcmc</span>(agehw[<span class="dv">1</span><span class="sc">:</span><span class="dv">25</span>, ], mu0, lambda0, s0, nu0)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print_quantiles</span>(my_prior_mcmc_short<span class="sc">$</span>theta, my_prior_mcmc_short<span class="sc">$</span>sigma)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Husband"
    2.5%      50%    97.5% 
40.96477 45.37803 49.83724 
[1] "Wife"
    2.5%      50%    97.5% 
38.41850 43.04898 47.77481 
[1] "Correlation"
     2.5%       50%     97.5% 
0.8418117 0.9136244 0.9540250 </code></pre>
</div>
</div>
</section>
<section id="diffuse-prior" class="level4">
<h4 class="anchored" data-anchor-id="diffuse-prior">Diffuse prior</h4>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, p)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>lambda0 <span class="ot">=</span> <span class="dv">10</span><span class="sc">^</span><span class="dv">5</span> <span class="sc">*</span> <span class="fu">diag</span>(p)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>s0 <span class="ot">=</span> <span class="dv">1000</span> <span class="sc">*</span> <span class="fu">diag</span>(p)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>nu0 <span class="ot">=</span> <span class="dv">3</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>diffuse_mcmc_short <span class="ot">=</span> <span class="fu">do_mcmc</span>(agehw[<span class="dv">1</span><span class="sc">:</span><span class="dv">25</span>, ], mu0, lambda0, s0, nu0)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="fu">print_quantiles</span>(diffuse_mcmc_short<span class="sc">$</span>theta, diffuse_mcmc_short<span class="sc">$</span>sigma)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Husband"
    2.5%      50%    97.5% 
39.07635 45.12302 51.00340 
[1] "Wife"
    2.5%      50%    97.5% 
36.53189 42.74457 48.81278 
[1] "Correlation"
     2.5%       50%     97.5% 
0.5427696 0.7613713 0.8823101 </code></pre>
</div>
</div>
<p>In this case, the effect of the prior on correlation especially is easily observed. The correlation for the diffuse prior is quite low, as it is being dragged towards nothing.</p>


<!-- -->

</section>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb50" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Chapter 7: The multivariate normal model"</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Jesse Mu"</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "November 10, 2016"</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Setup --&gt;</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a><span class="in">```{r echo=FALSE, message=FALSE}</span></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span>opts_chunk<span class="sc">$</span><span class="fu">set</span>(<span class="at">fig.align =</span> <span class="st">'center'</span>, <span class="at">message =</span> <span class="cn">FALSE</span>)</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(knitr)</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cowplot)</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(reshape)</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Begin writing --&gt;</span></span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a><span class="fu"># The multivariate normal density</span></span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: reading comprehension</span></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a>We make the step to two variables by example. Consider a sample ($n = 22$) of</span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a>children who are given reading comprehension tests before and after receiving a</span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>particular instructional method. So each student has a before and after test</span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a>score. We can denote these *two* variables for student $i$ as a vector</span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a>$\mathbf{Y}_i$, where $Y_{i, 1}$ is the before score, and $Y_{i, 2}$ is the</span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>after score:</span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>\mathbf{Y}_i = \begin{pmatrix}</span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a>Y_{i, 1} <span class="sc">\\</span></span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a>Y_{i, 2} <span class="sc">\\</span></span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a>Recall </span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a>\text{Cor}(X, Y) &amp;= \frac{\text{Cov}(X, Y)}{\text{SD}(X)\text{SD}(Y)} <span class="sc">\\</span></span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\sigma_{1, 2}}{\sigma_1 \sigma_2} <span class="sc">\\</span></span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\sigma_{1, 2}}{\sqrt{\sigma_1^2 \sigma_2^2}}</span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-44"><a href="#cb50-44" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multivariate normal density</span></span>
<span id="cb50-45"><a href="#cb50-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-46"><a href="#cb50-46" aria-hidden="true" tabindex="-1"></a>If</span>
<span id="cb50-47"><a href="#cb50-47" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-48"><a href="#cb50-48" aria-hidden="true" tabindex="-1"></a>\mathbf{y} &amp;= \begin{pmatrix}</span>
<span id="cb50-49"><a href="#cb50-49" aria-hidden="true" tabindex="-1"></a>y_1 <span class="sc">\\</span></span>
<span id="cb50-50"><a href="#cb50-50" aria-hidden="true" tabindex="-1"></a>y_2 <span class="sc">\\</span></span>
<span id="cb50-51"><a href="#cb50-51" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb50-52"><a href="#cb50-52" aria-hidden="true" tabindex="-1"></a>y_p</span>
<span id="cb50-53"><a href="#cb50-53" aria-hidden="true" tabindex="-1"></a>\end{pmatrix} \sim \mathcal{N}(\boldsymbol{\theta}, \Sigma)</span>
<span id="cb50-54"><a href="#cb50-54" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-55"><a href="#cb50-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-56"><a href="#cb50-56" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb50-57"><a href="#cb50-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-58"><a href="#cb50-58" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-59"><a href="#cb50-59" aria-hidden="true" tabindex="-1"></a>\boldsymbol{\theta} &amp;= \begin{pmatrix}</span>
<span id="cb50-60"><a href="#cb50-60" aria-hidden="true" tabindex="-1"></a>\theta_1 <span class="sc">\\</span></span>
<span id="cb50-61"><a href="#cb50-61" aria-hidden="true" tabindex="-1"></a>\theta_2 <span class="sc">\\</span></span>
<span id="cb50-62"><a href="#cb50-62" aria-hidden="true" tabindex="-1"></a>\vdots <span class="sc">\\</span></span>
<span id="cb50-63"><a href="#cb50-63" aria-hidden="true" tabindex="-1"></a>\theta_p</span>
<span id="cb50-64"><a href="#cb50-64" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb50-65"><a href="#cb50-65" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-66"><a href="#cb50-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-67"><a href="#cb50-67" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb50-68"><a href="#cb50-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-69"><a href="#cb50-69" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-70"><a href="#cb50-70" aria-hidden="true" tabindex="-1"></a>\Sigma &amp;= \begin{pmatrix}</span>
<span id="cb50-71"><a href="#cb50-71" aria-hidden="true" tabindex="-1"></a>\sigma_1^2 &amp; \sigma_{1, 2} &amp; \cdots &amp; \sigma_{1, p} <span class="sc">\\</span></span>
<span id="cb50-72"><a href="#cb50-72" aria-hidden="true" tabindex="-1"></a>\sigma_{1, 2} &amp; \sigma_2^2 &amp; \cdots &amp; \sigma_{2, p} <span class="sc">\\</span></span>
<span id="cb50-73"><a href="#cb50-73" aria-hidden="true" tabindex="-1"></a>\vdots &amp; \vdots &amp; &amp; \vdots <span class="sc">\\</span></span>
<span id="cb50-74"><a href="#cb50-74" aria-hidden="true" tabindex="-1"></a>\sigma_{1, p} &amp; \cdots &amp; \cdots &amp; \sigma_p^2</span>
<span id="cb50-75"><a href="#cb50-75" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb50-76"><a href="#cb50-76" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-77"><a href="#cb50-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-78"><a href="#cb50-78" aria-hidden="true" tabindex="-1"></a>then</span>
<span id="cb50-79"><a href="#cb50-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-80"><a href="#cb50-80" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-81"><a href="#cb50-81" aria-hidden="true" tabindex="-1"></a>p(\mathbf{y} \mid \boldsymbol{\theta}, \Sigma) = (2\pi)^{-p / 2} | \Sigma |^{-1/2} \text{exp}\left( \frac{1}{2} -(\mathbf{y} - \boldsymbol{\theta})^T \Sigma^{-1} (\mathbf{y} - \boldsymbol{\theta}) \right)</span>
<span id="cb50-82"><a href="#cb50-82" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-83"><a href="#cb50-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-84"><a href="#cb50-84" aria-hidden="true" tabindex="-1"></a>It's useful to compare this to the single variable case. The first two terms </span>
<span id="cb50-85"><a href="#cb50-85" aria-hidden="true" tabindex="-1"></a>represent $1/\sqrt{2\pi\sigma^2}$ in the univariate case, generalized to an</span>
<span id="cb50-86"><a href="#cb50-86" aria-hidden="true" tabindex="-1"></a>arbitrary dimension: there is no longer the square root of the variance for a</span>
<span id="cb50-87"><a href="#cb50-87" aria-hidden="true" tabindex="-1"></a>single variable, but rather a $p$-dimensional covariance matrix raised to the</span>
<span id="cb50-88"><a href="#cb50-88" aria-hidden="true" tabindex="-1"></a>$1/2$ power. Furthermore, the quadratic term in the exponential $(y -</span>
<span id="cb50-89"><a href="#cb50-89" aria-hidden="true" tabindex="-1"></a>\theta)^2$ can be seen in the exponential here, but also included is the matrix</span>
<span id="cb50-90"><a href="#cb50-90" aria-hidden="true" tabindex="-1"></a>multiplication with the inverse of the covariance matrix.</span>
<span id="cb50-91"><a href="#cb50-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-92"><a href="#cb50-92" aria-hidden="true" tabindex="-1"></a><span class="in">```{r echo=FALSE}</span></span>
<span id="cb50-93"><a href="#cb50-93" aria-hidden="true" tabindex="-1"></a><span class="co"># Multivariate normal</span></span>
<span id="cb50-94"><a href="#cb50-94" aria-hidden="true" tabindex="-1"></a><span class="co"># Takes MATRICES of EXACTLY the same format as those in the book</span></span>
<span id="cb50-95"><a href="#cb50-95" aria-hidden="true" tabindex="-1"></a><span class="co"># So Theta must be a COLUMN vector (1d matrix), etc</span></span>
<span id="cb50-96"><a href="#cb50-96" aria-hidden="true" tabindex="-1"></a><span class="co"># SOLVE is INV in R</span></span>
<span id="cb50-97"><a href="#cb50-97" aria-hidden="true" tabindex="-1"></a>dmnorm <span class="ot">=</span> <span class="cf">function</span>(y, Theta, Sigma) {</span>
<span id="cb50-98"><a href="#cb50-98" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="fu">as.matrix</span>(y)</span>
<span id="cb50-99"><a href="#cb50-99" aria-hidden="true" tabindex="-1"></a>  Theta <span class="ot">=</span> <span class="fu">as.matrix</span>(Theta)</span>
<span id="cb50-100"><a href="#cb50-100" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">=</span> <span class="fu">as.matrix</span>(Sigma)</span>
<span id="cb50-101"><a href="#cb50-101" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="fu">nrow</span>(Theta)</span>
<span id="cb50-102"><a href="#cb50-102" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">2</span> <span class="sc">*</span> pi)<span class="sc">^</span>(<span class="sc">-</span>p <span class="sc">/</span> <span class="dv">2</span>) <span class="sc">*</span> <span class="fu">det</span>(Sigma)<span class="sc">^</span>(<span class="sc">-</span><span class="dv">1</span> <span class="sc">/</span> <span class="dv">2</span>) <span class="sc">*</span></span>
<span id="cb50-103"><a href="#cb50-103" aria-hidden="true" tabindex="-1"></a>    <span class="fu">exp</span>(<span class="sc">-</span><span class="fu">t</span>(y <span class="sc">-</span> Theta) <span class="sc">%*%</span> <span class="fu">solve</span>(Sigma) <span class="sc">%*%</span> (y <span class="sc">-</span> Theta) <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb50-104"><a href="#cb50-104" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb50-105"><a href="#cb50-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-106"><a href="#cb50-106" aria-hidden="true" tabindex="-1"></a>y1 <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">20</span>, <span class="dv">80</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb50-107"><a href="#cb50-107" aria-hidden="true" tabindex="-1"></a>y2 <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">20</span>, <span class="dv">80</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb50-108"><a href="#cb50-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-109"><a href="#cb50-109" aria-hidden="true" tabindex="-1"></a>dfs <span class="ot">=</span> <span class="fu">lapply</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">48</span>, <span class="dv">0</span>, <span class="dv">48</span>), <span class="cf">function</span>(cov) {</span>
<span id="cb50-110"><a href="#cb50-110" aria-hidden="true" tabindex="-1"></a>  Theta <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">50</span>, <span class="dv">50</span>)</span>
<span id="cb50-111"><a href="#cb50-111" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">64</span>, cov), <span class="fu">c</span>(cov, <span class="dv">144</span>))</span>
<span id="cb50-112"><a href="#cb50-112" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-113"><a href="#cb50-113" aria-hidden="true" tabindex="-1"></a>  calc.density <span class="ot">=</span> <span class="fu">Vectorize</span>(<span class="cf">function</span>(y1, y2) {</span>
<span id="cb50-114"><a href="#cb50-114" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> <span class="fu">c</span>(y1, y2)</span>
<span id="cb50-115"><a href="#cb50-115" aria-hidden="true" tabindex="-1"></a>    <span class="fu">dmnorm</span>(y, Theta, Sigma)</span>
<span id="cb50-116"><a href="#cb50-116" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb50-117"><a href="#cb50-117" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-118"><a href="#cb50-118" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">=</span> <span class="fu">outer</span>(y1, y2, <span class="at">FUN =</span> calc.density)</span>
<span id="cb50-119"><a href="#cb50-119" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-120"><a href="#cb50-120" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames</span>(d) <span class="ot">=</span> y1</span>
<span id="cb50-121"><a href="#cb50-121" aria-hidden="true" tabindex="-1"></a>  <span class="fu">colnames</span>(d) <span class="ot">=</span> y2</span>
<span id="cb50-122"><a href="#cb50-122" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-123"><a href="#cb50-123" aria-hidden="true" tabindex="-1"></a>  df <span class="ot">=</span> <span class="fu">melt</span>(d)</span>
<span id="cb50-124"><a href="#cb50-124" aria-hidden="true" tabindex="-1"></a>  <span class="fu">colnames</span>(df) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">'y1'</span>, <span class="st">'y2'</span>, <span class="st">'density'</span>)</span>
<span id="cb50-125"><a href="#cb50-125" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-126"><a href="#cb50-126" aria-hidden="true" tabindex="-1"></a>  df<span class="sc">$</span>cov <span class="ot">=</span> <span class="fu">as.character</span>(cov)</span>
<span id="cb50-127"><a href="#cb50-127" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-128"><a href="#cb50-128" aria-hidden="true" tabindex="-1"></a>  df</span>
<span id="cb50-129"><a href="#cb50-129" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb50-130"><a href="#cb50-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-131"><a href="#cb50-131" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">rbind</span>(dfs[[<span class="dv">1</span>]], dfs[[<span class="dv">2</span>]], dfs[[<span class="dv">3</span>]])</span>
<span id="cb50-132"><a href="#cb50-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-133"><a href="#cb50-133" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> y1, <span class="at">y =</span> y2, <span class="at">z =</span> density)) <span class="sc">+</span></span>
<span id="cb50-134"><a href="#cb50-134" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_contour</span>(<span class="fu">aes</span>(<span class="at">color =</span> ..level..)) <span class="sc">+</span></span>
<span id="cb50-135"><a href="#cb50-135" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> cov, <span class="at">scales =</span> <span class="st">'fixed'</span>) <span class="sc">+</span></span>
<span id="cb50-136"><a href="#cb50-136" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">20</span>, <span class="dv">80</span>)) <span class="sc">+</span></span>
<span id="cb50-137"><a href="#cb50-137" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">20</span>, <span class="dv">80</span>)) <span class="sc">+</span></span>
<span id="cb50-138"><a href="#cb50-138" aria-hidden="true" tabindex="-1"></a>  <span class="fu">guides</span>(<span class="at">color =</span> <span class="cn">FALSE</span>)</span>
<span id="cb50-139"><a href="#cb50-139" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-140"><a href="#cb50-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-141"><a href="#cb50-141" aria-hidden="true" tabindex="-1"></a><span class="fu"># A semiconjugate prior distribution for the mean</span></span>
<span id="cb50-142"><a href="#cb50-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-143"><a href="#cb50-143" aria-hidden="true" tabindex="-1"></a>At this point we have given up on computing a full closed form solution for</span>
<span id="cb50-144"><a href="#cb50-144" aria-hidden="true" tabindex="-1"></a>the posterior, as it's too complicated. Rather, if we compute conjugate priors</span>
<span id="cb50-145"><a href="#cb50-145" aria-hidden="true" tabindex="-1"></a>and posteriors for the full conditional distributions of the</span>
<span id="cb50-146"><a href="#cb50-146" aria-hidden="true" tabindex="-1"></a>$\boldsymbol{\theta}$ and $\Sigma$ separately, we can use Gibbs sampling to easily estimate the joint posterior distribution. Let's start by calculating $\boldsymbol{\theta}$ first:</span>
<span id="cb50-147"><a href="#cb50-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-148"><a href="#cb50-148" aria-hidden="true" tabindex="-1"></a>Analogous the univariate case, for the multivariate normal distribution, a conjugate</span>
<span id="cb50-149"><a href="#cb50-149" aria-hidden="true" tabindex="-1"></a>prior for the population mean is a multivariate normal.</span>
<span id="cb50-150"><a href="#cb50-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-151"><a href="#cb50-151" aria-hidden="true" tabindex="-1"></a>Let $\boldsymbol{\mu}_0$ be the prior mean, and $\Lambda_0$ be the covariance matrix of</span>
<span id="cb50-152"><a href="#cb50-152" aria-hidden="true" tabindex="-1"></a>$\boldsymbol{\mu}_0$. Then let $\boldsymbol{\theta} \sim \mathcal{N}(\boldsymbol{\mu}_0, \Lambda_0)$. What does this prior look like?</span>
<span id="cb50-153"><a href="#cb50-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-154"><a href="#cb50-154" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-155"><a href="#cb50-155" aria-hidden="true" tabindex="-1"></a>p(\boldsymbol{\theta} \mid \Sigma) &amp;\propto \text{exp}\left<span class="co">[</span><span class="ot"> -\frac{1}{2} (\boldsymbol{\theta} - \boldsymbol{\mu}_0)^T \Lambda_0^{-1} (\boldsymbol{\theta} - \boldsymbol{\mu}_0) \right</span><span class="co">]</span> &amp; \text{Drop constants} <span class="sc">\\</span></span>
<span id="cb50-156"><a href="#cb50-156" aria-hidden="true" tabindex="-1"></a>&amp;= \text{exp}\left<span class="co">[</span><span class="ot">-\frac{1}{2} (\boldsymbol{\theta}^T - \boldsymbol{\mu}_0^T) (\Lambda_0^{-1}\boldsymbol{\theta} - \Lambda_0^{-1}\boldsymbol{\mu}_0 ) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb50-157"><a href="#cb50-157" aria-hidden="true" tabindex="-1"></a>&amp;= \text{exp}\left<span class="co">[</span><span class="ot">-\frac{1}{2} (\boldsymbol{\theta}^T \Lambda_0^{-1} \boldsymbol{\theta} - \boldsymbol{\mu}_0^T \Lambda_0^{-1} \boldsymbol{\theta} - \boldsymbol{\theta}^T \Lambda_0^{-1} \boldsymbol{\mu}_0 + \boldsymbol{\mu}_0^T \Lambda_0^{-1}\boldsymbol{\mu}_0  )\right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb50-158"><a href="#cb50-158" aria-hidden="true" tabindex="-1"></a>&amp;= \text{exp} \left<span class="co">[</span><span class="ot"> -\frac{1}{2} (\boldsymbol{\theta}^T \Lambda_0^{-1} \boldsymbol{\theta} - 2\boldsymbol{\theta}^T \Lambda_0^{-1} \boldsymbol{\mu}_0 + \boldsymbol{\mu}_0^T \Lambda_0^{-1}\boldsymbol{\mu}_0 ) \right</span><span class="co">]</span> &amp; \text{Middle terms combine} <span class="sc">\\</span></span>
<span id="cb50-159"><a href="#cb50-159" aria-hidden="true" tabindex="-1"></a>&amp;\propto \text{exp} \left<span class="co">[</span><span class="ot"> -\frac{1}{2}\boldsymbol{\theta}^T \Lambda_0^{-1} \boldsymbol{\theta} + \boldsymbol{\theta}^T \Lambda_0^{-1} \boldsymbol{\mu}_0 \right</span><span class="co">]</span> &amp; \text{Discard last term} <span class="sc">\\</span></span>
<span id="cb50-160"><a href="#cb50-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-161"><a href="#cb50-161" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-162"><a href="#cb50-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-163"><a href="#cb50-163" aria-hidden="true" tabindex="-1"></a>If we let $\mathbf{A}_0 = \Lambda_0^{-1}$, i.e. the precision matrix (which</span>
<span id="cb50-164"><a href="#cb50-164" aria-hidden="true" tabindex="-1"></a>echoes the univariate case) and $\mathbf{b}_0 = \Lambda_0^{-1}</span>
<span id="cb50-165"><a href="#cb50-165" aria-hidden="true" tabindex="-1"></a>\boldsymbol{\mu}_0$, this simplifies to</span>
<span id="cb50-166"><a href="#cb50-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-167"><a href="#cb50-167" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-168"><a href="#cb50-168" aria-hidden="true" tabindex="-1"></a>p(\boldsymbol{\theta}) &amp;= \text{exp}\left( -\frac{1}{2}\boldsymbol{\theta}^T</span>
<span id="cb50-169"><a href="#cb50-169" aria-hidden="true" tabindex="-1"></a>\mathbf{A}_0 \boldsymbol{\theta} + \boldsymbol{\theta}^T \mathbf{b}_0 \right) <span class="sc">\\</span></span>
<span id="cb50-170"><a href="#cb50-170" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-171"><a href="#cb50-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-172"><a href="#cb50-172" aria-hidden="true" tabindex="-1"></a>We will see this simplified form show up when working with the sampling models</span>
<span id="cb50-173"><a href="#cb50-173" aria-hidden="true" tabindex="-1"></a>and posterior distribution as well.</span>
<span id="cb50-174"><a href="#cb50-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-175"><a href="#cb50-175" aria-hidden="true" tabindex="-1"></a>Let's first look at the sampling model. Doing similar simplifications, we get</span>
<span id="cb50-176"><a href="#cb50-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-177"><a href="#cb50-177" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-178"><a href="#cb50-178" aria-hidden="true" tabindex="-1"></a>p(\mathbf{y}_1, \dots, \mathbf{y}_n \mid \boldsymbol{\theta}, \Sigma) \propto</span>
<span id="cb50-179"><a href="#cb50-179" aria-hidden="true" tabindex="-1"></a>\text{exp}(-\frac{1}{2}\boldsymbol{\theta}^T \mathbf{A}_1 \boldsymbol{\theta} +</span>
<span id="cb50-180"><a href="#cb50-180" aria-hidden="true" tabindex="-1"></a>\boldsymbol{\theta}^T \mathbf{b}_1)</span>
<span id="cb50-181"><a href="#cb50-181" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-182"><a href="#cb50-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-183"><a href="#cb50-183" aria-hidden="true" tabindex="-1"></a>where this time $\mathbf{A}_1 = n\Sigma^{-1}$ and $\mathbf{b}_1 = n</span>
<span id="cb50-184"><a href="#cb50-184" aria-hidden="true" tabindex="-1"></a>\Sigma^{-1}\bar{\mathbf{y}}$. $\bar{\mathbf{y}}$ is the vector of sample</span>
<span id="cb50-185"><a href="#cb50-185" aria-hidden="true" tabindex="-1"></a>averages for each variable.</span>
<span id="cb50-186"><a href="#cb50-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-187"><a href="#cb50-187" aria-hidden="true" tabindex="-1"></a>So finally, we can calculate the posterior for $\boldsymbol{\theta}$:</span>
<span id="cb50-188"><a href="#cb50-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-189"><a href="#cb50-189" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-190"><a href="#cb50-190" aria-hidden="true" tabindex="-1"></a>p(\boldsymbol{\theta} \mid \mathbf{y}_1, \dots, \mathbf{y}_n, \Sigma) &amp;=</span>
<span id="cb50-191"><a href="#cb50-191" aria-hidden="true" tabindex="-1"></a>p(\boldsymbol{\theta} \mid \Sigma) p(\mathbf{y}_1, \dots, \mathbf{y}_n \mid</span>
<span id="cb50-192"><a href="#cb50-192" aria-hidden="true" tabindex="-1"></a>\boldsymbol{\theta}, \Sigma) / p(\mathbf{y}_1, \dots, \mathbf{y}_n \mid \Sigma)</span>
<span id="cb50-193"><a href="#cb50-193" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span></span>
<span id="cb50-194"><a href="#cb50-194" aria-hidden="true" tabindex="-1"></a>&amp;\propto p(\boldsymbol{\theta} \mid \Sigma) p(\mathbf{y}_1, \dots, \mathbf{y}_n \mid</span>
<span id="cb50-195"><a href="#cb50-195" aria-hidden="true" tabindex="-1"></a>\boldsymbol{\theta}, \Sigma) <span class="sc">\\</span></span>
<span id="cb50-196"><a href="#cb50-196" aria-hidden="true" tabindex="-1"></a>&amp;\propto</span>
<span id="cb50-197"><a href="#cb50-197" aria-hidden="true" tabindex="-1"></a>\text{exp}\left( -\frac{1}{2}\boldsymbol{\theta}^T</span>
<span id="cb50-198"><a href="#cb50-198" aria-hidden="true" tabindex="-1"></a>\mathbf{A}_0 \boldsymbol{\theta} + \boldsymbol{\theta}^T \mathbf{b}_0 \right) </span>
<span id="cb50-199"><a href="#cb50-199" aria-hidden="true" tabindex="-1"></a>\times \text{exp}(-\frac{1}{2}\boldsymbol{\theta}^T \mathbf{A}_1 \boldsymbol{\theta} +</span>
<span id="cb50-200"><a href="#cb50-200" aria-hidden="true" tabindex="-1"></a>\boldsymbol{\theta}^T \mathbf{b}_1) <span class="sc">\\</span></span>
<span id="cb50-201"><a href="#cb50-201" aria-hidden="true" tabindex="-1"></a>&amp;= \text{exp}\left(-\frac{1}{2} \boldsymbol{\theta}^T \mathbf{A}_n \boldsymbol{\theta} + \boldsymbol{\theta}^T \mathbf{b}_n \right)</span>
<span id="cb50-202"><a href="#cb50-202" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-203"><a href="#cb50-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-204"><a href="#cb50-204" aria-hidden="true" tabindex="-1"></a>where we have combined terms such that $\mathbf{A}_n = \mathbf{A}_0 + \mathbf{A}_1 = \Lambda_0^{-1} + n\Sigma^{-1}$ and $\mathbf{b}_n = \mathbf{b}_0 + \mathbf{b}_1 = \Lambda_0^{-1} \boldsymbol{\mu}_0 + n\Sigma^{-1} \bar{\mathbf{y}}$.</span>
<span id="cb50-205"><a href="#cb50-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-206"><a href="#cb50-206" aria-hidden="true" tabindex="-1"></a>So if $\boldsymbol{\theta} \mid \Sigma \sim \mathcal{N}(\boldsymbol{\mu}_0,</span>
<span id="cb50-207"><a href="#cb50-207" aria-hidden="true" tabindex="-1"></a>\Lambda_0)$, then $\boldsymbol{\theta} \mid \mathbf{y}_1, \dots, \mathbf{y}_n, \Sigma</span>
<span id="cb50-208"><a href="#cb50-208" aria-hidden="true" tabindex="-1"></a>\sim \mathcal{N}(\boldsymbol{\mu}_n, \Lambda_n)$ where the parameters are</span>
<span id="cb50-209"><a href="#cb50-209" aria-hidden="true" tabindex="-1"></a>defined as above.</span>
<span id="cb50-210"><a href="#cb50-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-211"><a href="#cb50-211" aria-hidden="true" tabindex="-1"></a>Then notice that, like the univariate case:</span>
<span id="cb50-212"><a href="#cb50-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-213"><a href="#cb50-213" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{Cov}(\boldsymbol{\theta} \mid \mathbf{y}_1, \dots, \mathbf{y}_n,</span>
<span id="cb50-214"><a href="#cb50-214" aria-hidden="true" tabindex="-1"></a>\Sigma) = \Lambda_n = (A_0^{-1} + n\Sigma^{-1})^{-1}$, a combination of prior and</span>
<span id="cb50-215"><a href="#cb50-215" aria-hidden="true" tabindex="-1"></a>posterior precision, and</span>
<span id="cb50-216"><a href="#cb50-216" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{E}(\boldsymbol{\theta} \mid \mathbf{y}_1, \dots, \mathbf{y}_n,</span>
<span id="cb50-217"><a href="#cb50-217" aria-hidden="true" tabindex="-1"></a>\Sigma) = \boldsymbol{\mu}_n = (A_0^{-1} + n\Sigma^{-1})^{-1} (\Lambda_0^{-1} \boldsymbol{\mu}_0 + n\Sigma^{-1}\bar{\boldsymbol{y}})$, weighted average of the prior estimate of the mean and the sample mean.</span>
<span id="cb50-218"><a href="#cb50-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-219"><a href="#cb50-219" aria-hidden="true" tabindex="-1"></a><span class="fu"># The inverse-Wishart distribution</span></span>
<span id="cb50-220"><a href="#cb50-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-221"><a href="#cb50-221" aria-hidden="true" tabindex="-1"></a>We've seen the (semi)conjugate prior and posterior distribution for the mean.</span>
<span id="cb50-222"><a href="#cb50-222" aria-hidden="true" tabindex="-1"></a>Now to the covariance matrix $\Sigma$. Remember that for the univariate normal</span>
<span id="cb50-223"><a href="#cb50-223" aria-hidden="true" tabindex="-1"></a>distribution, a semi-conjugate prior distribution for the variance was the </span>
<span id="cb50-224"><a href="#cb50-224" aria-hidden="true" tabindex="-1"></a>inverse-Gamma distribution. Similarly, for the multivariate case, a </span>
<span id="cb50-225"><a href="#cb50-225" aria-hidden="true" tabindex="-1"></a>semi-conjugate prior distribution for the covariance matrix is the inverse of </span>
<span id="cb50-226"><a href="#cb50-226" aria-hidden="true" tabindex="-1"></a>the multivariate analog of the Gamma distribution, known as a Wishart </span>
<span id="cb50-227"><a href="#cb50-227" aria-hidden="true" tabindex="-1"></a>distribution.</span>
<span id="cb50-228"><a href="#cb50-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-229"><a href="#cb50-229" aria-hidden="true" tabindex="-1"></a>Generating samples from a Wishart distribution is analogous to sampling a set of</span>
<span id="cb50-230"><a href="#cb50-230" aria-hidden="true" tabindex="-1"></a>variables from a multivariate normal distribution and calculating the empirical</span>
<span id="cb50-231"><a href="#cb50-231" aria-hidden="true" tabindex="-1"></a>covriance matrix of the samples. More specifically, with parameters $\nu_0 \in</span>
<span id="cb50-232"><a href="#cb50-232" aria-hidden="true" tabindex="-1"></a>\mathbb{Z}^+$ and $\Phi_0$ (a $p \times p$ covariance matrix),</span>
<span id="cb50-233"><a href="#cb50-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-234"><a href="#cb50-234" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Sample $z_1, \dots, z_{\nu_0} \sim \mathcal{N}(\boldsymbol{0}, \Phi_0)$</span>
<span id="cb50-235"><a href="#cb50-235" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Calculate $\boldsymbol{Z}^T \boldsymbol{Z} = \sum_{i = 1}^{\nu_0} z_i z_i^T$</span>
<span id="cb50-236"><a href="#cb50-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-237"><a href="#cb50-237" aria-hidden="true" tabindex="-1"></a>Then $\boldsymbol{Z}_1^T \boldsymbol{Z}_1, \dots, \boldsymbol{Z}_S^T \boldsymbol{Z} \sim \text{Wishart}(\nu_0, \Phi_0)$.</span>
<span id="cb50-238"><a href="#cb50-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-239"><a href="#cb50-239" aria-hidden="true" tabindex="-1"></a>Some properties of samples from the Wishart which happen to stem from calculating empirical covariance matrices in the first place</span>
<span id="cb50-240"><a href="#cb50-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-241"><a href="#cb50-241" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $\nu_0 &gt; p$ then $\boldsymbol{Z}^\boldsymbol{Z}$ is positive definite</span>
<span id="cb50-242"><a href="#cb50-242" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\boldsymbol{Z}^\boldsymbol{Z}$ is symmetric</span>
<span id="cb50-243"><a href="#cb50-243" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{E}(\boldsymbol{Z}^T\boldsymbol{Z}) = \nu_0 \Phi_0$</span>
<span id="cb50-244"><a href="#cb50-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-245"><a href="#cb50-245" aria-hidden="true" tabindex="-1"></a>Like how the Gamma is not the conjugate prior of the variance for the Normal,</span>
<span id="cb50-246"><a href="#cb50-246" aria-hidden="true" tabindex="-1"></a>the Wishart is not the conjugate prior of the variance for the multivariate</span>
<span id="cb50-247"><a href="#cb50-247" aria-hidden="true" tabindex="-1"></a>Normal; rather, the inverse-Wishart is. Sampling from an inverse-Wishart just involves taking $\Sigma^{(s)} = (\boldsymbol{Z}^{(s)T} \boldsymbol{Z}^{(s)})^{-1}$</span>
<span id="cb50-248"><a href="#cb50-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-249"><a href="#cb50-249" aria-hidden="true" tabindex="-1"></a>If we're sampling from an inverse-Wishart we rename $\Phi_0$ to $\mathbf{S}_0^{-1}$ such that</span>
<span id="cb50-250"><a href="#cb50-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-251"><a href="#cb50-251" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{E}(\Sigma^{-1}) = \nu_0 \mathbf{S}_0^{-1}$ (instead of $\Phi_0$)</span>
<span id="cb50-252"><a href="#cb50-252" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{E}(\Sigma) = \frac{1}{\nu_0 - p - 1} \mathbf{S}_0$ (so not exactly the inverse of $\mathbf{S}_0^{-1}$)</span>
<span id="cb50-253"><a href="#cb50-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-254"><a href="#cb50-254" aria-hidden="true" tabindex="-1"></a><span class="fu">## Specifying parameters</span></span>
<span id="cb50-255"><a href="#cb50-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-256"><a href="#cb50-256" aria-hidden="true" tabindex="-1"></a>Specifying parameters is somewhat interesting for the inverse-Wishart becase</span>
<span id="cb50-257"><a href="#cb50-257" aria-hidden="true" tabindex="-1"></a>there are many of them - we need to specify the entire covariance matrix. If we</span>
<span id="cb50-258"><a href="#cb50-258" aria-hidden="true" tabindex="-1"></a>have a prior expectation of a covariance matrix $\Sigma_0$, then we can center</span>
<span id="cb50-259"><a href="#cb50-259" aria-hidden="true" tabindex="-1"></a>our prior around it in two suggested ways:</span>
<span id="cb50-260"><a href="#cb50-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-261"><a href="#cb50-261" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Set $\nu_0$ large and set $\mathbf{S}_0 = (\nu_0 - p - 1) \Sigma_0$, such that</span>
<span id="cb50-262"><a href="#cb50-262" aria-hidden="true" tabindex="-1"></a>$\mathbb{E}(\Sigma) = \frac{\nu_0 - p - 1}{\nu_0 - p - 1}\Sigma_0 = \Sigma_0$</span>
<span id="cb50-263"><a href="#cb50-263" aria-hidden="true" tabindex="-1"></a>and (due to large $\nu_0$) the prior is fairly concentrated around $\Sigma_0$;</span>
<span id="cb50-264"><a href="#cb50-264" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Set $\nu_0 = p + 2$ and let $\mathbf{S}_0 = \Sigma_0$, such that</span>
<span id="cb50-265"><a href="#cb50-265" aria-hidden="true" tabindex="-1"></a>$\mathbb{E}(\Sigma) = \frac{1}{p + 2 - p - 1}\Sigma_0 = \Sigma_0$ but only</span>
<span id="cb50-266"><a href="#cb50-266" aria-hidden="true" tabindex="-1"></a>loosely centered around $\Sigma_0$ (due to fairly small $\nu_0$)</span>
<span id="cb50-267"><a href="#cb50-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-268"><a href="#cb50-268" aria-hidden="true" tabindex="-1"></a>For an "empirical Bayes" approach,  we can center our prior around the empirical</span>
<span id="cb50-269"><a href="#cb50-269" aria-hidden="true" tabindex="-1"></a>covariance matrix of our sample.</span>
<span id="cb50-270"><a href="#cb50-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-271"><a href="#cb50-271" aria-hidden="true" tabindex="-1"></a><span class="fu">## Full conditional distribution of $\Sigma \mid \boldsymbol{y}_1, \dots, \boldsymbol{y}_n, \boldsymbol{\theta}$</span></span>
<span id="cb50-272"><a href="#cb50-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-273"><a href="#cb50-273" aria-hidden="true" tabindex="-1"></a>There is an intimidating normalizing constant for the inverse-Wishart. All we need to know is if $\Sigma \sim \text{inverse-Wishart}(\nu_0, \mathbf{S}_0^{-1})$,</span>
<span id="cb50-274"><a href="#cb50-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-275"><a href="#cb50-275" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-276"><a href="#cb50-276" aria-hidden="true" tabindex="-1"></a>p(\Sigma) \propto | \Sigma |^{-(\nu_0 + p + 1) / 2} \times \exp\left(-\text{tr}(\mathbf{S}_0 \Sigma^{-1})  / 2\right)</span>
<span id="cb50-277"><a href="#cb50-277" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-278"><a href="#cb50-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-279"><a href="#cb50-279" aria-hidden="true" tabindex="-1"></a>Recall the sampling model from earlier. We skipped some simplifications above</span>
<span id="cb50-280"><a href="#cb50-280" aria-hidden="true" tabindex="-1"></a>(substituting $\mathbf{A}_1$ and $\boldsymbol{b}_1$) so take it for granted that</span>
<span id="cb50-281"><a href="#cb50-281" aria-hidden="true" tabindex="-1"></a>a less-simplified form is</span>
<span id="cb50-282"><a href="#cb50-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-283"><a href="#cb50-283" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-284"><a href="#cb50-284" aria-hidden="true" tabindex="-1"></a>p(\mathbf{y}_1, \dots, \mathbf{y}_n \mid \boldsymbol{\theta}, \Sigma) &amp;= (2\pi)^{-np/2} | \Sigma |^{-n/2} \exp \left( - \sum_{i = 1}^n (\boldsymbol{y}_i - \boldsymbol{\theta})^T \Sigma^{-1} (\boldsymbol{y}_i - \boldsymbol{\theta}) / 2 \right)</span>
<span id="cb50-285"><a href="#cb50-285" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-286"><a href="#cb50-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-287"><a href="#cb50-287" aria-hidden="true" tabindex="-1"></a>Using some linear algebra,</span>
<span id="cb50-288"><a href="#cb50-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-289"><a href="#cb50-289" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-290"><a href="#cb50-290" aria-hidden="true" tabindex="-1"></a>\sum_{i = 1}^n (\boldsymbol{y}_i - \boldsymbol{\theta})^T \Sigma^{-1} (\boldsymbol{y}_i - \boldsymbol{\theta}) &amp;= \text{tr}\left( \left(\sum_{i = 1}^n (\boldsymbol{y}_i - \boldsymbol{\theta}) (\boldsymbol{y}_i - \boldsymbol{\theta})^T \right) \Sigma^{-1} \right)</span>
<span id="cb50-291"><a href="#cb50-291" aria-hidden="true" tabindex="-1"></a>\text{tr}\left( \mathbf{S}_{\theta} \Sigma^{-1} \right)</span>
<span id="cb50-292"><a href="#cb50-292" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-293"><a href="#cb50-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-294"><a href="#cb50-294" aria-hidden="true" tabindex="-1"></a>where $\mathbf{S}_{\theta} = \sum_{i = 1}^n (\boldsymbol{y}_i -</span>
<span id="cb50-295"><a href="#cb50-295" aria-hidden="true" tabindex="-1"></a>\boldsymbol{\theta}) (\boldsymbol{y}_i - \boldsymbol{\theta})^T$ is the</span>
<span id="cb50-296"><a href="#cb50-296" aria-hidden="true" tabindex="-1"></a>*residual sum of squares matrix* for the vectors $\boldsymbol{y}_1, \dots,</span>
<span id="cb50-297"><a href="#cb50-297" aria-hidden="true" tabindex="-1"></a>\boldsymbol{y}_n$. (To obtain the residual sum of squares matrix, you calculate</span>
<span id="cb50-298"><a href="#cb50-298" aria-hidden="true" tabindex="-1"></a>the sum of squares for the residual vectors $\boldsymbol{y}_i -</span>
<span id="cb50-299"><a href="#cb50-299" aria-hidden="true" tabindex="-1"></a>\boldsymbol{\theta}$)</span>
<span id="cb50-300"><a href="#cb50-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-301"><a href="#cb50-301" aria-hidden="true" tabindex="-1"></a>So</span>
<span id="cb50-302"><a href="#cb50-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-303"><a href="#cb50-303" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-304"><a href="#cb50-304" aria-hidden="true" tabindex="-1"></a>p(\mathbf{y}_1, \dots, \mathbf{y}_n \mid \boldsymbol{\theta}, \Sigma) &amp;= (2\pi)^{-np/2} | \Sigma |^{-n/2} \exp \left( - \text{tr}(\mathbf{S}_{\theta} \Sigma^{-1})  / 2 \right)</span>
<span id="cb50-305"><a href="#cb50-305" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-306"><a href="#cb50-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-307"><a href="#cb50-307" aria-hidden="true" tabindex="-1"></a>Now we can calculate the full conditional distribution of $\Sigma$:</span>
<span id="cb50-308"><a href="#cb50-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-309"><a href="#cb50-309" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-310"><a href="#cb50-310" aria-hidden="true" tabindex="-1"></a>p(\Sigma \mid \boldsymbol{y}_1, \dots, \boldsymbol{y}_n, \boldsymbol{\theta}) &amp;\propto p(\Sigma) \times p(\boldsymbol{y}_1, \dots, \boldsymbol{y}_n \mid \boldsymbol{\theta}, \Sigma) <span class="sc">\\</span></span>
<span id="cb50-311"><a href="#cb50-311" aria-hidden="true" tabindex="-1"></a>&amp;\propto \left<span class="co">[</span><span class="ot"> | \Sigma |^{-(\nu_0 + p + 1) / 2} \times \exp\left(-\text{tr}(\mathbf{S}_0 \Sigma^{-1})  / 2\right) \right</span><span class="co">]</span> \times</span>
<span id="cb50-312"><a href="#cb50-312" aria-hidden="true" tabindex="-1"></a>\left<span class="co">[</span><span class="ot"> | \Sigma |^{-n/2} \exp \left( - \text{tr}(\mathbf{S}_{\theta} \Sigma^{-1})  / 2 \right) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb50-313"><a href="#cb50-313" aria-hidden="true" tabindex="-1"></a>&amp;=  | \Sigma |^{-(\nu_0 + n + p + 1) / 2} \exp \left( -\text{tr}(\left( \mathbf{S}_0 + \mathbf{S}_{\theta} \right) \Sigma^{-1}) / 2 \right) <span class="sc">\\</span></span>
<span id="cb50-314"><a href="#cb50-314" aria-hidden="true" tabindex="-1"></a>&amp;\propto \text{dinverse-Wishart}\left(\nu_0 + n, \left<span class="co">[</span><span class="ot">\mathbf{S}_0 + \mathbf{S}_{\theta} \right</span><span class="co">]</span>^{-1} \right) <span class="sc">\\</span></span>
<span id="cb50-315"><a href="#cb50-315" aria-hidden="true" tabindex="-1"></a>&amp;= \text{dinverse-Wishart}\left(\nu_n, \mathbf{S}_n^{-1} \right)</span>
<span id="cb50-316"><a href="#cb50-316" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-317"><a href="#cb50-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-318"><a href="#cb50-318" aria-hidden="true" tabindex="-1"></a>where $\nu_n = \nu_0 + n$ and $\mathbf{S}_n = \mathbf{S}_0 + \mathbf{S}_{\theta}$.</span>
<span id="cb50-319"><a href="#cb50-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-320"><a href="#cb50-320" aria-hidden="true" tabindex="-1"></a>Like the univariate case, the conditional distribution on $\Sigma$ is dependent </span>
<span id="cb50-321"><a href="#cb50-321" aria-hidden="true" tabindex="-1"></a>on $\nu_0 + n$, a sum of the prior sample size and the data sample size, </span>
<span id="cb50-322"><a href="#cb50-322" aria-hidden="true" tabindex="-1"></a>$\textbf{S}_0 + \textbf{S}_\theta$, the sum of the "prior" residual sum of </span>
<span id="cb50-323"><a href="#cb50-323" aria-hidden="true" tabindex="-1"></a>squares and the empirical sum of squares.</span>
<span id="cb50-324"><a href="#cb50-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-325"><a href="#cb50-325" aria-hidden="true" tabindex="-1"></a>Recall that, since inverse-Wishart matrices involve sampling from a normal </span>
<span id="cb50-326"><a href="#cb50-326" aria-hidden="true" tabindex="-1"></a>distribution with mean $\mathbf{0}$, indeed $\mathbf{S}_0$ can be treated as a </span>
<span id="cb50-327"><a href="#cb50-327" aria-hidden="true" tabindex="-1"></a>*residual* covariance matrix, given that $\mathbb{E}(\boldsymbol{y}_i -</span>
<span id="cb50-328"><a href="#cb50-328" aria-hidden="true" tabindex="-1"></a>\boldsymbol{\theta}) = \mathbf{0}$.</span>
<span id="cb50-329"><a href="#cb50-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-330"><a href="#cb50-330" aria-hidden="true" tabindex="-1"></a>Finally, notice that the conditional expectation of the covariance matrix is is</span>
<span id="cb50-331"><a href="#cb50-331" aria-hidden="true" tabindex="-1"></a>a weighted average of the prior expectation $\frac{1}{\nu_0 - p -</span>
<span id="cb50-332"><a href="#cb50-332" aria-hidden="true" tabindex="-1"></a>1}\mathbf{S}_0$ and the unbiased estimator $\frac{1}{n} \mathbf{S}_{\theta}$:</span>
<span id="cb50-333"><a href="#cb50-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-334"><a href="#cb50-334" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-335"><a href="#cb50-335" aria-hidden="true" tabindex="-1"></a>\mathbb{E}(\Sigma \mid \boldsymbol{y}_1, \dots, \boldsymbol{y}_n, \boldsymbol{\theta}) &amp;= \frac{1}{\nu_0 + n - p - 1} (\mathbf{S}_0 + \mathbf{S}_{\theta}) <span class="sc">\\</span></span>
<span id="cb50-336"><a href="#cb50-336" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\nu_0 - p - 1}{\nu_0 + n - p - 1} \frac{1}{\nu_0 - p - 1} \mathbf{S}_0 + \frac{n}{\nu_0 + n - p - 1}\frac{1}{n} \mathbf{S}_{\theta}.</span>
<span id="cb50-337"><a href="#cb50-337" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-338"><a href="#cb50-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-339"><a href="#cb50-339" aria-hidden="true" tabindex="-1"></a>So the Bayesian estimator is a biased estimator, but is still demonstrably </span>
<span id="cb50-340"><a href="#cb50-340" aria-hidden="true" tabindex="-1"></a>consistent as $n \to \infty$. As mentioned in Chapter 5, we hope that the </span>
<span id="cb50-341"><a href="#cb50-341" aria-hidden="true" tabindex="-1"></a>estimator is biased more towards the true mean, as long as the prior is mildly </span>
<span id="cb50-342"><a href="#cb50-342" aria-hidden="true" tabindex="-1"></a>informative.</span>
<span id="cb50-343"><a href="#cb50-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-344"><a href="#cb50-344" aria-hidden="true" tabindex="-1"></a><span class="fu"># Summary of inference with the multivariate normal</span></span>
<span id="cb50-345"><a href="#cb50-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-346"><a href="#cb50-346" aria-hidden="true" tabindex="-1"></a>Like in Chapter 5, here we summarize the moving parts of inference with the </span>
<span id="cb50-347"><a href="#cb50-347" aria-hidden="true" tabindex="-1"></a>multivariate normal sampling model. There are four prior parameters (note some</span>
<span id="cb50-348"><a href="#cb50-348" aria-hidden="true" tabindex="-1"></a>are matrices):</span>
<span id="cb50-349"><a href="#cb50-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-350"><a href="#cb50-350" aria-hidden="true" tabindex="-1"></a><span class="fu">## (Semiconjugate) prior</span></span>
<span id="cb50-351"><a href="#cb50-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-352"><a href="#cb50-352" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{S}_0$ for the inverse-Wishart</span>
<span id="cb50-353"><a href="#cb50-353" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>*related to* the prior estimate of the covariance between the variables</span>
<span id="cb50-354"><a href="#cb50-354" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Only *related* to because as mentioned above, there are some guidelines for what to use for $\nu_0$ and $\mathbf{S}_0$ such that the prior distribution is centered around $\Sigma_0$, the *true* prior estimate of the covariance matrix you are looking for.</span>
<span id="cb50-355"><a href="#cb50-355" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\nu_0$ for the inverse-Wishart</span>
<span id="cb50-356"><a href="#cb50-356" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>a "prior sample size" from which the initial estimate of the *variance* is observed</span>
<span id="cb50-357"><a href="#cb50-357" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\boldsymbol{mu}_0$ for the multivariate normal</span>
<span id="cb50-358"><a href="#cb50-358" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>an initial estimate for the population mean</span>
<span id="cb50-359"><a href="#cb50-359" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\Lambda_0$ for the multivariate normal</span>
<span id="cb50-360"><a href="#cb50-360" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>the covariance (i.e. uncertainty) of the initial estimate for the population mean</span>
<span id="cb50-361"><a href="#cb50-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-362"><a href="#cb50-362" aria-hidden="true" tabindex="-1"></a>Somewhat similar to the univariate case, the estimate of the covariance matrix </span>
<span id="cb50-363"><a href="#cb50-363" aria-hidden="true" tabindex="-1"></a>for the inverse-Wishart prior is decoupled from the estimate of the covariance </span>
<span id="cb50-364"><a href="#cb50-364" aria-hidden="true" tabindex="-1"></a>of the mean vector in the multivariate normal prior, although it's common to set</span>
<span id="cb50-365"><a href="#cb50-365" aria-hidden="true" tabindex="-1"></a>these the same (but again, see rules for determining $\nu_0$).</span>
<span id="cb50-366"><a href="#cb50-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-367"><a href="#cb50-367" aria-hidden="true" tabindex="-1"></a>Note that this is somewhat *different* than the univariate case; since there</span>
<span id="cb50-368"><a href="#cb50-368" aria-hidden="true" tabindex="-1"></a>were no covariances to worry about, what was decoupled was "prior sample sizes"</span>
<span id="cb50-369"><a href="#cb50-369" aria-hidden="true" tabindex="-1"></a>from which the prior variance and prior mean are observed. Like here, it was</span>
<span id="cb50-370"><a href="#cb50-370" aria-hidden="true" tabindex="-1"></a>also common to set these the same.</span>
<span id="cb50-371"><a href="#cb50-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-372"><a href="#cb50-372" aria-hidden="true" tabindex="-1"></a><span class="fu">## Posterior</span></span>
<span id="cb50-373"><a href="#cb50-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-374"><a href="#cb50-374" aria-hidden="true" tabindex="-1"></a>The updated parameters are</span>
<span id="cb50-375"><a href="#cb50-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-376"><a href="#cb50-376" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{S}_n = \mathbf{S}_0 + \mathbf{S}_{\theta}$, where $\mathbf{S}_{\theta}$ is the residual sum of squares matrix</span>
<span id="cb50-377"><a href="#cb50-377" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\nu_n = \nu_0 + n$</span>
<span id="cb50-378"><a href="#cb50-378" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mu_n = (\Lambda_0^{-1} + n\Sigma^{-1})^{-1} (\Lambda_0 \boldsymbol{\mu}_0 + n\Sigma^{-1}\bar{\boldsymbol{y}}) = \Lambda_n (\Lambda_0^{-1}\boldsymbol{\mu}_0 + n\Sigma^{-1}\bar{\boldsymbol{y}})$</span>
<span id="cb50-379"><a href="#cb50-379" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\Lambda_n = (\Lambda_0^{-1} + n\Sigma^{-1})^{-1}$</span>
<span id="cb50-380"><a href="#cb50-380" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb50-381"><a href="#cb50-381" aria-hidden="true" tabindex="-1"></a><span class="fu"># Gibbs sampling of the mean and covariance</span></span>
<span id="cb50-382"><a href="#cb50-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-383"><a href="#cb50-383" aria-hidden="true" tabindex="-1"></a>Knowing these values, we can now perform Gibbs sampling to sample from</span>
<span id="cb50-384"><a href="#cb50-384" aria-hidden="true" tabindex="-1"></a>$p(\boldsymbol{\theta}, \Sigma \mid \boldsymbol{y}_1, \dots, \boldsymbol{y_n})$.</span>
<span id="cb50-385"><a href="#cb50-385" aria-hidden="true" tabindex="-1"></a>Specifically, we start with an estimate of one of the two values -</span>
<span id="cb50-386"><a href="#cb50-386" aria-hidden="true" tabindex="-1"></a>$\Sigma^{(0)}$ for simplicity - and use the following algorithm:</span>
<span id="cb50-387"><a href="#cb50-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-388"><a href="#cb50-388" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Sample $\boldsymbol{\theta}^{(s + 1)} \sim \mathcal{N}(\boldsymbol{\mu}_n,</span>
<span id="cb50-389"><a href="#cb50-389" aria-hidden="true" tabindex="-1"></a>\Lambda_n)$ where the parameters are calculated as above. This depends on the</span>
<span id="cb50-390"><a href="#cb50-390" aria-hidden="true" tabindex="-1"></a>inverse of the previous $\Sigma^{(s)}$.</span>
<span id="cb50-391"><a href="#cb50-391" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Sample $\Sigma^{(s + 1)} \sim \text{inverse-Wishart}(\nu_n, \mathbf{S}_n^{-1})$, where the parameters depend on $\boldsymbol{\theta}^{(s + 1)}$.</span>
<span id="cb50-392"><a href="#cb50-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-393"><a href="#cb50-393" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: reading comprehension</span></span>
<span id="cb50-394"><a href="#cb50-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-395"><a href="#cb50-395" aria-hidden="true" tabindex="-1"></a>We return to the example of two reading comprehension exams, given pre- and</span>
<span id="cb50-396"><a href="#cb50-396" aria-hidden="true" tabindex="-1"></a>post-training.</span>
<span id="cb50-397"><a href="#cb50-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-398"><a href="#cb50-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-399"><a href="#cb50-399" aria-hidden="true" tabindex="-1"></a><span class="fu">### Specifying prior</span></span>
<span id="cb50-400"><a href="#cb50-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-401"><a href="#cb50-401" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Mean</span></span>
<span id="cb50-402"><a href="#cb50-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-403"><a href="#cb50-403" aria-hidden="true" tabindex="-1"></a>Assume the tests are designed such that people generally score 50 out of 100.</span>
<span id="cb50-404"><a href="#cb50-404" aria-hidden="true" tabindex="-1"></a>Thus, our prior mean is $\boldsymbol{\mu}_0 = (50, 50)^T$. Note that this</span>
<span id="cb50-405"><a href="#cb50-405" aria-hidden="true" tabindex="-1"></a>implicitly assumes there is no effect of the training - the prior expectation of</span>
<span id="cb50-406"><a href="#cb50-406" aria-hidden="true" tabindex="-1"></a>the post-training score is the same as the pre-training score.</span>
<span id="cb50-407"><a href="#cb50-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-408"><a href="#cb50-408" aria-hidden="true" tabindex="-1"></a>Next, we need to specify the covariance of the prior expectation of the mean.</span>
<span id="cb50-409"><a href="#cb50-409" aria-hidden="true" tabindex="-1"></a>Specifically, since the scores are bounded between 0 and 100, we should put</span>
<span id="cb50-410"><a href="#cb50-410" aria-hidden="true" tabindex="-1"></a>little probability outside the $<span class="co">[</span><span class="ot">0, 100</span><span class="co">]</span>$ range (so a bell curve centered on 50</span>
<span id="cb50-411"><a href="#cb50-411" aria-hidden="true" tabindex="-1"></a>with 2 standard deviations $\in <span class="co">[</span><span class="ot">0, 100</span><span class="co">]</span>$). Then 1 standard deviation is 25 and</span>
<span id="cb50-412"><a href="#cb50-412" aria-hidden="true" tabindex="-1"></a>the variance is thus 625.</span>
<span id="cb50-413"><a href="#cb50-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-414"><a href="#cb50-414" aria-hidden="true" tabindex="-1"></a>Since the scores are both testing reading ability the scores are probably </span>
<span id="cb50-415"><a href="#cb50-415" aria-hidden="true" tabindex="-1"></a>correlated. So if we want a prior correlation between $\theta_1$ and $\theta_2$</span>
<span id="cb50-416"><a href="#cb50-416" aria-hidden="true" tabindex="-1"></a>of 0.5, we need to solve the correlation equation</span>
<span id="cb50-417"><a href="#cb50-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-418"><a href="#cb50-418" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-419"><a href="#cb50-419" aria-hidden="true" tabindex="-1"></a>&amp; 0.5 = \frac{\sigma_{1, 2}}{\sqrt{\sigma_1^2 \sigma_2^2}} <span class="sc">\\</span></span>
<span id="cb50-420"><a href="#cb50-420" aria-hidden="true" tabindex="-1"></a>\implies&amp; \sigma_{1, 2} = 0.5 \sqrt{625^2} = 312.5 <span class="sc">\\</span></span>
<span id="cb50-421"><a href="#cb50-421" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-422"><a href="#cb50-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-423"><a href="#cb50-423" aria-hidden="true" tabindex="-1"></a>Therefore,</span>
<span id="cb50-424"><a href="#cb50-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-425"><a href="#cb50-425" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb50-426"><a href="#cb50-426" aria-hidden="true" tabindex="-1"></a>\Lambda_0 = \begin{bmatrix} 625 &amp; 312.5 <span class="sc">\\</span> 312.5 &amp; 625 \end{bmatrix}</span>
<span id="cb50-427"><a href="#cb50-427" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb50-428"><a href="#cb50-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-429"><a href="#cb50-429" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Variance</span></span>
<span id="cb50-430"><a href="#cb50-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-431"><a href="#cb50-431" aria-hidden="true" tabindex="-1"></a>We will use the guideline mentioned earlier for loosely centering our covariance</span>
<span id="cb50-432"><a href="#cb50-432" aria-hidden="true" tabindex="-1"></a>matrix prior on $\Lambda_0$. We'll set $\mathbf{S}_0^{-1} = \Lambda_0$ and</span>
<span id="cb50-433"><a href="#cb50-433" aria-hidden="true" tabindex="-1"></a>$\nu_0 = p + 2 = 4$.</span>
<span id="cb50-434"><a href="#cb50-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-435"><a href="#cb50-435" aria-hidden="true" tabindex="-1"></a><span class="fu">### Code</span></span>
<span id="cb50-436"><a href="#cb50-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-439"><a href="#cb50-439" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb50-440"><a href="#cb50-440" aria-hidden="true" tabindex="-1"></a><span class="co"># Prior specification</span></span>
<span id="cb50-441"><a href="#cb50-441" aria-hidden="true" tabindex="-1"></a>Mu_0 <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">50</span>, <span class="dv">50</span>)  <span class="co"># If coerced, will be treated as column</span></span>
<span id="cb50-442"><a href="#cb50-442" aria-hidden="true" tabindex="-1"></a>Lambda_0 <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">625</span>, <span class="fl">312.5</span>), <span class="fu">c</span>(<span class="fl">312.5</span>, <span class="dv">625</span>))</span>
<span id="cb50-443"><a href="#cb50-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-444"><a href="#cb50-444" aria-hidden="true" tabindex="-1"></a>nu_0 <span class="ot">=</span> <span class="dv">4</span></span>
<span id="cb50-445"><a href="#cb50-445" aria-hidden="true" tabindex="-1"></a>S_0 <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rep</span>(Lambda_0), <span class="at">nrow =</span> <span class="dv">2</span>)</span>
<span id="cb50-446"><a href="#cb50-446" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-447"><a href="#cb50-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-450"><a href="#cb50-450" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb50-451"><a href="#cb50-451" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that this defines rmvnorm and rwish, but default</span></span>
<span id="cb50-452"><a href="#cb50-452" aria-hidden="true" tabindex="-1"></a><span class="co"># implementations or packages are used here for real-world applications.</span></span>
<span id="cb50-453"><a href="#cb50-453" aria-hidden="true" tabindex="-1"></a><span class="co"># Specifically MASS::mvrnorm (Modern Applied Statistics with S) is needed, and</span></span>
<span id="cb50-454"><a href="#cb50-454" aria-hidden="true" tabindex="-1"></a><span class="co"># wishart comes default in newer R distributions with rWishart</span></span>
<span id="cb50-455"><a href="#cb50-455" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"Inline/chapter7.R"</span>)</span>
<span id="cb50-456"><a href="#cb50-456" aria-hidden="true" tabindex="-1"></a><span class="co"># Try plotting</span></span>
<span id="cb50-457"><a href="#cb50-457" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> Y.reading</span>
<span id="cb50-458"><a href="#cb50-458" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(Y.reading)) <span class="sc">+</span></span>
<span id="cb50-459"><a href="#cb50-459" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> pretest, <span class="at">y =</span> posttest))</span>
<span id="cb50-460"><a href="#cb50-460" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-461"><a href="#cb50-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-464"><a href="#cb50-464" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb50-465"><a href="#cb50-465" aria-hidden="true" tabindex="-1"></a><span class="co"># Gibbs sampling</span></span>
<span id="cb50-466"><a href="#cb50-466" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb50-467"><a href="#cb50-467" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(Y) <span class="co"># Number of observations</span></span>
<span id="cb50-468"><a href="#cb50-468" aria-hidden="true" tabindex="-1"></a>Sigma_0 <span class="ot">=</span> <span class="fu">cov</span>(Y) <span class="co"># Calculate covariance matrix; initial Sigma sample</span></span>
<span id="cb50-469"><a href="#cb50-469" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: This initial Sigma sample doesn't end up in the gibbs sample - we throw</span></span>
<span id="cb50-470"><a href="#cb50-470" aria-hidden="true" tabindex="-1"></a><span class="co"># it away and start from scratch where sample 1 is the Theta based on Sigma_0, and</span></span>
<span id="cb50-471"><a href="#cb50-471" aria-hidden="true" tabindex="-1"></a><span class="co"># the Sigma based on that new Theta</span></span>
<span id="cb50-472"><a href="#cb50-472" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">=</span> Sigma_0</span>
<span id="cb50-473"><a href="#cb50-473" aria-hidden="true" tabindex="-1"></a>ybar <span class="ot">=</span> <span class="fu">colMeans</span>(Y) <span class="co"># Faster way of calculating column means</span></span>
<span id="cb50-474"><a href="#cb50-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-475"><a href="#cb50-475" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">TODO</span><span class="co">: Preallocate space for these instead of setting as NULL</span></span>
<span id="cb50-476"><a href="#cb50-476" aria-hidden="true" tabindex="-1"></a>THETA <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb50-477"><a href="#cb50-477" aria-hidden="true" tabindex="-1"></a>SIGMA <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb50-478"><a href="#cb50-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-479"><a href="#cb50-479" aria-hidden="true" tabindex="-1"></a><span class="co"># Also, inv = solve to make it more readable</span></span>
<span id="cb50-480"><a href="#cb50-480" aria-hidden="true" tabindex="-1"></a>inv <span class="ot">=</span> solve</span>
<span id="cb50-481"><a href="#cb50-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-482"><a href="#cb50-482" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb50-483"><a href="#cb50-483" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5000</span>) {</span>
<span id="cb50-484"><a href="#cb50-484" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update theta</span></span>
<span id="cb50-485"><a href="#cb50-485" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 1a. Compute params: Mu_n and Lambda_n from y_1, \dots, y_n and \Sigma^{(s)}</span></span>
<span id="cb50-486"><a href="#cb50-486" aria-hidden="true" tabindex="-1"></a>  <span class="co"># &gt; Compute Lambda_n according to equation 7.4</span></span>
<span id="cb50-487"><a href="#cb50-487" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Note: could cache inverse of priors</span></span>
<span id="cb50-488"><a href="#cb50-488" aria-hidden="true" tabindex="-1"></a>  Lambda_n <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">inv</span>(Lambda_0) <span class="sc">+</span> n <span class="sc">*</span> <span class="fu">inv</span>(Sigma))</span>
<span id="cb50-489"><a href="#cb50-489" aria-hidden="true" tabindex="-1"></a>  <span class="co"># &gt; Compute Mu_n according to 7.5. Use Matrix mult</span></span>
<span id="cb50-490"><a href="#cb50-490" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Note that he first term in 7.5 is Lambda_n</span></span>
<span id="cb50-491"><a href="#cb50-491" aria-hidden="true" tabindex="-1"></a>  Mu_n <span class="ot">=</span> Lambda_n <span class="sc">%*%</span> (<span class="fu">inv</span>(Lambda_0) <span class="sc">%*%</span> Mu_0 <span class="sc">+</span> n <span class="sc">*</span> <span class="fu">inv</span>(Sigma) <span class="sc">%*%</span> ybar)</span>
<span id="cb50-492"><a href="#cb50-492" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-493"><a href="#cb50-493" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 1b. Sample \theta^{(s + 1)} \sim multivariate normal mu_n, lambda_n</span></span>
<span id="cb50-494"><a href="#cb50-494" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Now we know Lambda_n, Mu_n as implied by the known Sigma and data (p. 108).</span></span>
<span id="cb50-495"><a href="#cb50-495" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Sample theta from multivariate normal (7.6, implied by 7.3)</span></span>
<span id="cb50-496"><a href="#cb50-496" aria-hidden="true" tabindex="-1"></a>  Theta <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">1</span>, Mu_n, Lambda_n)</span>
<span id="cb50-497"><a href="#cb50-497" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-498"><a href="#cb50-498" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Known Theta. Now sample a new Sigma. </span><span class="al">NOTE</span><span class="co">: Old sigma gets thrown away!</span></span>
<span id="cb50-499"><a href="#cb50-499" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 2a) Compute params for Sigma: Compute S_n from data and \theta^{(s + 1)}</span></span>
<span id="cb50-500"><a href="#cb50-500" aria-hidden="true" tabindex="-1"></a>  <span class="co"># i.e. Given the data and this Theta, we need to calculate the parameters that</span></span>
<span id="cb50-501"><a href="#cb50-501" aria-hidden="true" tabindex="-1"></a>  <span class="co"># define the full conditional distribution of \Sigma^{(s + 1)}</span></span>
<span id="cb50-502"><a href="#cb50-502" aria-hidden="true" tabindex="-1"></a>  <span class="co"># S_n according to p.112 first paragraph - not defined, but could be in 7.9</span></span>
<span id="cb50-503"><a href="#cb50-503" aria-hidden="true" tabindex="-1"></a>  <span class="co"># S_\theta is the residual sum of squares, defined in unlabeled equation after</span></span>
<span id="cb50-504"><a href="#cb50-504" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 7.8</span></span>
<span id="cb50-505"><a href="#cb50-505" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Use vectorized to avoid summation</span></span>
<span id="cb50-506"><a href="#cb50-506" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate residuals, then do sum of squares</span></span>
<span id="cb50-507"><a href="#cb50-507" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Q: why t(Y)? just how elementwise works I guess</span></span>
<span id="cb50-508"><a href="#cb50-508" aria-hidden="true" tabindex="-1"></a>  resid <span class="ot">=</span> <span class="fu">t</span>(Y) <span class="sc">-</span> <span class="fu">c</span>(Theta)</span>
<span id="cb50-509"><a href="#cb50-509" aria-hidden="true" tabindex="-1"></a>  S_theta <span class="ot">=</span> resid <span class="sc">%*%</span> <span class="fu">t</span>(resid)</span>
<span id="cb50-510"><a href="#cb50-510" aria-hidden="true" tabindex="-1"></a>  S_n <span class="ot">=</span> S_0 <span class="sc">+</span> S_theta</span>
<span id="cb50-511"><a href="#cb50-511" aria-hidden="true" tabindex="-1"></a>  <span class="co"># 2b) Knowing the parameters for the full conditional distribution on Sigma</span></span>
<span id="cb50-512"><a href="#cb50-512" aria-hidden="true" tabindex="-1"></a>  <span class="co"># (inverse wishart with nu_0 and S_n known), sample</span></span>
<span id="cb50-513"><a href="#cb50-513" aria-hidden="true" tabindex="-1"></a>  <span class="co"># df = number of samples (degrees of freedom)</span></span>
<span id="cb50-514"><a href="#cb50-514" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Don't forget to invert it afterwards (inverse wishart)</span></span>
<span id="cb50-515"><a href="#cb50-515" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Weird thing is rWishart returns a weird list of arrays</span></span>
<span id="cb50-516"><a href="#cb50-516" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">rWishart</span>(<span class="dv">1</span>, nu_0 <span class="sc">+</span> n, <span class="fu">inv</span>(S_n))[, , <span class="dv">1</span>])</span>
<span id="cb50-517"><a href="#cb50-517" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-518"><a href="#cb50-518" aria-hidden="true" tabindex="-1"></a>  THETA <span class="ot">=</span> <span class="fu">rbind</span>(THETA, Theta)</span>
<span id="cb50-519"><a href="#cb50-519" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Flatten sigma??</span></span>
<span id="cb50-520"><a href="#cb50-520" aria-hidden="true" tabindex="-1"></a>  SIGMA <span class="ot">=</span> <span class="fu">rbind</span>(SIGMA, <span class="fu">c</span>(Sigma))</span>
<span id="cb50-521"><a href="#cb50-521" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb50-522"><a href="#cb50-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-523"><a href="#cb50-523" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-524"><a href="#cb50-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-525"><a href="#cb50-525" aria-hidden="true" tabindex="-1"></a>Here are some associated calculations with this sample</span>
<span id="cb50-526"><a href="#cb50-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-529"><a href="#cb50-529" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb50-530"><a href="#cb50-530" aria-hidden="true" tabindex="-1"></a><span class="co"># Confidence interval for difference between post and pre test</span></span>
<span id="cb50-531"><a href="#cb50-531" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(THETA[, <span class="dv">2</span>] <span class="sc">-</span> THETA[, <span class="dv">1</span>], <span class="at">prob =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))</span>
<span id="cb50-532"><a href="#cb50-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-533"><a href="#cb50-533" aria-hidden="true" tabindex="-1"></a><span class="co"># Likelihood that the mean for the second test is greater than the mean for the</span></span>
<span id="cb50-534"><a href="#cb50-534" aria-hidden="true" tabindex="-1"></a><span class="co"># 1st test</span></span>
<span id="cb50-535"><a href="#cb50-535" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(THETA[, <span class="dv">2</span>] <span class="sc">&gt;</span> THETA[, <span class="dv">1</span>])</span>
<span id="cb50-536"><a href="#cb50-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-537"><a href="#cb50-537" aria-hidden="true" tabindex="-1"></a><span class="co"># Confidence interval for the correlation</span></span>
<span id="cb50-538"><a href="#cb50-538" aria-hidden="true" tabindex="-1"></a>CORR <span class="ot">=</span> <span class="fu">apply</span>(SIGMA, <span class="at">MARGIN =</span> <span class="dv">1</span>, <span class="at">FUN =</span> <span class="cf">function</span>(row) {</span>
<span id="cb50-539"><a href="#cb50-539" aria-hidden="true" tabindex="-1"></a>  <span class="co"># indices 1 and 4 are correlation of dims 1 and 2</span></span>
<span id="cb50-540"><a href="#cb50-540" aria-hidden="true" tabindex="-1"></a>  <span class="co"># indices 2 is equal to index 3 is equal to covariance</span></span>
<span id="cb50-541"><a href="#cb50-541" aria-hidden="true" tabindex="-1"></a>  row[<span class="dv">2</span>] <span class="sc">/</span> <span class="fu">sqrt</span>(row[<span class="dv">1</span>] <span class="sc">*</span> row[<span class="dv">4</span>])</span>
<span id="cb50-542"><a href="#cb50-542" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb50-543"><a href="#cb50-543" aria-hidden="true" tabindex="-1"></a><span class="co"># Obviously there is a correlation</span></span>
<span id="cb50-544"><a href="#cb50-544" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(CORR, <span class="at">prob =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))</span>
<span id="cb50-545"><a href="#cb50-545" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-546"><a href="#cb50-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-547"><a href="#cb50-547" aria-hidden="true" tabindex="-1"></a>We can also work with the posterior predictive distribution by sampling new</span>
<span id="cb50-548"><a href="#cb50-548" aria-hidden="true" tabindex="-1"></a>pairs $(y_1, y_2)^{T(s)}$ from our samples of $\boldsymbol{\theta}^{(s)}$ and</span>
<span id="cb50-549"><a href="#cb50-549" aria-hidden="true" tabindex="-1"></a>$\Sigma^{(s)}$:</span>
<span id="cb50-550"><a href="#cb50-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-553"><a href="#cb50-553" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb50-554"><a href="#cb50-554" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="dv">5000</span>, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb50-555"><a href="#cb50-555" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5000</span>) {</span>
<span id="cb50-556"><a href="#cb50-556" aria-hidden="true" tabindex="-1"></a>  Y[s, ] <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="dv">1</span>, THETA[s, ], <span class="fu">matrix</span>(SIGMA[s, ], <span class="at">nrow =</span> <span class="dv">2</span>))</span>
<span id="cb50-557"><a href="#cb50-557" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb50-558"><a href="#cb50-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-559"><a href="#cb50-559" aria-hidden="true" tabindex="-1"></a><span class="co"># Probability that the post-test score of a randomly selected person is greater</span></span>
<span id="cb50-560"><a href="#cb50-560" aria-hidden="true" tabindex="-1"></a><span class="co"># than the pre-test score</span></span>
<span id="cb50-561"><a href="#cb50-561" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(Y[, <span class="dv">2</span>] <span class="sc">&gt;</span> Y[, <span class="dv">1</span>])</span>
<span id="cb50-562"><a href="#cb50-562" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-563"><a href="#cb50-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-564"><a href="#cb50-564" aria-hidden="true" tabindex="-1"></a>Importantly, the probability of an individual having a higher post-test score</span>
<span id="cb50-565"><a href="#cb50-565" aria-hidden="true" tabindex="-1"></a>than pre-test is much lower than the near-1 probability of the difference in</span>
<span id="cb50-566"><a href="#cb50-566" aria-hidden="true" tabindex="-1"></a>mean test scores. It's important to have clarified the difference between</span>
<span id="cb50-567"><a href="#cb50-567" aria-hidden="true" tabindex="-1"></a>population means and individuals when drawing conclusions about your data.</span>
<span id="cb50-568"><a href="#cb50-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-569"><a href="#cb50-569" aria-hidden="true" tabindex="-1"></a><span class="fu"># Missing data and imputation</span></span>
<span id="cb50-570"><a href="#cb50-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-571"><a href="#cb50-571" aria-hidden="true" tabindex="-1"></a>Another useful application of Bayesian analysis and the multivariate normal</span>
<span id="cb50-572"><a href="#cb50-572" aria-hidden="true" tabindex="-1"></a>model is the imputation of missing data. There are various naive ways to deal with missing data in datasets:</span>
<span id="cb50-573"><a href="#cb50-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-574"><a href="#cb50-574" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>listwise deletion: just discard points with missing data. But this wastes valuable data!</span>
<span id="cb50-575"><a href="#cb50-575" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>replace missing variables for points with the mean of that variable for the </span>
<span id="cb50-576"><a href="#cb50-576" aria-hidden="true" tabindex="-1"></a>entire dataset. But this results in inaccurate estimates, since a data point's </span>
<span id="cb50-577"><a href="#cb50-577" aria-hidden="true" tabindex="-1"></a>other variables may contain information about the data point's </span>
<span id="cb50-578"><a href="#cb50-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-579"><a href="#cb50-579" aria-hidden="true" tabindex="-1"></a>The Bayesian approach offers a neat solution around this. The idea is that the</span>
<span id="cb50-580"><a href="#cb50-580" aria-hidden="true" tabindex="-1"></a>likelihood of a datapoint with missing values is the likelihood of the observed</span>
<span id="cb50-581"><a href="#cb50-581" aria-hidden="true" tabindex="-1"></a>values while integrating over the missing values.</span>
<span id="cb50-582"><a href="#cb50-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-583"><a href="#cb50-583" aria-hidden="true" tabindex="-1"></a>Specifically, let's assume in a dataset $\boldsymbol{Y}$ with missing values, we</span>
<span id="cb50-584"><a href="#cb50-584" aria-hidden="true" tabindex="-1"></a>have a corresponding matrix $\boldsymbol{O}$ which contains a 1 if the</span>
<span id="cb50-585"><a href="#cb50-585" aria-hidden="true" tabindex="-1"></a>corresponding element in $\boldsymbol{Y}$ exists, else 0. (This is just to help</span>
<span id="cb50-586"><a href="#cb50-586" aria-hidden="true" tabindex="-1"></a>us mathematically indicate which variables are missing)</span>
<span id="cb50-587"><a href="#cb50-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-588"><a href="#cb50-588" aria-hidden="true" tabindex="-1"></a>Then consider a single observation $\boldsymbol{y}_i$. We have</span>
<span id="cb50-589"><a href="#cb50-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-590"><a href="#cb50-590" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-591"><a href="#cb50-591" aria-hidden="true" tabindex="-1"></a>p(\boldsymbol{o}_i, \{ y_{i, j} \; : \; o_{i, j} = 1 <span class="sc">\}</span> \mid \boldsymbol{\theta}, \Sigma) &amp;=</span>
<span id="cb50-592"><a href="#cb50-592" aria-hidden="true" tabindex="-1"></a>p(\boldsymbol{o}_i) \times \int \left[ p(\boldsymbol{y}_i \mid \boldsymbol{\theta}, \Sigma) \prod_{y_{i, j} \; : \; o_{i, j} = 0} dy_{i, j} \right]</span>
<span id="cb50-593"><a href="#cb50-593" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-594"><a href="#cb50-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-595"><a href="#cb50-595" aria-hidden="true" tabindex="-1"></a>i.e. we are integrating over all possible "full" observations of datapoints </span>
<span id="cb50-596"><a href="#cb50-596" aria-hidden="true" tabindex="-1"></a>$\boldsymbol{y}$ with respect to the variables that we don't have. The variables</span>
<span id="cb50-597"><a href="#cb50-597" aria-hidden="true" tabindex="-1"></a>we have observed stay constant.</span>
<span id="cb50-598"><a href="#cb50-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-599"><a href="#cb50-599" aria-hidden="true" tabindex="-1"></a><span class="fu">## Gibbs sampling with missing data</span></span>
<span id="cb50-600"><a href="#cb50-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-601"><a href="#cb50-601" aria-hidden="true" tabindex="-1"></a>Normally we use Gibbs sampling to estimate the posterior $p(\boldsymbol{\theta},</span>
<span id="cb50-602"><a href="#cb50-602" aria-hidden="true" tabindex="-1"></a>\Sigma \mid \mathbf{Y})$ Here, however, we don't have a full dataset</span>
<span id="cb50-603"><a href="#cb50-603" aria-hidden="true" tabindex="-1"></a>$\mathbf{Y}$; rather, we have an observed dataset $\mathbf{Y}_{\text{obs}}$ and</span>
<span id="cb50-604"><a href="#cb50-604" aria-hidden="true" tabindex="-1"></a>missing values $\mathbf{Y}_{\text{miss}}$. The key idea is to *also* estimate</span>
<span id="cb50-605"><a href="#cb50-605" aria-hidden="true" tabindex="-1"></a>the posterior distribution on $\mathbf{Y}_{\text{miss}}$, which will also help</span>
<span id="cb50-606"><a href="#cb50-606" aria-hidden="true" tabindex="-1"></a>us make more accurate estimates on $\boldsymbol{\theta}$ and $\Sigma$. Using</span>
<span id="cb50-607"><a href="#cb50-607" aria-hidden="true" tabindex="-1"></a>Gibbs sampling, when we have sample values $\boldsymbol{\theta}^{(s)}$ and</span>
<span id="cb50-608"><a href="#cb50-608" aria-hidden="true" tabindex="-1"></a>$\Sigma^{(s)}$, we can sample from</span>
<span id="cb50-609"><a href="#cb50-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-610"><a href="#cb50-610" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb50-611"><a href="#cb50-611" aria-hidden="true" tabindex="-1"></a>\mathbf{Y}_{\text{miss}}^{(s)} \sim p(\mathbf{Y}_{\text{miss}} \mid \mathbf{Y}_{\text{obs}}, \boldsymbol{\theta}^{(s)}, \Sigma^{(s)})</span>
<span id="cb50-612"><a href="#cb50-612" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb50-613"><a href="#cb50-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-614"><a href="#cb50-614" aria-hidden="true" tabindex="-1"></a>Specifically, to sample from the above, we simply sample the missing values of</span>
<span id="cb50-615"><a href="#cb50-615" aria-hidden="true" tabindex="-1"></a>each data point independently. For a data point $\boldsymbol{y}$ with missing</span>
<span id="cb50-616"><a href="#cb50-616" aria-hidden="true" tabindex="-1"></a>values, let $a$ be the indices of the observed values and $b$ be the indices of</span>
<span id="cb50-617"><a href="#cb50-617" aria-hidden="true" tabindex="-1"></a>the missing values. Then it is shown that sampling $\boldsymbol{y}_{<span class="co">[</span><span class="ot">b</span><span class="co">]</span>}$ given</span>
<span id="cb50-618"><a href="#cb50-618" aria-hidden="true" tabindex="-1"></a>known observed variables and the parameters $\boldsymbol{\theta}$ and $\Sigma$</span>
<span id="cb50-619"><a href="#cb50-619" aria-hidden="true" tabindex="-1"></a>also follows a multivariate normal distribution, but with mean and covariance</span>
<span id="cb50-620"><a href="#cb50-620" aria-hidden="true" tabindex="-1"></a>matrices with dimension $| b |$ that take into account the existing variables:</span>
<span id="cb50-621"><a href="#cb50-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-622"><a href="#cb50-622" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-623"><a href="#cb50-623" aria-hidden="true" tabindex="-1"></a>\boldsymbol{y}_{[b]} \mid \boldsymbol{y}_{[a]}, \boldsymbol{\theta}, \Sigma \sim \mathcal{N}(\boldsymbol{\theta}_{b \mid a}, \Sigma_{b \mid a})</span>
<span id="cb50-624"><a href="#cb50-624" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-625"><a href="#cb50-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-626"><a href="#cb50-626" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb50-627"><a href="#cb50-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-628"><a href="#cb50-628" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\boldsymbol{\theta}_{b \mid a} = \boldsymbol{\theta}_b + \Sigma_{[b, a]}(\Sigma_{[a, a]})^{-1} (\boldsymbol{y}_{[a]} - \boldsymbol{\theta}_{<span class="co">[</span><span class="ot">a</span><span class="co">]</span>})$;</span>
<span id="cb50-629"><a href="#cb50-629" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Intuitively, the mean of the multivariate normal distribution on the missing</span>
<span id="cb50-630"><a href="#cb50-630" aria-hidden="true" tabindex="-1"></a>  values *given* some observed values starts with the unconditional mean of the </span>
<span id="cb50-631"><a href="#cb50-631" aria-hidden="true" tabindex="-1"></a>  observed values, plus or minus some offset that depends on the observed values</span>
<span id="cb50-632"><a href="#cb50-632" aria-hidden="true" tabindex="-1"></a>  and the correlations between the observed and missing values. For example, if</span>
<span id="cb50-633"><a href="#cb50-633" aria-hidden="true" tabindex="-1"></a>  it is known that a datapoint's observed values are quite high relative to the</span>
<span id="cb50-634"><a href="#cb50-634" aria-hidden="true" tabindex="-1"></a>  mean $(\boldsymbol{y}_a - \boldsymbol{\theta}_a)$, and that there is a </span>
<span id="cb50-635"><a href="#cb50-635" aria-hidden="true" tabindex="-1"></a>  positive correlation between observed values and missing values, we would </span>
<span id="cb50-636"><a href="#cb50-636" aria-hidden="true" tabindex="-1"></a>  expect the missing values to generally be higher as well.</span>
<span id="cb50-637"><a href="#cb50-637" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\Sigma_{b \mid a} = \Sigma_{[b, b]} - \Sigma_{[b, a]} (\Sigma_{[a, a]})^{-1} \Sigma_{<span class="co">[</span><span class="ot">a, b</span><span class="co">]</span>}$</span>
<span id="cb50-638"><a href="#cb50-638" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Intuitively, the covariance matrix of the conditional distribution on the</span>
<span id="cb50-639"><a href="#cb50-639" aria-hidden="true" tabindex="-1"></a>  missing values starts with the unconditional covariance, but notice the minus</span>
<span id="cb50-640"><a href="#cb50-640" aria-hidden="true" tabindex="-1"></a>  sign; since the covariance matrix is positive definite, knowing about some</span>
<span id="cb50-641"><a href="#cb50-641" aria-hidden="true" tabindex="-1"></a>  observed variables will decrease our uncertainty about the missing values.</span>
<span id="cb50-642"><a href="#cb50-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-643"><a href="#cb50-643" aria-hidden="true" tabindex="-1"></a>Once we've sampled a set of missing values, notice that we now have a full </span>
<span id="cb50-644"><a href="#cb50-644" aria-hidden="true" tabindex="-1"></a>"dataset" if we combine our observed values with the newly sampled missing </span>
<span id="cb50-645"><a href="#cb50-645" aria-hidden="true" tabindex="-1"></a>values. This means that we can sample from the full conditional distributions of</span>
<span id="cb50-646"><a href="#cb50-646" aria-hidden="true" tabindex="-1"></a>$\boldsymbol{\theta}$ and $\Sigma$ normally, and from there, once again sample a</span>
<span id="cb50-647"><a href="#cb50-647" aria-hidden="true" tabindex="-1"></a>new set of $\mathbf{Y}_{\text{miss}}$.</span>
<span id="cb50-648"><a href="#cb50-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-649"><a href="#cb50-649" aria-hidden="true" tabindex="-1"></a>To summarize Gibbs sampling with missing data: assume starting values $\Sigma^{(0)}$ and $\mathbf{Y}_{\text{miss}}^{(0)}$ - perhaps the empirical covariance matrix and the unconditional means of the observed sample. Then the algorithm has just one more step:</span>
<span id="cb50-650"><a href="#cb50-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-651"><a href="#cb50-651" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Sample $\boldsymbol{\theta}^{(s + 1)}$ from $p(\boldsymbol{\theta} \mid \mathbf{Y}_{\text{obs}}, \mathbf{Y}_{\text{miss}}^{(s)}, \Sigma^{(s)})$</span>
<span id="cb50-652"><a href="#cb50-652" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Sample $\Sigma^{(s + 1)}$ from $p(\Sigma \mid \mathbf{Y}_{\text{obs}}, \mathbf{Y}_{\text{miss}}^{(s)}, \boldsymbol{\theta}^{(s)})$</span>
<span id="cb50-653"><a href="#cb50-653" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Sample $\mathbf{Y}_{\text{miss}}^{(s + 1)}$ from $p(\mathbf{Y}_{\text{miss}} \mid \mathbf{Y}_{\text{obs}}, \boldsymbol{\theta}^{(s)}, \Sigma^{(s)})$</span>
<span id="cb50-654"><a href="#cb50-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-655"><a href="#cb50-655" aria-hidden="true" tabindex="-1"></a>For steps 1 and 2, you simply combine the sampled missing data and the</span>
<span id="cb50-656"><a href="#cb50-656" aria-hidden="true" tabindex="-1"></a>observed data for a full dataset $\mathbf{Y}$ and sample from the full</span>
<span id="cb50-657"><a href="#cb50-657" aria-hidden="true" tabindex="-1"></a>conditional distributions like normal.</span>
<span id="cb50-658"><a href="#cb50-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-659"><a href="#cb50-659" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example</span></span>
<span id="cb50-660"><a href="#cb50-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-661"><a href="#cb50-661" aria-hidden="true" tabindex="-1"></a>Let's take a look at an example using a dataset with four health-related</span>
<span id="cb50-662"><a href="#cb50-662" aria-hidden="true" tabindex="-1"></a>measurements on 200 women near Phoenix, Arizona. Notice that this dataset has</span>
<span id="cb50-663"><a href="#cb50-663" aria-hidden="true" tabindex="-1"></a>missing values. We'll assume that the data is *missing at random*, which is</span>
<span id="cb50-664"><a href="#cb50-664" aria-hidden="true" tabindex="-1"></a>necessary for this analysis.</span>
<span id="cb50-665"><a href="#cb50-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-666"><a href="#cb50-666" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, warning = FALSE}</span></span>
<span id="cb50-667"><a href="#cb50-667" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> Y.pima.miss</span>
<span id="cb50-668"><a href="#cb50-668" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Y) <span class="co"># Notice missing data</span></span>
<span id="cb50-669"><a href="#cb50-669" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(GGally)</span>
<span id="cb50-670"><a href="#cb50-670" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressWarnings</span>(<span class="fu">ggpairs</span>(Y))</span>
<span id="cb50-671"><a href="#cb50-671" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-672"><a href="#cb50-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-673"><a href="#cb50-673" aria-hidden="true" tabindex="-1"></a>Gibbs sampling according to the 3-step scheme described above is implemented</span>
<span id="cb50-674"><a href="#cb50-674" aria-hidden="true" tabindex="-1"></a>below. For priors, we set $\boldsymbol{\mu}_0 = (120, 64, 26, 26)$ assuming we</span>
<span id="cb50-675"><a href="#cb50-675" aria-hidden="true" tabindex="-1"></a>know these are the national averages of the health measurements. We then (waving</span>
<span id="cb50-676"><a href="#cb50-676" aria-hidden="true" tabindex="-1"></a>our hands a little) select prior variances that keep these measurements mostly </span>
<span id="cb50-677"><a href="#cb50-677" aria-hidden="true" tabindex="-1"></a>around zero and only lightly centered around our estimates.</span>
<span id="cb50-678"><a href="#cb50-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-679"><a href="#cb50-679" aria-hidden="true" tabindex="-1"></a><span class="in">```{r cache=TRUE}</span></span>
<span id="cb50-680"><a href="#cb50-680" aria-hidden="true" tabindex="-1"></a><span class="co"># Gibbs sampling</span></span>
<span id="cb50-681"><a href="#cb50-681" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(Y)</span>
<span id="cb50-682"><a href="#cb50-682" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">ncol</span>(Y)</span>
<span id="cb50-683"><a href="#cb50-683" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">120</span>, <span class="dv">64</span>, <span class="dv">26</span>, <span class="dv">26</span>)</span>
<span id="cb50-684"><a href="#cb50-684" aria-hidden="true" tabindex="-1"></a>sd0 <span class="ot">=</span> mu0 <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb50-685"><a href="#cb50-685" aria-hidden="true" tabindex="-1"></a><span class="co"># Setting prior on covariance</span></span>
<span id="cb50-686"><a href="#cb50-686" aria-hidden="true" tabindex="-1"></a>L0 <span class="ot">=</span> <span class="fu">matrix</span>(.<span class="dv">1</span>, p, p)</span>
<span id="cb50-687"><a href="#cb50-687" aria-hidden="true" tabindex="-1"></a><span class="fu">diag</span>(L0) <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb50-688"><a href="#cb50-688" aria-hidden="true" tabindex="-1"></a>L0 <span class="ot">=</span> L0 <span class="sc">*</span> <span class="fu">outer</span>(sd0, sd0)</span>
<span id="cb50-689"><a href="#cb50-689" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(L0)</span>
<span id="cb50-690"><a href="#cb50-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-691"><a href="#cb50-691" aria-hidden="true" tabindex="-1"></a><span class="co"># Variance prior lightly concentrated, where nu0 &gt; p is nu0 = p + 2</span></span>
<span id="cb50-692"><a href="#cb50-692" aria-hidden="true" tabindex="-1"></a>nu0 <span class="ot">=</span> p <span class="sc">+</span> <span class="dv">2</span></span>
<span id="cb50-693"><a href="#cb50-693" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale matrix - set to be the same as L0</span></span>
<span id="cb50-694"><a href="#cb50-694" aria-hidden="true" tabindex="-1"></a>S0 <span class="ot">=</span> L0</span>
<span id="cb50-695"><a href="#cb50-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-696"><a href="#cb50-696" aria-hidden="true" tabindex="-1"></a><span class="co"># Set starting values</span></span>
<span id="cb50-697"><a href="#cb50-697" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">=</span> S0</span>
<span id="cb50-698"><a href="#cb50-698" aria-hidden="true" tabindex="-1"></a>Y.full <span class="ot">=</span> Y</span>
<span id="cb50-699"><a href="#cb50-699" aria-hidden="true" tabindex="-1"></a>O <span class="ot">=</span> <span class="dv">1</span> <span class="sc">*</span> (<span class="sc">!</span><span class="fu">is.na</span>(Y))  <span class="co"># Get NOT NAs, coerce to int via * 1</span></span>
<span id="cb50-700"><a href="#cb50-700" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>p) { <span class="co"># Looping through columns</span></span>
<span id="cb50-701"><a href="#cb50-701" aria-hidden="true" tabindex="-1"></a>  <span class="co"># For missing values in column, set to mean of the observed values</span></span>
<span id="cb50-702"><a href="#cb50-702" aria-hidden="true" tabindex="-1"></a>  mean.wo.na <span class="ot">=</span> <span class="fu">mean</span>(Y.full[, j], <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb50-703"><a href="#cb50-703" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Rows: all of those that are NA, and the jth column, set it to this mean</span></span>
<span id="cb50-704"><a href="#cb50-704" aria-hidden="true" tabindex="-1"></a>  Y.full[<span class="fu">is.na</span>(Y.full[, j]), j] <span class="ot">=</span> mean.wo.na</span>
<span id="cb50-705"><a href="#cb50-705" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb50-706"><a href="#cb50-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-707"><a href="#cb50-707" aria-hidden="true" tabindex="-1"></a><span class="co"># Gibbs</span></span>
<span id="cb50-708"><a href="#cb50-708" aria-hidden="true" tabindex="-1"></a>THETA <span class="ot">=</span> SIGMA <span class="ot">=</span> Y.MISS <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb50-709"><a href="#cb50-709" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb50-710"><a href="#cb50-710" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>) {</span>
<span id="cb50-711"><a href="#cb50-711" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update theta: step 1 of p.117 which is the same as previous</span></span>
<span id="cb50-712"><a href="#cb50-712" aria-hidden="true" tabindex="-1"></a>  ybar <span class="ot">=</span> <span class="fu">colMeans</span>(Y.full)</span>
<span id="cb50-713"><a href="#cb50-713" aria-hidden="true" tabindex="-1"></a>  Ln <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">inv</span>(L0) <span class="sc">+</span> n <span class="sc">*</span> <span class="fu">inv</span>(Sigma))</span>
<span id="cb50-714"><a href="#cb50-714" aria-hidden="true" tabindex="-1"></a>  mun <span class="ot">=</span> Ln <span class="sc">%*%</span> (<span class="fu">inv</span>(L0) <span class="sc">%*%</span> mu0 <span class="sc">+</span> n <span class="sc">*</span> <span class="fu">inv</span>(Sigma) <span class="sc">%*%</span> ybar)</span>
<span id="cb50-715"><a href="#cb50-715" aria-hidden="true" tabindex="-1"></a>  theta <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="dv">1</span>, mun, Ln)</span>
<span id="cb50-716"><a href="#cb50-716" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-717"><a href="#cb50-717" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update sigma: step 2 of p.117, same as previous</span></span>
<span id="cb50-718"><a href="#cb50-718" aria-hidden="true" tabindex="-1"></a>  resid <span class="ot">=</span> <span class="fu">t</span>(Y.full) <span class="sc">-</span> <span class="fu">c</span>(theta)</span>
<span id="cb50-719"><a href="#cb50-719" aria-hidden="true" tabindex="-1"></a>  Stheta <span class="ot">=</span> resid <span class="sc">%*%</span> <span class="fu">t</span>(resid)</span>
<span id="cb50-720"><a href="#cb50-720" aria-hidden="true" tabindex="-1"></a>  Sn <span class="ot">=</span> S0 <span class="sc">+</span> Stheta</span>
<span id="cb50-721"><a href="#cb50-721" aria-hidden="true" tabindex="-1"></a>  Sigma <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">rWishart</span>(<span class="dv">1</span>, nu0 <span class="sc">+</span> n, <span class="fu">inv</span>(Sn))[, , <span class="dv">1</span>])</span>
<span id="cb50-722"><a href="#cb50-722" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-723"><a href="#cb50-723" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update ymiss: step 3 of p.117, requires eqs 7.10, 7.11</span></span>
<span id="cb50-724"><a href="#cb50-724" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Loop through rows of data (takes longer!!), need to sample rows individually</span></span>
<span id="cb50-725"><a href="#cb50-725" aria-hidden="true" tabindex="-1"></a>  <span class="co"># (independent) top of p.118</span></span>
<span id="cb50-726"><a href="#cb50-726" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb50-727"><a href="#cb50-727" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Skip if we already have it</span></span>
<span id="cb50-728"><a href="#cb50-728" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">all</span>(O[i, ] <span class="sc">==</span> <span class="dv">1</span>)) {</span>
<span id="cb50-729"><a href="#cb50-729" aria-hidden="true" tabindex="-1"></a>      <span class="cf">next</span></span>
<span id="cb50-730"><a href="#cb50-730" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb50-731"><a href="#cb50-731" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Partition b = NA rows, a = present rows</span></span>
<span id="cb50-732"><a href="#cb50-732" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Still works of a, b are empty, I presume</span></span>
<span id="cb50-733"><a href="#cb50-733" aria-hidden="true" tabindex="-1"></a>    oi <span class="ot">=</span> O[i, ]</span>
<span id="cb50-734"><a href="#cb50-734" aria-hidden="true" tabindex="-1"></a>    a <span class="ot">=</span> oi <span class="sc">==</span> <span class="dv">1</span></span>
<span id="cb50-735"><a href="#cb50-735" aria-hidden="true" tabindex="-1"></a>    b <span class="ot">=</span> oi <span class="sc">==</span> <span class="dv">0</span></span>
<span id="cb50-736"><a href="#cb50-736" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-737"><a href="#cb50-737" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Now we want to sample yb | ya, Sigma, Theta</span></span>
<span id="cb50-738"><a href="#cb50-738" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-739"><a href="#cb50-739" aria-hidden="true" tabindex="-1"></a>    <span class="co"># \Sigma_{[a, a]}^{-1}, used in eqs 7.10 AND 7.11 (so calc once)</span></span>
<span id="cb50-740"><a href="#cb50-740" aria-hidden="true" tabindex="-1"></a>    iSa <span class="ot">=</span> <span class="fu">inv</span>(Sigma[a, a])</span>
<span id="cb50-741"><a href="#cb50-741" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calcualte \Sigma_{[b, a]}(\Sigma_{[a, a]})^{-1} used in 7.10 AND 7.11</span></span>
<span id="cb50-742"><a href="#cb50-742" aria-hidden="true" tabindex="-1"></a>    beta.j <span class="ot">=</span> Sigma[b, a] <span class="sc">%*%</span> iSa</span>
<span id="cb50-743"><a href="#cb50-743" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate Sigma.j (7.11). Start with covariacne matrix for the missing</span></span>
<span id="cb50-744"><a href="#cb50-744" aria-hidden="true" tabindex="-1"></a>    <span class="co"># vars, then influence by beta (decrease variance)</span></span>
<span id="cb50-745"><a href="#cb50-745" aria-hidden="true" tabindex="-1"></a>    Sigma.j <span class="ot">=</span> Sigma[b, b] <span class="sc">-</span> beta.j <span class="sc">%*%</span> Sigma[a, b]</span>
<span id="cb50-746"><a href="#cb50-746" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate theta.j (7.10). Start with standard theta, then change based on</span></span>
<span id="cb50-747"><a href="#cb50-747" aria-hidden="true" tabindex="-1"></a>    <span class="co"># residuals of other vals and what we wknow about covariances</span></span>
<span id="cb50-748"><a href="#cb50-748" aria-hidden="true" tabindex="-1"></a>    yi <span class="ot">=</span> Y.full[i, ]</span>
<span id="cb50-749"><a href="#cb50-749" aria-hidden="true" tabindex="-1"></a>    theta.j <span class="ot">=</span> theta[b] <span class="sc">+</span> beta.j <span class="sc">%*%</span> <span class="fu">t</span>(yi[a] <span class="sc">-</span> theta[a])</span>
<span id="cb50-750"><a href="#cb50-750" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-751"><a href="#cb50-751" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Now we have samples for subset of b. Preserver order, now sample</span></span>
<span id="cb50-752"><a href="#cb50-752" aria-hidden="true" tabindex="-1"></a>    Y.full[i, b] <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="dv">1</span>, theta.j, Sigma.j)</span>
<span id="cb50-753"><a href="#cb50-753" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb50-754"><a href="#cb50-754" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-755"><a href="#cb50-755" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Concat</span></span>
<span id="cb50-756"><a href="#cb50-756" aria-hidden="true" tabindex="-1"></a>  THETA <span class="ot">=</span> <span class="fu">rbind</span>(THETA, theta)</span>
<span id="cb50-757"><a href="#cb50-757" aria-hidden="true" tabindex="-1"></a>  SIGMA <span class="ot">=</span> <span class="fu">rbind</span>(SIGMA, <span class="fu">c</span>(Sigma))</span>
<span id="cb50-758"><a href="#cb50-758" aria-hidden="true" tabindex="-1"></a>  Y.MISS <span class="ot">=</span> <span class="fu">rbind</span>(Y.MISS, Y.full[O <span class="sc">==</span> <span class="dv">0</span>])</span>
<span id="cb50-759"><a href="#cb50-759" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb50-760"><a href="#cb50-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-761"><a href="#cb50-761" aria-hidden="true" tabindex="-1"></a><span class="co"># Means and confidence intervals for posterior means</span></span>
<span id="cb50-762"><a href="#cb50-762" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(Y)</span>
<span id="cb50-763"><a href="#cb50-763" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(THETA)</span>
<span id="cb50-764"><a href="#cb50-764" aria-hidden="true" tabindex="-1"></a><span class="fu">apply</span>(THETA, <span class="at">MARGIN =</span> <span class="dv">2</span>, <span class="at">FUN =</span> <span class="cf">function</span>(d) <span class="fu">quantile</span>(d, <span class="at">prob =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>)))</span>
<span id="cb50-765"><a href="#cb50-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-766"><a href="#cb50-766" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample correlation matrices</span></span>
<span id="cb50-767"><a href="#cb50-767" aria-hidden="true" tabindex="-1"></a>COR <span class="ot">=</span> <span class="fu">array</span>(<span class="at">dim =</span> <span class="fu">c</span>(p, p, <span class="dv">1000</span>))</span>
<span id="cb50-768"><a href="#cb50-768" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(SIGMA)) {</span>
<span id="cb50-769"><a href="#cb50-769" aria-hidden="true" tabindex="-1"></a>  <span class="co"># It's in rows right now, refold into matrix</span></span>
<span id="cb50-770"><a href="#cb50-770" aria-hidden="true" tabindex="-1"></a>  Sig <span class="ot">=</span> <span class="fu">matrix</span>(SIGMA[s, ], <span class="at">nrow =</span> p, <span class="at">ncol =</span> p)</span>
<span id="cb50-771"><a href="#cb50-771" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Calculate correlation matrix: (</span><span class="al">TODO</span><span class="co">: figure out how this works)</span></span>
<span id="cb50-772"><a href="#cb50-772" aria-hidden="true" tabindex="-1"></a>  COR[, , s] <span class="ot">=</span> Sig <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="fu">diag</span>(Sig) <span class="sc">%o%</span> <span class="fu">diag</span>(Sig))</span>
<span id="cb50-773"><a href="#cb50-773" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb50-774"><a href="#cb50-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-775"><a href="#cb50-775" aria-hidden="true" tabindex="-1"></a><span class="co"># Mean correlations from the correlation matrix samples. Can also calculate</span></span>
<span id="cb50-776"><a href="#cb50-776" aria-hidden="true" tabindex="-1"></a><span class="co"># confidence intervals with quantile</span></span>
<span id="cb50-777"><a href="#cb50-777" aria-hidden="true" tabindex="-1"></a><span class="fu">apply</span>(COR, <span class="at">MARGIN =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">FUN =</span> mean)</span>
<span id="cb50-778"><a href="#cb50-778" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-779"><a href="#cb50-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-780"><a href="#cb50-780" aria-hidden="true" tabindex="-1"></a><span class="fu"># Exercises</span></span>
<span id="cb50-781"><a href="#cb50-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-782"><a href="#cb50-782" aria-hidden="true" tabindex="-1"></a><span class="fu">## 7.1</span></span>
<span id="cb50-783"><a href="#cb50-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-784"><a href="#cb50-784" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb50-785"><a href="#cb50-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-786"><a href="#cb50-786" aria-hidden="true" tabindex="-1"></a>Since the density is uniform with respect to $\boldsymbol{\theta}$, the integral</span>
<span id="cb50-787"><a href="#cb50-787" aria-hidden="true" tabindex="-1"></a>over the support of this function is infinite and cannot be 1.</span>
<span id="cb50-788"><a href="#cb50-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-789"><a href="#cb50-789" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb50-790"><a href="#cb50-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-791"><a href="#cb50-791" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-792"><a href="#cb50-792" aria-hidden="true" tabindex="-1"></a>p_J(\boldsymbol{\theta}, \Sigma \mid \boldsymbol{y}_1, \dots, \boldsymbol{y}_n) &amp;\propto p(\boldsymbol{\theta}, \Sigma) \times p(\boldsymbol{y}_1, \dots, \boldsymbol{y}_n \mid \boldsymbol{\theta}, \Sigma) <span class="sc">\\</span></span>
<span id="cb50-793"><a href="#cb50-793" aria-hidden="true" tabindex="-1"></a>&amp;\propto \left<span class="co">[</span><span class="ot"> | \Sigma |^{-(p + 2) / 2} \right</span><span class="co">]</span> \times \left<span class="co">[</span><span class="ot"> | \Sigma |^{-n / 2} \exp\left( -\text{tr}(\mathbf{S}_\theta \Sigma^{-1}) \right) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb50-794"><a href="#cb50-794" aria-hidden="true" tabindex="-1"></a>&amp;\propto | \Sigma |^{-(p + n + 2) / 2} \exp \left( -\text{tr}(\mathbf{S}_\theta \Sigma^{-1}) / 2 \right)</span>
<span id="cb50-795"><a href="#cb50-795" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-796"><a href="#cb50-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-797"><a href="#cb50-797" aria-hidden="true" tabindex="-1"></a>To obtain the full conditionals of a parameter, we treat the other parameters as constant, so</span>
<span id="cb50-798"><a href="#cb50-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-799"><a href="#cb50-799" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-800"><a href="#cb50-800" aria-hidden="true" tabindex="-1"></a>p_J(\boldsymbol{\theta} \mid \boldsymbol{y}_1, \dots, \boldsymbol{y}_n, \Sigma) &amp;\propto \exp(- \text{tr}(\mathbf{S}_\theta \Sigma^{-1}) / 2) <span class="sc">\\</span></span>
<span id="cb50-801"><a href="#cb50-801" aria-hidden="true" tabindex="-1"></a>&amp;= \exp(-\sum_{i = 1}^n (\boldsymbol{y}_i - \theta)^T \Sigma^{-1} (\boldsymbol{y}_i - \theta) / 2 ) &amp; \text{Expand back} <span class="sc">\\</span></span>
<span id="cb50-802"><a href="#cb50-802" aria-hidden="true" tabindex="-1"></a>&amp;= \exp(- n (\bar{\boldsymbol{y}} - \theta)^T \Sigma^{-1} (\bar{\boldsymbol{y}} - \theta) / 2 ) <span class="sc">\\</span></span>
<span id="cb50-803"><a href="#cb50-803" aria-hidden="true" tabindex="-1"></a>&amp;= \text{dnormal}(\bar{\boldsymbol{y}}, \Sigma / n)</span>
<span id="cb50-804"><a href="#cb50-804" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-805"><a href="#cb50-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-806"><a href="#cb50-806" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb50-807"><a href="#cb50-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-808"><a href="#cb50-808" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-809"><a href="#cb50-809" aria-hidden="true" tabindex="-1"></a>p_J(\Sigma \mid \boldsymbol{y}_1, \dots, \boldsymbol{y}_n, \boldsymbol{\theta}) &amp;\propto | \Sigma | ^{-(p + n + 2) / 2 } \exp(- \text{tr}(\mathbf{S}_\theta \Sigma^{-1}) / 2) <span class="sc">\\</span></span>
<span id="cb50-810"><a href="#cb50-810" aria-hidden="true" tabindex="-1"></a>&amp;\propto \text{dinverse-wishart}\left(n + 1, \mathbf{S}_\theta^{-1} \right)</span>
<span id="cb50-811"><a href="#cb50-811" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-812"><a href="#cb50-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-813"><a href="#cb50-813" aria-hidden="true" tabindex="-1"></a><span class="fu">## 7.3</span></span>
<span id="cb50-814"><a href="#cb50-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-817"><a href="#cb50-817" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb50-818"><a href="#cb50-818" aria-hidden="true" tabindex="-1"></a>bluecrab <span class="ot">=</span> <span class="fu">as.matrix</span>(<span class="fu">read.table</span>(<span class="st">'Exercises/bluecrab.dat'</span>))</span>
<span id="cb50-819"><a href="#cb50-819" aria-hidden="true" tabindex="-1"></a>orangecrab <span class="ot">=</span> <span class="fu">as.matrix</span>(<span class="fu">read.table</span>(<span class="st">'Exercises/orangecrab.dat'</span>))</span>
<span id="cb50-820"><a href="#cb50-820" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-821"><a href="#cb50-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-822"><a href="#cb50-822" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb50-823"><a href="#cb50-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-824"><a href="#cb50-824" aria-hidden="true" tabindex="-1"></a><span class="in">```{r cache=TRUE}</span></span>
<span id="cb50-825"><a href="#cb50-825" aria-hidden="true" tabindex="-1"></a>crab.mcmc <span class="ot">=</span> <span class="fu">lapply</span>(<span class="fu">list</span>(<span class="st">'bluecrab'</span> <span class="ot">=</span> bluecrab, <span class="st">'orangecrab'</span> <span class="ot">=</span> orangecrab), <span class="cf">function</span>(crab) {</span>
<span id="cb50-826"><a href="#cb50-826" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="fu">ncol</span>(crab)</span>
<span id="cb50-827"><a href="#cb50-827" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="fu">nrow</span>(crab)</span>
<span id="cb50-828"><a href="#cb50-828" aria-hidden="true" tabindex="-1"></a>  ybar <span class="ot">=</span> <span class="fu">colMeans</span>(crab)</span>
<span id="cb50-829"><a href="#cb50-829" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-830"><a href="#cb50-830" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Prior parameters</span></span>
<span id="cb50-831"><a href="#cb50-831" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-832"><a href="#cb50-832" aria-hidden="true" tabindex="-1"></a>  mu0 <span class="ot">=</span> ybar</span>
<span id="cb50-833"><a href="#cb50-833" aria-hidden="true" tabindex="-1"></a>  lambda0 <span class="ot">=</span> s0 <span class="ot">=</span> <span class="fu">cov</span>(crab)</span>
<span id="cb50-834"><a href="#cb50-834" aria-hidden="true" tabindex="-1"></a>  nu0 <span class="ot">=</span> <span class="dv">4</span></span>
<span id="cb50-835"><a href="#cb50-835" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-836"><a href="#cb50-836" aria-hidden="true" tabindex="-1"></a>  S <span class="ot">=</span> <span class="dv">10000</span></span>
<span id="cb50-837"><a href="#cb50-837" aria-hidden="true" tabindex="-1"></a>  THETA <span class="ot">=</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> S, <span class="at">ncol =</span> p)</span>
<span id="cb50-838"><a href="#cb50-838" aria-hidden="true" tabindex="-1"></a>  SIGMA <span class="ot">=</span> <span class="fu">array</span>(<span class="at">dim =</span> <span class="fu">c</span>(p, p, S))</span>
<span id="cb50-839"><a href="#cb50-839" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-840"><a href="#cb50-840" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Start with sigma sample</span></span>
<span id="cb50-841"><a href="#cb50-841" aria-hidden="true" tabindex="-1"></a>  sigma <span class="ot">=</span> s0</span>
<span id="cb50-842"><a href="#cb50-842" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-843"><a href="#cb50-843" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Gibbs sampling</span></span>
<span id="cb50-844"><a href="#cb50-844" aria-hidden="true" tabindex="-1"></a>  <span class="fu">library</span>(MASS)</span>
<span id="cb50-845"><a href="#cb50-845" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-846"><a href="#cb50-846" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Also, inv = solve to make it more readable</span></span>
<span id="cb50-847"><a href="#cb50-847" aria-hidden="true" tabindex="-1"></a>  inv <span class="ot">=</span> solve</span>
<span id="cb50-848"><a href="#cb50-848" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-849"><a href="#cb50-849" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>S) {</span>
<span id="cb50-850"><a href="#cb50-850" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update theta</span></span>
<span id="cb50-851"><a href="#cb50-851" aria-hidden="true" tabindex="-1"></a>    lambdan <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">inv</span>(lambda0) <span class="sc">+</span> n <span class="sc">*</span> <span class="fu">inv</span>(sigma))</span>
<span id="cb50-852"><a href="#cb50-852" aria-hidden="true" tabindex="-1"></a>    mun <span class="ot">=</span> lambdan <span class="sc">%*%</span> (<span class="fu">inv</span>(lambda0) <span class="sc">%*%</span> mu0 <span class="sc">+</span> n <span class="sc">*</span> <span class="fu">inv</span>(sigma) <span class="sc">%*%</span> ybar)</span>
<span id="cb50-853"><a href="#cb50-853" aria-hidden="true" tabindex="-1"></a>    theta <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">1</span>, mun, lambdan)</span>
<span id="cb50-854"><a href="#cb50-854" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-855"><a href="#cb50-855" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update sigma</span></span>
<span id="cb50-856"><a href="#cb50-856" aria-hidden="true" tabindex="-1"></a>    resid <span class="ot">=</span> <span class="fu">t</span>(crab) <span class="sc">-</span> <span class="fu">c</span>(theta)</span>
<span id="cb50-857"><a href="#cb50-857" aria-hidden="true" tabindex="-1"></a>    stheta <span class="ot">=</span> resid <span class="sc">%*%</span> <span class="fu">t</span>(resid)</span>
<span id="cb50-858"><a href="#cb50-858" aria-hidden="true" tabindex="-1"></a>    sn <span class="ot">=</span> s0 <span class="sc">+</span> stheta</span>
<span id="cb50-859"><a href="#cb50-859" aria-hidden="true" tabindex="-1"></a>    sigma <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">rWishart</span>(<span class="dv">1</span>, nu0 <span class="sc">+</span> n, <span class="fu">inv</span>(sn))[, , <span class="dv">1</span>])</span>
<span id="cb50-860"><a href="#cb50-860" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-861"><a href="#cb50-861" aria-hidden="true" tabindex="-1"></a>    THETA[s, ] <span class="ot">=</span> theta</span>
<span id="cb50-862"><a href="#cb50-862" aria-hidden="true" tabindex="-1"></a>    SIGMA[, , s] <span class="ot">=</span> sigma</span>
<span id="cb50-863"><a href="#cb50-863" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb50-864"><a href="#cb50-864" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-865"><a href="#cb50-865" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">theta =</span> THETA, <span class="at">sigma =</span> SIGMA)</span>
<span id="cb50-866"><a href="#cb50-866" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb50-867"><a href="#cb50-867" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-868"><a href="#cb50-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-869"><a href="#cb50-869" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb50-870"><a href="#cb50-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-871"><a href="#cb50-871" aria-hidden="true" tabindex="-1"></a><span class="in">```{r echo=FALSE}</span></span>
<span id="cb50-872"><a href="#cb50-872" aria-hidden="true" tabindex="-1"></a>bluecrab.df <span class="ot">=</span> <span class="fu">data.frame</span>(crab.mcmc<span class="sc">$</span>bluecrab<span class="sc">$</span>theta, <span class="at">species =</span> <span class="st">'blue'</span>)</span>
<span id="cb50-873"><a href="#cb50-873" aria-hidden="true" tabindex="-1"></a>orangecrab.df <span class="ot">=</span> <span class="fu">data.frame</span>(crab.mcmc<span class="sc">$</span>orangecrab<span class="sc">$</span>theta, <span class="at">species =</span> <span class="st">'orange'</span>)</span>
<span id="cb50-874"><a href="#cb50-874" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(bluecrab.df) <span class="ot">=</span> <span class="fu">colnames</span>(orangecrab.df) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">'theta1'</span>, <span class="st">'theta2'</span>, <span class="st">'species'</span>)</span>
<span id="cb50-875"><a href="#cb50-875" aria-hidden="true" tabindex="-1"></a>crab.df <span class="ot">=</span> <span class="fu">rbind</span>(bluecrab.df, orangecrab.df)</span>
<span id="cb50-876"><a href="#cb50-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-877"><a href="#cb50-877" aria-hidden="true" tabindex="-1"></a>bluecrab.means <span class="ot">=</span> <span class="fu">as.data.frame</span>(<span class="fu">t</span>(<span class="fu">as.matrix</span>(<span class="fu">colMeans</span>(bluecrab.df[, <span class="fu">c</span>(<span class="st">'theta1'</span>, <span class="st">'theta2'</span>)]))))</span>
<span id="cb50-878"><a href="#cb50-878" aria-hidden="true" tabindex="-1"></a>orangecrab.means <span class="ot">=</span> <span class="fu">as.data.frame</span>(<span class="fu">t</span>(<span class="fu">as.matrix</span>(<span class="fu">colMeans</span>(orangecrab.df[, <span class="fu">c</span>(<span class="st">'theta1'</span>, <span class="st">'theta2'</span>)]))))</span>
<span id="cb50-879"><a href="#cb50-879" aria-hidden="true" tabindex="-1"></a>bluecrab.means<span class="sc">$</span>species <span class="ot">=</span> <span class="st">'blue'</span></span>
<span id="cb50-880"><a href="#cb50-880" aria-hidden="true" tabindex="-1"></a>orangecrab.means<span class="sc">$</span>species <span class="ot">=</span> <span class="st">'orange'</span></span>
<span id="cb50-881"><a href="#cb50-881" aria-hidden="true" tabindex="-1"></a>crab.means <span class="ot">=</span> <span class="fu">rbind</span>(bluecrab.means, orangecrab.means)</span>
<span id="cb50-882"><a href="#cb50-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-883"><a href="#cb50-883" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggrepel)</span>
<span id="cb50-884"><a href="#cb50-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-885"><a href="#cb50-885" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(crab.df, <span class="fu">aes</span>(<span class="at">x =</span> theta1, <span class="at">y =</span> theta2)) <span class="sc">+</span></span>
<span id="cb50-886"><a href="#cb50-886" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.01</span>) <span class="sc">+</span></span>
<span id="cb50-887"><a href="#cb50-887" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> crab.means, <span class="at">color =</span> <span class="st">'red'</span>) <span class="sc">+</span></span>
<span id="cb50-888"><a href="#cb50-888" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_label_repel</span>(<span class="at">data =</span> crab.means, <span class="fu">aes</span>(<span class="at">label =</span> <span class="fu">paste0</span>(<span class="st">"("</span>, <span class="fu">round</span>(theta1, <span class="dv">2</span>), <span class="st">", "</span>, <span class="fu">round</span>(theta2, <span class="dv">2</span>), <span class="st">")"</span>))) <span class="sc">+</span></span>
<span id="cb50-889"><a href="#cb50-889" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> species)</span>
<span id="cb50-890"><a href="#cb50-890" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-891"><a href="#cb50-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-892"><a href="#cb50-892" aria-hidden="true" tabindex="-1"></a>There is strong evidence that orange crabs tend to be larger in both</span>
<span id="cb50-893"><a href="#cb50-893" aria-hidden="true" tabindex="-1"></a>measurements than blue crabs:</span>
<span id="cb50-894"><a href="#cb50-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-897"><a href="#cb50-897" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb50-898"><a href="#cb50-898" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(orangecrab.df<span class="sc">$</span>theta1 <span class="sc">&gt;</span> bluecrab.df<span class="sc">$</span>theta1)</span>
<span id="cb50-899"><a href="#cb50-899" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(orangecrab.df<span class="sc">$</span>theta2 <span class="sc">&gt;</span> bluecrab.df<span class="sc">$</span>theta2)</span>
<span id="cb50-900"><a href="#cb50-900" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-901"><a href="#cb50-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-902"><a href="#cb50-902" aria-hidden="true" tabindex="-1"></a><span class="fu">### c</span></span>
<span id="cb50-903"><a href="#cb50-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-906"><a href="#cb50-906" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb50-907"><a href="#cb50-907" aria-hidden="true" tabindex="-1"></a>bluecrab.cor <span class="ot">=</span> <span class="fu">apply</span>(crab.mcmc<span class="sc">$</span>bluecrab<span class="sc">$</span>sigma, <span class="at">MARGIN =</span> <span class="dv">3</span>, <span class="at">FUN =</span> <span class="cf">function</span>(covmat) {</span>
<span id="cb50-908"><a href="#cb50-908" aria-hidden="true" tabindex="-1"></a>  covmat[<span class="dv">1</span>, <span class="dv">2</span>] <span class="sc">/</span> (<span class="fu">sqrt</span>(covmat[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">*</span> covmat[<span class="dv">2</span>, <span class="dv">2</span>]))</span>
<span id="cb50-909"><a href="#cb50-909" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb50-910"><a href="#cb50-910" aria-hidden="true" tabindex="-1"></a>orangecrab.cor <span class="ot">=</span> <span class="fu">apply</span>(crab.mcmc<span class="sc">$</span>orangecrab<span class="sc">$</span>sigma, <span class="at">MARGIN =</span> <span class="dv">3</span>, <span class="at">FUN =</span> <span class="cf">function</span>(covmat) {</span>
<span id="cb50-911"><a href="#cb50-911" aria-hidden="true" tabindex="-1"></a>  covmat[<span class="dv">1</span>, <span class="dv">2</span>] <span class="sc">/</span> (<span class="fu">sqrt</span>(covmat[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">*</span> covmat[<span class="dv">2</span>, <span class="dv">2</span>]))</span>
<span id="cb50-912"><a href="#cb50-912" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb50-913"><a href="#cb50-913" aria-hidden="true" tabindex="-1"></a>cor.df <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">species =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">'blue'</span>, <span class="fu">length</span>(bluecrab.cor)), <span class="fu">rep</span>(<span class="st">'orange'</span>, <span class="fu">length</span>(orangecrab.cor))),</span>
<span id="cb50-914"><a href="#cb50-914" aria-hidden="true" tabindex="-1"></a>                    <span class="at">cor =</span> <span class="fu">c</span>(bluecrab.cor, orangecrab.cor))</span>
<span id="cb50-915"><a href="#cb50-915" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(cor.df, <span class="fu">aes</span>(<span class="at">x =</span> cor, <span class="at">fill =</span> species)) <span class="sc">+</span></span>
<span id="cb50-916"><a href="#cb50-916" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb50-917"><a href="#cb50-917" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">'blue'</span>, <span class="st">'orange'</span>))</span>
<span id="cb50-918"><a href="#cb50-918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-919"><a href="#cb50-919" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(bluecrab.cor <span class="sc">&lt;</span> orangecrab.cor)</span>
<span id="cb50-920"><a href="#cb50-920" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-921"><a href="#cb50-921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-922"><a href="#cb50-922" aria-hidden="true" tabindex="-1"></a>The orange crab species appears to have a much higher correlation between its two measurements than the blue crab species.</span>
<span id="cb50-923"><a href="#cb50-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-924"><a href="#cb50-924" aria-hidden="true" tabindex="-1"></a><span class="fu">## 7.4</span></span>
<span id="cb50-925"><a href="#cb50-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-928"><a href="#cb50-928" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb50-929"><a href="#cb50-929" aria-hidden="true" tabindex="-1"></a>agehw <span class="ot">=</span> <span class="fu">as.matrix</span>(<span class="fu">read.table</span>(<span class="st">'Exercises/agehw.dat'</span>))</span>
<span id="cb50-930"><a href="#cb50-930" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(agehw) <span class="ot">=</span> agehw[<span class="dv">1</span>, ]</span>
<span id="cb50-931"><a href="#cb50-931" aria-hidden="true" tabindex="-1"></a>agehw <span class="ot">=</span> agehw[<span class="sc">-</span><span class="dv">1</span>, ]</span>
<span id="cb50-932"><a href="#cb50-932" aria-hidden="true" tabindex="-1"></a>agehw <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">as.numeric</span>(agehw), <span class="at">nrow =</span> <span class="dv">100</span>)</span>
<span id="cb50-933"><a href="#cb50-933" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-934"><a href="#cb50-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-935"><a href="#cb50-935" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb50-936"><a href="#cb50-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-937"><a href="#cb50-937" aria-hidden="true" tabindex="-1"></a>With typical intuitions about life expectancy and age of marriage, I suspect</span>
<span id="cb50-938"><a href="#cb50-938" aria-hidden="true" tabindex="-1"></a>that the ages of most of the married couples will fall between 25 and 80. There</span>
<span id="cb50-939"><a href="#cb50-939" aria-hidden="true" tabindex="-1"></a>may be slight age differences among men and women, but not enough to warrant</span>
<span id="cb50-940"><a href="#cb50-940" aria-hidden="true" tabindex="-1"></a>encoding in the prior. Thus $\boldsymbol{\mu}_0 = ((25 + 80) / 2, (25 + 80) / 2) = (52.5, 52.5)^T$ is set.</span>
<span id="cb50-941"><a href="#cb50-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-942"><a href="#cb50-942" aria-hidden="true" tabindex="-1"></a>A semiconjugate prior distribution is chosen in this case,</span>
<span id="cb50-943"><a href="#cb50-943" aria-hidden="true" tabindex="-1"></a>as it seems intuitive that there are fewer married couples at ages 25 and 80</span>
<span id="cb50-944"><a href="#cb50-944" aria-hidden="true" tabindex="-1"></a>than there are married couples around age 50, thus justifying a bell curve</span>
<span id="cb50-945"><a href="#cb50-945" aria-hidden="true" tabindex="-1"></a>centered around $52.5$ with variance $13.75^2 \approx 189$ such that</span>
<span id="cb50-946"><a href="#cb50-946" aria-hidden="true" tabindex="-1"></a>approximately 95\% of the prior falls within the range $(25, 80)$.</span>
<span id="cb50-947"><a href="#cb50-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-948"><a href="#cb50-948" aria-hidden="true" tabindex="-1"></a>The ages of the couples are expected to be quite tightly</span>
<span id="cb50-949"><a href="#cb50-949" aria-hidden="true" tabindex="-1"></a>correlated, so knowing the above variance, a prior correlation of</span>
<span id="cb50-950"><a href="#cb50-950" aria-hidden="true" tabindex="-1"></a>$0.75$ is targeted. Solving the correlation equation gives</span>
<span id="cb50-951"><a href="#cb50-951" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-952"><a href="#cb50-952" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb50-953"><a href="#cb50-953" aria-hidden="true" tabindex="-1"></a>&amp; 0.75 = \frac{\sigma_{1, 2}}{189} <span class="sc">\\</span></span>
<span id="cb50-954"><a href="#cb50-954" aria-hidden="true" tabindex="-1"></a>\implies&amp; \sigma_{1, 2} = 141.75</span>
<span id="cb50-955"><a href="#cb50-955" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb50-956"><a href="#cb50-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-957"><a href="#cb50-957" aria-hidden="true" tabindex="-1"></a>Thus $$\Lambda_0 = \begin{bmatrix} 189 &amp; 141.75 <span class="sc">\\</span> 141.75 &amp; 189 \end{bmatrix} $$ is set.</span>
<span id="cb50-958"><a href="#cb50-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-959"><a href="#cb50-959" aria-hidden="true" tabindex="-1"></a>Like previous problems, for the variance, $\mathbf{S}_0^{-1} =</span>
<span id="cb50-960"><a href="#cb50-960" aria-hidden="true" tabindex="-1"></a>\Lambda_0$ and $\nu_0 = p + 2 = 4$ are set. Note that this only loosely centers the</span>
<span id="cb50-961"><a href="#cb50-961" aria-hidden="true" tabindex="-1"></a>covariance matrix prior on $\Lambda_0$, representing a conservative approach</span>
<span id="cb50-962"><a href="#cb50-962" aria-hidden="true" tabindex="-1"></a>in terms of belief in the variance and the correlation between the ages.</span>
<span id="cb50-963"><a href="#cb50-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-966"><a href="#cb50-966" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb50-967"><a href="#cb50-967" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> agehw</span>
<span id="cb50-968"><a href="#cb50-968" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">ncol</span>(agehw)</span>
<span id="cb50-969"><a href="#cb50-969" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(agehw)</span>
<span id="cb50-970"><a href="#cb50-970" aria-hidden="true" tabindex="-1"></a>ybar <span class="ot">=</span> <span class="fu">colMeans</span>(agehw)</span>
<span id="cb50-971"><a href="#cb50-971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-972"><a href="#cb50-972" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="fu">rep</span>(<span class="fl">52.5</span>, p)</span>
<span id="cb50-973"><a href="#cb50-973" aria-hidden="true" tabindex="-1"></a>lambda0 <span class="ot">=</span> s0 <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">189</span>, <span class="fl">141.75</span>), <span class="fu">c</span>(<span class="fl">141.75</span>, <span class="dv">189</span>))</span>
<span id="cb50-974"><a href="#cb50-974" aria-hidden="true" tabindex="-1"></a><span class="co"># nu0 = p + 2</span></span>
<span id="cb50-975"><a href="#cb50-975" aria-hidden="true" tabindex="-1"></a>nu0 <span class="ot">=</span> p <span class="sc">+</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">10</span></span>
<span id="cb50-976"><a href="#cb50-976" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-977"><a href="#cb50-977" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-978"><a href="#cb50-978" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb50-979"><a href="#cb50-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-980"><a href="#cb50-980" aria-hidden="true" tabindex="-1"></a>The wording of the question is interesting - the assumption is to sample a </span>
<span id="cb50-981"><a href="#cb50-981" aria-hidden="true" tabindex="-1"></a>fixed $\boldsymbol{\theta}, \Sigma$ and from there sample $100$ points all with </span>
<span id="cb50-982"><a href="#cb50-982" aria-hidden="true" tabindex="-1"></a>the same parameters. An alternative approach would be to sample a</span>
<span id="cb50-983"><a href="#cb50-983" aria-hidden="true" tabindex="-1"></a>new data point for each sample of $\boldsymbol{\theta}, \Sigma$.</span>
<span id="cb50-984"><a href="#cb50-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-985"><a href="#cb50-985" aria-hidden="true" tabindex="-1"></a>In fact, because of that wording, $\nu_0 = p + 2 = 4$ was originally set to</span>
<span id="cb50-986"><a href="#cb50-986" aria-hidden="true" tabindex="-1"></a>loosely center the prior. But given that this variance will often produce uncorrelated</span>
<span id="cb50-987"><a href="#cb50-987" aria-hidden="true" tabindex="-1"></a>prior predictive datasets, $\nu_0$ is increased slightly.</span>
<span id="cb50-988"><a href="#cb50-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-989"><a href="#cb50-989" aria-hidden="true" tabindex="-1"></a>After increasing $\nu_0$, the posterior</span>
<span id="cb50-990"><a href="#cb50-990" aria-hidden="true" tabindex="-1"></a>predictive datasets appear satisfactory.</span>
<span id="cb50-991"><a href="#cb50-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-994"><a href="#cb50-994" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb50-995"><a href="#cb50-995" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb50-996"><a href="#cb50-996" aria-hidden="true" tabindex="-1"></a>S <span class="ot">=</span> <span class="dv">12</span></span>
<span id="cb50-997"><a href="#cb50-997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-998"><a href="#cb50-998" aria-hidden="true" tabindex="-1"></a>Y_preds <span class="ot">=</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span>S, <span class="cf">function</span>(s) {</span>
<span id="cb50-999"><a href="#cb50-999" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Sample THETA according to prior</span></span>
<span id="cb50-1000"><a href="#cb50-1000" aria-hidden="true" tabindex="-1"></a>  theta <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">1</span>, mu0, lambda0)</span>
<span id="cb50-1001"><a href="#cb50-1001" aria-hidden="true" tabindex="-1"></a>  sigma <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">rWishart</span>(<span class="dv">1</span>, nu0, <span class="fu">inv</span>(s0))[, , <span class="dv">1</span>])</span>
<span id="cb50-1002"><a href="#cb50-1002" aria-hidden="true" tabindex="-1"></a>  Y_s <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">100</span>, theta, sigma)</span>
<span id="cb50-1003"><a href="#cb50-1003" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">Y1 =</span> Y_s[, <span class="dv">1</span>], <span class="at">Y2 =</span> Y_s[, <span class="dv">2</span>], <span class="at">dataset =</span> s)</span>
<span id="cb50-1004"><a href="#cb50-1004" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb50-1005"><a href="#cb50-1005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1006"><a href="#cb50-1006" aria-hidden="true" tabindex="-1"></a>Y_comb <span class="ot">=</span> <span class="fu">do.call</span>(rbind, Y_preds)</span>
<span id="cb50-1007"><a href="#cb50-1007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1008"><a href="#cb50-1008" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(Y_comb, <span class="fu">aes</span>(<span class="at">x =</span> Y1, <span class="at">y =</span> Y2)) <span class="sc">+</span></span>
<span id="cb50-1009"><a href="#cb50-1009" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb50-1010"><a href="#cb50-1010" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> dataset)</span>
<span id="cb50-1011"><a href="#cb50-1011" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-1012"><a href="#cb50-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1013"><a href="#cb50-1013" aria-hidden="true" tabindex="-1"></a><span class="fu">### c</span></span>
<span id="cb50-1014"><a href="#cb50-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1017"><a href="#cb50-1017" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb50-1018"><a href="#cb50-1018" aria-hidden="true" tabindex="-1"></a>S <span class="ot">=</span> <span class="dv">10000</span></span>
<span id="cb50-1019"><a href="#cb50-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1020"><a href="#cb50-1020" aria-hidden="true" tabindex="-1"></a><span class="co"># Reuse this since we'll need to specify different priors</span></span>
<span id="cb50-1021"><a href="#cb50-1021" aria-hidden="true" tabindex="-1"></a>do_mcmc <span class="ot">=</span> <span class="cf">function</span>(Y, mu0, lambda0, s0, nu0) {</span>
<span id="cb50-1022"><a href="#cb50-1022" aria-hidden="true" tabindex="-1"></a>  ybar <span class="ot">=</span> <span class="fu">colMeans</span>(Y)</span>
<span id="cb50-1023"><a href="#cb50-1023" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="fu">ncol</span>(Y)</span>
<span id="cb50-1024"><a href="#cb50-1024" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">=</span> <span class="fu">nrow</span>(Y)</span>
<span id="cb50-1025"><a href="#cb50-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1026"><a href="#cb50-1026" aria-hidden="true" tabindex="-1"></a>  THETA <span class="ot">=</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> S, <span class="at">ncol =</span> p)</span>
<span id="cb50-1027"><a href="#cb50-1027" aria-hidden="true" tabindex="-1"></a>  SIGMA <span class="ot">=</span> <span class="fu">array</span>(<span class="at">dim =</span> <span class="fu">c</span>(p, p, S))</span>
<span id="cb50-1028"><a href="#cb50-1028" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-1029"><a href="#cb50-1029" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Start with sigma sample</span></span>
<span id="cb50-1030"><a href="#cb50-1030" aria-hidden="true" tabindex="-1"></a>  sigma <span class="ot">=</span> <span class="fu">cov</span>(Y)</span>
<span id="cb50-1031"><a href="#cb50-1031" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-1032"><a href="#cb50-1032" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Gibbs sampling</span></span>
<span id="cb50-1033"><a href="#cb50-1033" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-1034"><a href="#cb50-1034" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Also, inv = solve to make it more readable</span></span>
<span id="cb50-1035"><a href="#cb50-1035" aria-hidden="true" tabindex="-1"></a>  inv <span class="ot">=</span> solve</span>
<span id="cb50-1036"><a href="#cb50-1036" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-1037"><a href="#cb50-1037" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>S) {</span>
<span id="cb50-1038"><a href="#cb50-1038" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update theta</span></span>
<span id="cb50-1039"><a href="#cb50-1039" aria-hidden="true" tabindex="-1"></a>    lambdan <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">inv</span>(lambda0) <span class="sc">+</span> n <span class="sc">*</span> <span class="fu">inv</span>(sigma))</span>
<span id="cb50-1040"><a href="#cb50-1040" aria-hidden="true" tabindex="-1"></a>    mun <span class="ot">=</span> lambdan <span class="sc">%*%</span> (<span class="fu">inv</span>(lambda0) <span class="sc">%*%</span> mu0 <span class="sc">+</span> n <span class="sc">*</span> <span class="fu">inv</span>(sigma) <span class="sc">%*%</span> ybar)</span>
<span id="cb50-1041"><a href="#cb50-1041" aria-hidden="true" tabindex="-1"></a>    theta <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">1</span>, mun, lambdan)</span>
<span id="cb50-1042"><a href="#cb50-1042" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-1043"><a href="#cb50-1043" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update sigma</span></span>
<span id="cb50-1044"><a href="#cb50-1044" aria-hidden="true" tabindex="-1"></a>    resid <span class="ot">=</span> <span class="fu">t</span>(Y) <span class="sc">-</span> <span class="fu">c</span>(theta)</span>
<span id="cb50-1045"><a href="#cb50-1045" aria-hidden="true" tabindex="-1"></a>    stheta <span class="ot">=</span> resid <span class="sc">%*%</span> <span class="fu">t</span>(resid)</span>
<span id="cb50-1046"><a href="#cb50-1046" aria-hidden="true" tabindex="-1"></a>    sn <span class="ot">=</span> s0 <span class="sc">+</span> stheta</span>
<span id="cb50-1047"><a href="#cb50-1047" aria-hidden="true" tabindex="-1"></a>    sigma <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">rWishart</span>(<span class="dv">1</span>, nu0 <span class="sc">+</span> n, <span class="fu">inv</span>(sn))[, , <span class="dv">1</span>])</span>
<span id="cb50-1048"><a href="#cb50-1048" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb50-1049"><a href="#cb50-1049" aria-hidden="true" tabindex="-1"></a>    THETA[s, ] <span class="ot">=</span> theta</span>
<span id="cb50-1050"><a href="#cb50-1050" aria-hidden="true" tabindex="-1"></a>    SIGMA[, , s] <span class="ot">=</span> sigma</span>
<span id="cb50-1051"><a href="#cb50-1051" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb50-1052"><a href="#cb50-1052" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-1053"><a href="#cb50-1053" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">theta =</span> THETA, <span class="at">sigma =</span> SIGMA)</span>
<span id="cb50-1054"><a href="#cb50-1054" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb50-1055"><a href="#cb50-1055" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1056"><a href="#cb50-1056" aria-hidden="true" tabindex="-1"></a>my_prior_mcmc <span class="ot">=</span> <span class="fu">do_mcmc</span>(agehw, mu0, lambda0, s0, nu0)</span>
<span id="cb50-1057"><a href="#cb50-1057" aria-hidden="true" tabindex="-1"></a>THETA <span class="ot">=</span> my_prior_mcmc<span class="sc">$</span>theta</span>
<span id="cb50-1058"><a href="#cb50-1058" aria-hidden="true" tabindex="-1"></a>SIGMA <span class="ot">=</span> my_prior_mcmc<span class="sc">$</span>sigma</span>
<span id="cb50-1059"><a href="#cb50-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1060"><a href="#cb50-1060" aria-hidden="true" tabindex="-1"></a><span class="co"># For reuse later</span></span>
<span id="cb50-1061"><a href="#cb50-1061" aria-hidden="true" tabindex="-1"></a>print_quantiles <span class="ot">=</span> <span class="cf">function</span>(THETA, SIGMA) {</span>
<span id="cb50-1062"><a href="#cb50-1062" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="st">"Husband"</span>)</span>
<span id="cb50-1063"><a href="#cb50-1063" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">quantile</span>(THETA[, <span class="dv">1</span>], <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))) <span class="co"># Husband</span></span>
<span id="cb50-1064"><a href="#cb50-1064" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="st">"Wife"</span>)</span>
<span id="cb50-1065"><a href="#cb50-1065" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">quantile</span>(THETA[, <span class="dv">2</span>], <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>))) <span class="co"># Wife</span></span>
<span id="cb50-1066"><a href="#cb50-1066" aria-hidden="true" tabindex="-1"></a>  cors <span class="ot">=</span> <span class="fu">apply</span>(SIGMA, <span class="at">MARGIN =</span> <span class="dv">3</span>, <span class="at">FUN =</span> <span class="cf">function</span>(covmat) {</span>
<span id="cb50-1067"><a href="#cb50-1067" aria-hidden="true" tabindex="-1"></a>    covmat[<span class="dv">1</span>, <span class="dv">2</span>] <span class="sc">/</span> (<span class="fu">sqrt</span>(covmat[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">*</span> covmat[<span class="dv">2</span>, <span class="dv">2</span>]))</span>
<span id="cb50-1068"><a href="#cb50-1068" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb50-1069"><a href="#cb50-1069" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="st">"Correlation"</span>)</span>
<span id="cb50-1070"><a href="#cb50-1070" aria-hidden="true" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">quantile</span>(cors, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.5</span>, <span class="fl">0.975</span>)))</span>
<span id="cb50-1071"><a href="#cb50-1071" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb50-1072"><a href="#cb50-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1073"><a href="#cb50-1073" aria-hidden="true" tabindex="-1"></a><span class="fu">print_quantiles</span>(THETA, SIGMA)</span>
<span id="cb50-1074"><a href="#cb50-1074" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-1075"><a href="#cb50-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1076"><a href="#cb50-1076" aria-hidden="true" tabindex="-1"></a><span class="fu">### d</span></span>
<span id="cb50-1077"><a href="#cb50-1077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1078"><a href="#cb50-1078" aria-hidden="true" tabindex="-1"></a>Exercise 7.2 has not been completed, but doing Jeffreys' prior and a "diffuse prior" below will</span>
<span id="cb50-1079"><a href="#cb50-1079" aria-hidden="true" tabindex="-1"></a>still be helpful to see what effect prior information has.</span>
<span id="cb50-1080"><a href="#cb50-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1081"><a href="#cb50-1081" aria-hidden="true" tabindex="-1"></a><span class="fu">#### i</span></span>
<span id="cb50-1082"><a href="#cb50-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1085"><a href="#cb50-1085" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb50-1086"><a href="#cb50-1086" aria-hidden="true" tabindex="-1"></a>THETA <span class="ot">=</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> S, <span class="at">ncol =</span> p)</span>
<span id="cb50-1087"><a href="#cb50-1087" aria-hidden="true" tabindex="-1"></a>SIGMA <span class="ot">=</span> <span class="fu">array</span>(<span class="at">dim =</span> <span class="fu">c</span>(p, p, S))</span>
<span id="cb50-1088"><a href="#cb50-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1089"><a href="#cb50-1089" aria-hidden="true" tabindex="-1"></a><span class="co"># Start with sigma sample</span></span>
<span id="cb50-1090"><a href="#cb50-1090" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">=</span> <span class="fu">cov</span>(Y)</span>
<span id="cb50-1091"><a href="#cb50-1091" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1092"><a href="#cb50-1092" aria-hidden="true" tabindex="-1"></a><span class="co"># Gibbs sampling</span></span>
<span id="cb50-1093"><a href="#cb50-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1094"><a href="#cb50-1094" aria-hidden="true" tabindex="-1"></a><span class="co"># Also, inv = solve to make it more readable</span></span>
<span id="cb50-1095"><a href="#cb50-1095" aria-hidden="true" tabindex="-1"></a>inv <span class="ot">=</span> solve</span>
<span id="cb50-1096"><a href="#cb50-1096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1097"><a href="#cb50-1097" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>S) {</span>
<span id="cb50-1098"><a href="#cb50-1098" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update theta</span></span>
<span id="cb50-1099"><a href="#cb50-1099" aria-hidden="true" tabindex="-1"></a>  theta <span class="ot">=</span> <span class="fu">mvrnorm</span>(<span class="at">n =</span> <span class="dv">1</span>, ybar, sigma <span class="sc">/</span> n)</span>
<span id="cb50-1100"><a href="#cb50-1100" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-1101"><a href="#cb50-1101" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Update sigma</span></span>
<span id="cb50-1102"><a href="#cb50-1102" aria-hidden="true" tabindex="-1"></a>  resid <span class="ot">=</span> <span class="fu">t</span>(Y) <span class="sc">-</span> <span class="fu">c</span>(theta)</span>
<span id="cb50-1103"><a href="#cb50-1103" aria-hidden="true" tabindex="-1"></a>  stheta <span class="ot">=</span> resid <span class="sc">%*%</span> <span class="fu">t</span>(resid)</span>
<span id="cb50-1104"><a href="#cb50-1104" aria-hidden="true" tabindex="-1"></a>  sigma <span class="ot">=</span> <span class="fu">inv</span>(<span class="fu">rWishart</span>(<span class="dv">1</span>, n <span class="sc">+</span> <span class="dv">1</span>, <span class="fu">inv</span>(stheta))[, , <span class="dv">1</span>])</span>
<span id="cb50-1105"><a href="#cb50-1105" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-1106"><a href="#cb50-1106" aria-hidden="true" tabindex="-1"></a>  THETA[s, ] <span class="ot">=</span> theta</span>
<span id="cb50-1107"><a href="#cb50-1107" aria-hidden="true" tabindex="-1"></a>  SIGMA[, , s] <span class="ot">=</span> sigma</span>
<span id="cb50-1108"><a href="#cb50-1108" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb50-1109"><a href="#cb50-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1110"><a href="#cb50-1110" aria-hidden="true" tabindex="-1"></a><span class="fu">print_quantiles</span>(THETA, SIGMA)</span>
<span id="cb50-1111"><a href="#cb50-1111" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-1112"><a href="#cb50-1112" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb50-1113"><a href="#cb50-1113" aria-hidden="true" tabindex="-1"></a><span class="fu">#### ii</span></span>
<span id="cb50-1114"><a href="#cb50-1114" aria-hidden="true" tabindex="-1"></a>Unit information prior (skipping)</span>
<span id="cb50-1115"><a href="#cb50-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1116"><a href="#cb50-1116" aria-hidden="true" tabindex="-1"></a><span class="fu">#### iii</span></span>
<span id="cb50-1119"><a href="#cb50-1119" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb50-1120"><a href="#cb50-1120" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, p)</span>
<span id="cb50-1121"><a href="#cb50-1121" aria-hidden="true" tabindex="-1"></a>lambda0 <span class="ot">=</span> <span class="dv">10</span><span class="sc">^</span><span class="dv">5</span> <span class="sc">*</span> <span class="fu">diag</span>(p)</span>
<span id="cb50-1122"><a href="#cb50-1122" aria-hidden="true" tabindex="-1"></a>s0 <span class="ot">=</span> <span class="dv">1000</span> <span class="sc">*</span> <span class="fu">diag</span>(p)</span>
<span id="cb50-1123"><a href="#cb50-1123" aria-hidden="true" tabindex="-1"></a>nu0 <span class="ot">=</span> <span class="dv">3</span></span>
<span id="cb50-1124"><a href="#cb50-1124" aria-hidden="true" tabindex="-1"></a>diffuse_mcmc <span class="ot">=</span> <span class="fu">do_mcmc</span>(agehw, mu0, lambda0, s0, nu0)</span>
<span id="cb50-1125"><a href="#cb50-1125" aria-hidden="true" tabindex="-1"></a><span class="fu">print_quantiles</span>(diffuse_mcmc<span class="sc">$</span>theta, diffuse_mcmc<span class="sc">$</span>sigma)</span>
<span id="cb50-1126"><a href="#cb50-1126" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-1127"><a href="#cb50-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1128"><a href="#cb50-1128" aria-hidden="true" tabindex="-1"></a><span class="fu">### e</span></span>
<span id="cb50-1129"><a href="#cb50-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1130"><a href="#cb50-1130" aria-hidden="true" tabindex="-1"></a>It doesn't really seem like the prior information matters because the sample </span>
<span id="cb50-1131"><a href="#cb50-1131" aria-hidden="true" tabindex="-1"></a>size is so large. Regardless of whether the prior is informative, the quantiles </span>
<span id="cb50-1132"><a href="#cb50-1132" aria-hidden="true" tabindex="-1"></a>and correlations end up quite similar. Maybe the diffuse prior is slightly </span>
<span id="cb50-1133"><a href="#cb50-1133" aria-hidden="true" tabindex="-1"></a>different, but it's not a big difference.</span>
<span id="cb50-1134"><a href="#cb50-1134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1135"><a href="#cb50-1135" aria-hidden="true" tabindex="-1"></a>If a smaller sample size is used, this may be different. The following shows what </span>
<span id="cb50-1136"><a href="#cb50-1136" aria-hidden="true" tabindex="-1"></a>happens when the dataset is truncated to the first 25 variables and rerun with the </span>
<span id="cb50-1137"><a href="#cb50-1137" aria-hidden="true" tabindex="-1"></a>informative prior and the diffuse prior:</span>
<span id="cb50-1138"><a href="#cb50-1138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1139"><a href="#cb50-1139" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Informative prior</span></span>
<span id="cb50-1140"><a href="#cb50-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1143"><a href="#cb50-1143" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb50-1144"><a href="#cb50-1144" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="fu">rep</span>(<span class="fl">52.5</span>, p)</span>
<span id="cb50-1145"><a href="#cb50-1145" aria-hidden="true" tabindex="-1"></a>lambda0 <span class="ot">=</span> s0 <span class="ot">=</span> <span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">189</span>, <span class="fl">141.75</span>), <span class="fu">c</span>(<span class="fl">141.75</span>, <span class="dv">189</span>))</span>
<span id="cb50-1146"><a href="#cb50-1146" aria-hidden="true" tabindex="-1"></a><span class="co"># nu0 = p + 2</span></span>
<span id="cb50-1147"><a href="#cb50-1147" aria-hidden="true" tabindex="-1"></a>nu0 <span class="ot">=</span> p <span class="sc">+</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">10</span></span>
<span id="cb50-1148"><a href="#cb50-1148" aria-hidden="true" tabindex="-1"></a>my_prior_mcmc_short <span class="ot">=</span> <span class="fu">do_mcmc</span>(agehw[<span class="dv">1</span><span class="sc">:</span><span class="dv">25</span>, ], mu0, lambda0, s0, nu0)</span>
<span id="cb50-1149"><a href="#cb50-1149" aria-hidden="true" tabindex="-1"></a><span class="fu">print_quantiles</span>(my_prior_mcmc_short<span class="sc">$</span>theta, my_prior_mcmc_short<span class="sc">$</span>sigma)</span>
<span id="cb50-1150"><a href="#cb50-1150" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-1151"><a href="#cb50-1151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1152"><a href="#cb50-1152" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Diffuse prior</span></span>
<span id="cb50-1153"><a href="#cb50-1153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1156"><a href="#cb50-1156" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb50-1157"><a href="#cb50-1157" aria-hidden="true" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, p)</span>
<span id="cb50-1158"><a href="#cb50-1158" aria-hidden="true" tabindex="-1"></a>lambda0 <span class="ot">=</span> <span class="dv">10</span><span class="sc">^</span><span class="dv">5</span> <span class="sc">*</span> <span class="fu">diag</span>(p)</span>
<span id="cb50-1159"><a href="#cb50-1159" aria-hidden="true" tabindex="-1"></a>s0 <span class="ot">=</span> <span class="dv">1000</span> <span class="sc">*</span> <span class="fu">diag</span>(p)</span>
<span id="cb50-1160"><a href="#cb50-1160" aria-hidden="true" tabindex="-1"></a>nu0 <span class="ot">=</span> <span class="dv">3</span></span>
<span id="cb50-1161"><a href="#cb50-1161" aria-hidden="true" tabindex="-1"></a>diffuse_mcmc_short <span class="ot">=</span> <span class="fu">do_mcmc</span>(agehw[<span class="dv">1</span><span class="sc">:</span><span class="dv">25</span>, ], mu0, lambda0, s0, nu0)</span>
<span id="cb50-1162"><a href="#cb50-1162" aria-hidden="true" tabindex="-1"></a><span class="fu">print_quantiles</span>(diffuse_mcmc_short<span class="sc">$</span>theta, diffuse_mcmc_short<span class="sc">$</span>sigma)</span>
<span id="cb50-1163"><a href="#cb50-1163" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb50-1164"><a href="#cb50-1164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-1165"><a href="#cb50-1165" aria-hidden="true" tabindex="-1"></a>In this case, the effect of the prior on correlation especially is easily </span>
<span id="cb50-1166"><a href="#cb50-1166" aria-hidden="true" tabindex="-1"></a>observed. The correlation for the diffuse prior is quite low, as it is being </span>
<span id="cb50-1167"><a href="#cb50-1167" aria-hidden="true" tabindex="-1"></a>dragged towards nothing.</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>