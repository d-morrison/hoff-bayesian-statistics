<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Jesse Mu">
<title>Chapter 2: Belief, probability, and exchangeability – Hoff Bayesian Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-68c8bffd90dad8f2b55c52d7b6410dc0.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-68c8bffd90dad8f2b55c52d7b6410dc0.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-68c8bffd90dad8f2b55c52d7b6410dc0.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-26500bfc55c7891837a911d6d50a6255.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-0358071c7a347cfe579aff309456bf3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-26500bfc55c7891837a911d6d50a6255.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false,
  "enableExperimentalNewNoteButton": true
}
</script><script async="" src="https://hypothes.is/embed.js"></script><script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script><link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="styles.css">
</head>
<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="navbar navbar-expand-lg " data-bs-theme="dark"><div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Hoff Bayesian Statistics</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
<li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-chapters" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Chapters</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-chapters">
<li>
    <a class="dropdown-item" href="./1.html">
 <span class="dropdown-text">Chapter 1: Introduction and examples</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./2.html">
 <span class="dropdown-text">Chapter 2: Belief, probability, and exchangeability</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./3.html">
 <span class="dropdown-text">Chapter 3: One-parameter models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./4.html">
 <span class="dropdown-text">Chapter 4: Monte Carlo approximation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./5.html">
 <span class="dropdown-text">Chapter 5: The Normal Model</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./6.html">
 <span class="dropdown-text">Chapter 6: Posterior approximation with the Gibbs sampler</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./7.html">
 <span class="dropdown-text">Chapter 7: The multivariate normal model</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./8.html">
 <span class="dropdown-text">Chapter 8: Group comparisons and hierarchical modeling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./9.html">
 <span class="dropdown-text">Chapter 9: Linear regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./10.html">
 <span class="dropdown-text">Chapter 10: Nonconjugate priors and Metropolis-Hastings algorithms</span></a>
  </li>  
    </ul>
</li>
  <li class="nav-item">
    <a class="nav-link" href="./irm.html"> 
<span class="menu-text">Infinite Relational Model</span></a>
  </li>  
</ul>
</div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">On this page</h2>
   
  <ul>
<li><a href="#belief-functions-and-properties" id="toc-belief-functions-and-properties" class="nav-link active" data-scroll-target="#belief-functions-and-properties"><span class="header-section-number">1</span> Belief functions and properties</a></li>
  <li><a href="#events-partitions-and-bayes-rule" id="toc-events-partitions-and-bayes-rule" class="nav-link" data-scroll-target="#events-partitions-and-bayes-rule"><span class="header-section-number">2</span> Events, partitions, and Bayes’ rule</a></li>
  <li><a href="#independence" id="toc-independence" class="nav-link" data-scroll-target="#independence"><span class="header-section-number">3</span> Independence</a></li>
  <li>
<a href="#random-variables-and-their-distributions" id="toc-random-variables-and-their-distributions" class="nav-link" data-scroll-target="#random-variables-and-their-distributions"><span class="header-section-number">4</span> Random variables and their distributions</a>
  <ul class="collapse">
<li><a href="#descriptions-of-distributions" id="toc-descriptions-of-distributions" class="nav-link" data-scroll-target="#descriptions-of-distributions"><span class="header-section-number">4.1</span> Descriptions of distributions</a></li>
  <li>
<a href="#joint-distributions" id="toc-joint-distributions" class="nav-link" data-scroll-target="#joint-distributions"><span class="header-section-number">4.2</span> Joint distributions</a>
  <ul class="collapse">
<li><a href="#discrete" id="toc-discrete" class="nav-link" data-scroll-target="#discrete"><span class="header-section-number">4.2.1</span> Discrete</a></li>
  <li><a href="#continuous" id="toc-continuous" class="nav-link" data-scroll-target="#continuous"><span class="header-section-number">4.2.2</span> Continuous</a></li>
  </ul>
</li>
  <li><a href="#independent-random-variables" id="toc-independent-random-variables" class="nav-link" data-scroll-target="#independent-random-variables"><span class="header-section-number">4.3</span> Independent random variables</a></li>
  </ul>
</li>
  <li><a href="#exchangeability" id="toc-exchangeability" class="nav-link" data-scroll-target="#exchangeability"><span class="header-section-number">5</span> Exchangeability</a></li>
  <li>
<a href="#de-finettis-theorem" id="toc-de-finettis-theorem" class="nav-link" data-scroll-target="#de-finettis-theorem"><span class="header-section-number">6</span> de Finetti’s theorem</a>
  <ul class="collapse">
<li>
<a href="#proof-for-0-1-random-variables" id="toc-proof-for-0-1-random-variables" class="nav-link" data-scroll-target="#proof-for-0-1-random-variables"><span class="header-section-number">6.1</span> Proof for {0, 1} random variables</a>
  <ul class="collapse">
<li><a href="#alternative-statement-of-theorem" id="toc-alternative-statement-of-theorem" class="nav-link" data-scroll-target="#alternative-statement-of-theorem"><span class="header-section-number">6.1.1</span> Alternative statement of theorem</a></li>
  <li><a href="#proof" id="toc-proof" class="nav-link" data-scroll-target="#proof"><span class="header-section-number">6.1.2</span> Proof</a></li>
  </ul>
</li>
  </ul>
</li>
  <li>
<a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">7</span> Exercises</a>
  <ul class="collapse">
<li>
<a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section"><span class="header-section-number">7.1</span> 2.1</a>
  <ul class="collapse">
<li><a href="#a" id="toc-a" class="nav-link" data-scroll-target="#a"><span class="header-section-number">7.1.1</span> a</a></li>
  <li><a href="#b" id="toc-b" class="nav-link" data-scroll-target="#b"><span class="header-section-number">7.1.2</span> b</a></li>
  <li><a href="#c" id="toc-c" class="nav-link" data-scroll-target="#c"><span class="header-section-number">7.1.3</span> c</a></li>
  <li><a href="#d" id="toc-d" class="nav-link" data-scroll-target="#d"><span class="header-section-number">7.1.4</span> d</a></li>
  </ul>
</li>
  <li>
<a href="#section-1" id="toc-section-1" class="nav-link" data-scroll-target="#section-1"><span class="header-section-number">7.2</span> 2.2</a>
  <ul class="collapse">
<li><a href="#a-1" id="toc-a-1" class="nav-link" data-scroll-target="#a-1"><span class="header-section-number">7.2.1</span> a</a></li>
  <li><a href="#b-1" id="toc-b-1" class="nav-link" data-scroll-target="#b-1"><span class="header-section-number">7.2.2</span> b</a></li>
  </ul>
</li>
  <li>
<a href="#section-2" id="toc-section-2" class="nav-link" data-scroll-target="#section-2"><span class="header-section-number">7.3</span> 2.3</a>
  <ul class="collapse">
<li><a href="#a-2" id="toc-a-2" class="nav-link" data-scroll-target="#a-2"><span class="header-section-number">7.3.1</span> a</a></li>
  <li><a href="#b-2" id="toc-b-2" class="nav-link" data-scroll-target="#b-2"><span class="header-section-number">7.3.2</span> b</a></li>
  <li><a href="#c-1" id="toc-c-1" class="nav-link" data-scroll-target="#c-1"><span class="header-section-number">7.3.3</span> c</a></li>
  </ul>
</li>
  <li>
<a href="#section-3" id="toc-section-3" class="nav-link" data-scroll-target="#section-3"><span class="header-section-number">7.4</span> 2.4</a>
  <ul class="collapse">
<li><a href="#a-3" id="toc-a-3" class="nav-link" data-scroll-target="#a-3"><span class="header-section-number">7.4.1</span> a</a></li>
  <li><a href="#b-3" id="toc-b-3" class="nav-link" data-scroll-target="#b-3"><span class="header-section-number">7.4.2</span> b</a></li>
  <li><a href="#c-2" id="toc-c-2" class="nav-link" data-scroll-target="#c-2"><span class="header-section-number">7.4.3</span> c</a></li>
  <li><a href="#d-1" id="toc-d-1" class="nav-link" data-scroll-target="#d-1"><span class="header-section-number">7.4.4</span> d</a></li>
  </ul>
</li>
  <li>
<a href="#section-4" id="toc-section-4" class="nav-link" data-scroll-target="#section-4"><span class="header-section-number">7.5</span> 2.5</a>
  <ul class="collapse">
<li><a href="#a-4" id="toc-a-4" class="nav-link" data-scroll-target="#a-4"><span class="header-section-number">7.5.1</span> a</a></li>
  <li><a href="#b-4" id="toc-b-4" class="nav-link" data-scroll-target="#b-4"><span class="header-section-number">7.5.2</span> b</a></li>
  <li><a href="#c-3" id="toc-c-3" class="nav-link" data-scroll-target="#c-3"><span class="header-section-number">7.5.3</span> c</a></li>
  <li><a href="#d-2" id="toc-d-2" class="nav-link" data-scroll-target="#d-2"><span class="header-section-number">7.5.4</span> d</a></li>
  </ul>
</li>
  <li><a href="#section-5" id="toc-section-5" class="nav-link" data-scroll-target="#section-5"><span class="header-section-number">7.6</span> 2.6</a></li>
  <li>
<a href="#section-6" id="toc-section-6" class="nav-link" data-scroll-target="#section-6"><span class="header-section-number">7.7</span> 2.7</a>
  <ul class="collapse">
<li><a href="#a-5" id="toc-a-5" class="nav-link" data-scroll-target="#a-5"><span class="header-section-number">7.7.1</span> a</a></li>
  <li><a href="#b-5" id="toc-b-5" class="nav-link" data-scroll-target="#b-5"><span class="header-section-number">7.7.2</span> b</a></li>
  </ul>
</li>
  <li>
<a href="#section-7" id="toc-section-7" class="nav-link" data-scroll-target="#section-7"><span class="header-section-number">7.8</span> 2.8</a>
  <ul class="collapse">
<li><a href="#a-6" id="toc-a-6" class="nav-link" data-scroll-target="#a-6"><span class="header-section-number">7.8.1</span> a</a></li>
  <li><a href="#b-6" id="toc-b-6" class="nav-link" data-scroll-target="#b-6"><span class="header-section-number">7.8.2</span> b</a></li>
  <li><a href="#c-4" id="toc-c-4" class="nav-link" data-scroll-target="#c-4"><span class="header-section-number">7.8.3</span> c</a></li>
  </ul>
</li>
  </ul>
</li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/d-morrison/hoff-bayesian-statistics/edit/main/2.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/d-morrison/hoff-bayesian-statistics/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/d-morrison/hoff-bayesian-statistics/blob/main/2.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Chapter 2: Belief, probability, and exchangeability</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Jesse Mu </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">Last modified: 2016-09-08: 0:00:00 (UTC)</p>
    </div>
  </div>
  
    
  </div>
  


</header><!-- Setup --><!-- Begin writing --><p>This section will be brief, as it is just a review of probability. However, as exchangeability and de Finetti’s theorem are important especially in Bayesian statistics, I’ll go into more depth there.</p>
<section id="belief-functions-and-properties" class="level1" data-number="1"><h1 data-number="1">
<span class="header-section-number">1</span> Belief functions and properties</h1>
<p>Let <span class="math inline">\(F, G,\)</span> and <span class="math inline">\(H\)</span> be events. A “belief” function <span class="math inline">\(\text{Be}(\cdot)\)</span> should correspond to certain intuitions about our beliefs about the likelihood of events. Probabilitiy functions have axioms that satisfy our notions of belief:</p>
<ol type="1">
<li>Contradictions and tautologies: <span class="math inline">\(0 = P(\text{not } H \mid H) \leq P(F \mid H) \leq P(H \mid H) = 1\)</span>
</li>
<li>Addition rule: <span class="math inline">\(P(F \cup G \mid H) = P(F \mid H) + P(G \mid H) \text{ if $F \cap G = \emptyset$}\)</span>
</li>
<li>Multiplication rule: <span class="math inline">\(P(F \cap G \mid H) = P(G \mid H) P(F \mid G \cap H)\)</span>
</li>
</ol>
<p>Note that the axioms of probability and theorems discussed in this section are the same whether you subscribe to a Bayesian or frequentist interpretation of probability.</p>
</section><section id="events-partitions-and-bayes-rule" class="level1" data-number="2"><h1 data-number="2">
<span class="header-section-number">2</span> Events, partitions, and Bayes’ rule</h1>
<p>Consider a set <span class="math inline">\(\mathcal{H}\)</span>, which is the “set of all possible truths.” We can partition <span class="math inline">\(\mathcal{H}\)</span> into discrete subsets <span class="math inline">\(\{H_1, \dots, H_k\}\)</span>, where only one subset consists of the truth.</p>
<p>We can assign probabilities whether each of these sets contains the truth. First, some event in <span class="math inline">\(\mathcal{H}\)</span> is true, so <span class="math inline">\(P(\mathcal{H}) = 1\)</span>. Let <span class="math inline">\(E\)</span> be some observation (in this case related to the truth of one <span class="math inline">\(H_i\)</span>). Then,</p>
<ul>
<li>Rule of total probability: <span class="math inline">\(\sum_i P(H_i) = 1\)</span>
</li>
<li>Marginal probability: <span class="math inline">\(P(E) = \sum_i P(E \cap H_i) = \sum_i P(E \mid H_i) P(H_i)\)</span>
<ul>
<li>The total probability of an event occurring is the sum of all of its probabilities under the possible partitions of truths</li>
</ul>
</li>
<li>Bayes’ rule: <span class="math display">\[\begin{align}
P(H_i \mid E) = \frac{\overbrace{P(E \mid H_i)}^{\text{likelihood}} \overbrace{P(H_i)}^{\text{prior}}}{P(E)}
\end{align}\]</span>
</li>
</ul></section><section id="independence" class="level1" data-number="3"><h1 data-number="3">
<span class="header-section-number">3</span> Independence</h1>
<p>Two events <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are <em>independent</em> if <span class="math inline">\(P(F \cap G) = P(F) P(G)\)</span>.</p>
<p>Two events <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are <em>conditionally independent</em> given <span class="math inline">\(H\)</span> if</p>
<p><span class="math display">\[\begin{align}
P(F \cap G \mid H) = P(F \mid H) P(G \mid H)
\end{align}\]</span></p>
<p>This implies <span class="math inline">\(P(F \mid H \cap G) = P(F \mid H)\)</span>. That is, if we know about <span class="math inline">\(H\)</span>, and <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are conditionally independent given <span class="math inline">\(H\)</span>, then knowing <span class="math inline">\(G\)</span> does not change our belief about <span class="math inline">\(H\)</span>. This is a key property leveraged in Bayesian networks.</p>
</section><section id="random-variables-and-their-distributions" class="level1" data-number="4"><h1 data-number="4">
<span class="header-section-number">4</span> Random variables and their distributions</h1>
<p>A random variable <span class="math inline">\(Y\)</span> is <em>discrete</em> if the set of all its possible values <span class="math inline">\(\mathcal{Y}\)</span> is countable, i.e.&nbsp;they can be enumerated <span class="math inline">\(\mathcal{Y} = \{y_1, y_2, \dots \}\)</span> Examples include the binomial and poisson distributions. Discrete random variables have a <em>probability mass function</em> (PMF) <span class="math inline">\(f(y) = P(Y = y)\)</span> which assigns a certain probability to every discrete point in its sample space. From this probability mass function, a <em>continuous distribution function</em> (CDF) is also defined:</p>
<p><span class="math display">\[F(y) = P(Y \leq y) = \sum_{y_i \leq y} f(y_i)\]</span></p>
<p><span class="math inline">\(Y\)</span> is <em>continuous</em> if <span class="math inline">\(\mathcal{Y}\)</span> can take <em>any</em> value in an interval. Therefore, the probability of <span class="math inline">\(Y\)</span> taking a single value in the sample space is 0. So instead we describe such distributions with <em>probability density functions</em> (PDFs) <span class="math inline">\(f(y)\)</span>, which must be integrated over an interval to obtain a probability: <span class="math inline">\(P(a \leq y \leq b) =
\int_a^b f(y) \; dy\)</span>. These variables also have CDFs:</p>
<p><span class="math display">\[\begin{align}
F(y) = P(Y \leq y) = \int_{-\infty}^y f(x) \; dx
\end{align}\]</span></p>
<p>Examples include the normal and beta distributions.</p>
<section id="descriptions-of-distributions" class="level2" data-number="4.1"><h2 data-number="4.1" class="anchored" data-anchor-id="descriptions-of-distributions">
<span class="header-section-number">4.1</span> Descriptions of distributions</h2>
<p>In the same way that we use the mean, mode, and median to describe samples, we can use them to describe distributions. Notice that for many distributions, these quantities are <em>not</em> the same.</p>
<p>We also use variance and quantiles to measure the <em>spread</em> of distributions.</p>
</section><section id="joint-distributions" class="level2" data-number="4.2"><h2 data-number="4.2" class="anchored" data-anchor-id="joint-distributions">
<span class="header-section-number">4.2</span> Joint distributions</h2>
<section id="discrete" class="level3" data-number="4.2.1"><h3 data-number="4.2.1" class="anchored" data-anchor-id="discrete">
<span class="header-section-number">4.2.1</span> Discrete</h3>
<p>Let <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> be random variables with sample spaces <span class="math inline">\(\mathcal{Y}_1\)</span> and <span class="math inline">\(\mathcal{Y}_2\)</span>. Then the joint pdf/density of <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> is defined as:</p>
<p><span class="math display">\[
p(y_1, y_2) = P(\{Y_1 = y_1\} \cap \{Y_2 = y_2 \})
\]</span></p>
<p>and the <em>marginal density</em> of <span class="math inline">\(Y_1\)</span> is obtained by summing over all possible values of <span class="math inline">\(Y_2\)</span>:</p>
<p><span class="math display">\[\begin{align}
p(y_1) &amp;= \sum_{y_2 \in \mathcal{Y}_2} p(y_1, y_2) \\
       &amp;= \sum_{y_2 \in \mathcal{Y}_2} p(y_1 \mid y_2) p(y_2).
\end{align}\]</span></p>
<p>The <em>conditional density</em> is</p>
<p><span class="math display">\[\begin{align}
p(y_2 \mid y_1) = \frac{p(y_1, y_2)}{p(y_1)}
\end{align}\]</span></p>
<p>Notice that given the joint density <span class="math inline">\(p(y_1, y_2)\)</span>, we can calculate marginal and conditional densities <span class="math inline">\(\{ p(y_1), p(y_2), p(y_1 \mid y_2), p(y_2 \mid y_1) \}\)</span> by simply summing up the relevant variables. Additionally, given <span class="math inline">\(p(y_1)\)</span> and <span class="math inline">\(p(y_2 \mid y_1)\)</span>, (or the reverse), we can reconstruct the joint distribution. However, given only marginal densities <span class="math inline">\(p(y_1)\)</span> and <span class="math inline">\(p(y_2)\)</span>, we can’t reconstruct the joint distribution, since we don’t know whether the events are independent.</p>
</section><section id="continuous" class="level3" data-number="4.2.2"><h3 data-number="4.2.2" class="anchored" data-anchor-id="continuous">
<span class="header-section-number">4.2.2</span> Continuous</h3>
<p>In the continuous case, the probability density function is a function of <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> such that the CDF is</p>
<p><span class="math display">\[
F(y_1, y_2) = \int_{-\infty}^{y_1} \int_{-\infty}^{y_2} p(y_1, y_2) \; dy_2 \; dy_1.
\]</span></p>
<p>Obtaining the marginal densities can be done by integrating out the irrelevant variable:</p>
<p><span class="math display">\[\begin{align}
p(y_1) &amp;= \int_{-\infty}^{\infty} p(y_1, y_2) \; dy_2 \\
p(y_2) &amp;= \int_{-\infty}^{\infty} p(y_1, y_2) \; dy_1 \\
\end{align}\]</span></p>
<p>With the marginal densities, you can compute the conditional densities <span class="math inline">\(p(y_2
\mid y_1) = p(y_1, y_2) / p(y_1)\)</span>, etc.</p>
</section></section><section id="independent-random-variables" class="level2" data-number="4.3"><h2 data-number="4.3" class="anchored" data-anchor-id="independent-random-variables">
<span class="header-section-number">4.3</span> Independent random variables</h2>
<p>Let <span class="math inline">\(Y_1, \dots, Y_n\)</span> be random variables dependent on a common parameter <span class="math inline">\(\theta\)</span>. Then <span class="math inline">\(Y_1 \dots, Y_n\)</span> are conditionally independent given <span class="math inline">\(\theta\)</span> if</p>
<p><span class="math display">\[\begin{align}
p(y_1, \dots, y_n \mid \theta) = p(y_1 \mid \theta) \times \dots \times p(y_n \mid \theta).
\end{align}\]</span></p>
<p>Note this extends naturally from the definition of independent of two random variables, <span class="math inline">\(P(A \cap B) = P(A) P(B)\)</span>. Thus, knowing about any <span class="math inline">\(Y_i\)</span> does not give any information about the other <span class="math inline">\(Y_j\)</span>. Lastly, the joint density of these variables can be defined as</p>
<p><span class="math display">\[
p(y_1, \dots, y_n \mid \theta) = \prod_i p(y_i \mid \theta).
\]</span></p>
<p>We say that <span class="math inline">\(Y_1, \dots, Y_n\)</span> are conditionallly independent and identically distributed (i.i.d.):</p>
<p><span class="math display">\[
Y_1, \dots, Y_n \mid \theta \sim \text{i.i.d.} \; p(y \mid \theta).
\]</span></p>
</section></section><section id="exchangeability" class="level1" data-number="5"><h1 data-number="5">
<span class="header-section-number">5</span> Exchangeability</h1>
<p>In many situations with several random variables, we would intuit that the specific order of observation of these random variables aren’t important. For example, consider a random sample of 3 participants from an infinite population which may or may not have a property (1 or 0) It makes sense that</p>
<p><span class="math display">\[p(0, 0, 1) = p(1, 0, 0) = p(0, 1, 0).\]</span></p>
<p>since the likelihood of a person having the property or not is <span class="math inline">\(\theta\)</span>, regardless of the sample. This property is called exchangeability.</p>
<blockquote class="blockquote">
<p>Exchangability. Let <span class="math inline">\(Y_1, \dots, Y_n\)</span> be random variables. If <span class="math inline">\(p(y_1, \dots,
y_n) = p(y_{\pi_1}, \dots, y_{\pi_n})\)</span> for all permutations <span class="math inline">\(\pi\)</span> of <span class="math inline">\(\{1, \dots, n\}\)</span>, then <span class="math inline">\(Y_1, \dots, Y_n\)</span> are exchangable.</p>
</blockquote>
<p>If <span class="math inline">\(Y_1, \dots Y_n\)</span> are i.i.d., then they are exchangeable:</p>
<p><span class="math display">\[\begin{align}
p(y_1, \dots, y_n) &amp;= \int p(y_1, \dots, y_n \mid \theta)\; d\theta &amp; \\
&amp;= \int \left( \prod_i p(y_i \mid \theta) \right) p(\theta) \; d\theta &amp; \text{(i.i.d.)} \\
&amp;= \int \left( \prod_i p(y_{\pi_i} \mid \theta) \right) p(\theta) \; d\theta &amp; \text{(order of product doesn't matter)} \\
&amp;= p(y_{\pi_1}, \dots, y_{\pi_n}).
\end{align}\]</span></p>
<p>Classical assumption of Bernoulli variables <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> as outcomes of the same experiment (e.g.&nbsp;a coin flip): <em>independence</em>. But continuing to observe <span class="math inline">\(X_j\)</span>s should result in a change of opinion about the distribution of coin flip outcomes (e.g.&nbsp;gradually learning coin bias). So Bayesian statisticians should assume <em>exchangeability</em>, a weaker condition than <em>independence</em>.</p>
</section><section id="de-finettis-theorem" class="level1" data-number="6"><h1 data-number="6">
<span class="header-section-number">6</span> de Finetti’s theorem</h1>
<blockquote class="blockquote">
<p>de Finetti’s theorem. Let <span class="math inline">\(Y_1, \dots, Y_n\)</span> be a finite subset of an infinitely exchangeable but <strong>not necessarily i.i.d.</strong> random variables: <span class="math display">\[p(y_1, \dots, y_n) = p(y_{\pi_1}, \dots, y_{\pi_n})\]</span> for all partitions <span class="math inline">\(\pi\)</span>. Then our model can be written as <span class="math display">\[p(y_1, \dots, y_n) = \int \left( \prod_1^n p(y_i \mid \theta) \right) p(\theta) \; d\theta.\]</span></p>
</blockquote>
<p>So, in general, <span class="math display">\[Y_1, \dots, Y_n \mid \theta \text{ are i.i.d.} \Leftrightarrow Y_1, \dots, Y_n \text{ are exchangable for all $n$}\]</span></p>
<p>Importantly, if we sample from a sufficiently large population, then we can model the sample variables as being approximately conditionally i.i.d.</p>
<section id="proof-for-0-1-random-variables" class="level2" data-number="6.1"><h2 data-number="6.1" class="anchored" data-anchor-id="proof-for-0-1-random-variables">
<span class="header-section-number">6.1</span> Proof for {0, 1} random variables</h2>
<p>From Heath &amp; Sudderth (1976). De Finetti’s Theorem on Exchangeable Variables. Also see this <a href="http://stats.stackexchange.com/questions/34465/what-is-so-cool-about-de-finettis-representation-theorem">neat StackExchange answer</a>.</p>
<section id="alternative-statement-of-theorem" class="level3" data-number="6.1.1"><h3 data-number="6.1.1" class="anchored" data-anchor-id="alternative-statement-of-theorem">
<span class="header-section-number">6.1.1</span> Alternative statement of theorem</h3>
<p>For every infinite sequence of exchangeable random variables <span class="math inline">\((X_n)\)</span> having values in <span class="math inline">\(\{0, 1\}\)</span> and a finite subsequence <span class="math inline">\(X_1, \dots, X_n\)</span>, there is a probability distribution <span class="math inline">\(F\)</span> such that</p>
<p><span class="math display">\[\begin{align}
P(X_1 = 1, \dots, X_k = 1, X_{k+1} = 0, \dots, X_n = 0) &amp;=  \\
P\left(\underbrace{1, \dots, 1}_{\text{$k$ times}}, \underbrace{0, \dots, 0}_{\text{$n - k$ times}}\right) &amp;= \int_{0, 1} \theta^k (1 - \theta)^{n-k} F(d \theta)
\end{align}\]</span></p>
<p>Where <span class="math inline">\(F\)</span> is the prior over <span class="math inline">\(\Theta\)</span>, i.e.&nbsp;the values that <span class="math inline">\(\theta\)</span> can take.</p>
</section><section id="proof" class="level3" data-number="6.1.2"><h3 data-number="6.1.2" class="anchored" data-anchor-id="proof">
<span class="header-section-number">6.1.2</span> Proof</h3>
<p><em>Lemma</em>. Let <span class="math inline">\(p_{k, n} = P(X_1 = 1, \dots, X_k = 1, X_{k + 1} = 0, \dots, X_n =
0)\)</span>, the quantity on the left side of the above equation, and suppose <span class="math inline">\(X_1,
\dots, X_m\)</span> are exchangable Boolean random variables as before.</p>
<p>Let <span class="math inline">\(q_r = P\left( \sum_{j
= 1}^m X_j = r \right)\)</span>, that is, <span class="math inline">\(q_i\)</span> is the probability that exactly <span class="math inline">\(i\)</span> of the <span class="math inline">\(m\)</span> variables are 1.</p>
<p>Also denote <span class="math inline">\((x)_k = \prod_{j = 0}^{k - 1} (x - j) = \frac{x!}{(x - k)!}\)</span>.</p>
<p>Then,</p>
<p><span class="math display">\[\begin{align}
p_{k, n} = \sum_{r = 0}^{m} \frac{(r)_k (m - r)_{n - k}}{(m)_n} q_r \text{ for } 0 \leq k \leq n \leq m
\end{align}\]</span></p>
<p>where <span class="math inline">\(\frac{(r)_k (m - r)_{n - k}}{(m)_n}\)</span> is some complicated combinatorics term for the number of ways to that <span class="math inline">\(r\)</span> ones and <span class="math inline">\(m - r\)</span> zeros can be placed in a sequence of length <span class="math inline">\(m\)</span>.</p>
<p>With the sustitution <span class="math inline">\(\frac{r}{m} = \theta\)</span>, we can re-package the summation of discrete <span class="math inline">\(r\)</span> as an integral over a stepwise function <span class="math inline">\(F_m\)</span> with domain <span class="math inline">\([0, 1]\)</span> that jumps <span class="math inline">\(q_r\)</span> at every interval <span class="math inline">\(0 \leq r / m \leq 1\)</span>:</p>
<p><span class="math display">\[
p_{k, n} = \int_{0}^{1} \frac{(\theta m)_k ((1 - \theta)m)_{n - k}}{(m)_n} F_m(d\theta).
\]</span></p>
<p>As <span class="math inline">\(m\)</span> goes to infinity, the steps of the stepwise function grow infinitely small, such that the function converges to the continuous probability distribution <span class="math inline">\(F\)</span>. By Helly’s theorem and some hand-waving in this explanation, the components of the integrand converge to the exponential terms in the theorem: <span class="math inline">\(\theta^k (1 - \theta)^{n - k}\)</span>.</p>
</section></section></section><section id="exercises" class="level1" data-number="7"><h1 data-number="7">
<span class="header-section-number">7</span> Exercises</h1>
<section id="section" class="level2" data-number="7.1"><h2 data-number="7.1" class="anchored" data-anchor-id="section">
<span class="header-section-number">7.1</span> 2.1</h2>
<section id="a" class="level3" data-number="7.1.1"><h3 data-number="7.1.1" class="anchored" data-anchor-id="a">
<span class="header-section-number">7.1.1</span> a</h3>
<p>The marginal probability distribution of a father’s occupation is found by summing over the rows of the son’s occupation. Formally, let <span class="math inline">\(F\)</span> be the father’s occupation and <span class="math inline">\(S\)</span> be the son’s. Then <span class="math inline">\(P(F = f) = \sum P(F = f, S = s)\)</span>.</p>
<ul>
<li>Farm: 0.11</li>
<li>Operatives: 0.279</li>
<li>etc…</li>
</ul></section><section id="b" class="level3" data-number="7.1.2"><h3 data-number="7.1.2" class="anchored" data-anchor-id="b">
<span class="header-section-number">7.1.2</span> b</h3>
<p>This time sum over columns.</p>
</section><section id="c" class="level3" data-number="7.1.3"><h3 data-number="7.1.3" class="anchored" data-anchor-id="c">
<span class="header-section-number">7.1.3</span> c</h3>
<p>In the row of the table where the father’s occupation is “farm”, normalize the values of the son’s occupation such that they sum to one. This can be done by dividing each value by the sum of the row. Formally, this is calculating</p>
<p><span class="math display">\[\begin{align}
P(S = s \mid F = \text{farm}) &amp;= \frac{P(S = s, F = \text{farm})}{P(F = \text{farm})} \\
&amp;= \frac{P(S = s, F = \text{farm})}{\sum_{s' \in S} P(S = s', F = \text{farm})}.
\end{align}\]</span></p>
</section><section id="d" class="level3" data-number="7.1.4"><h3 data-number="7.1.4" class="anchored" data-anchor-id="d">
<span class="header-section-number">7.1.4</span> d</h3>
<p>Normalize the column where the son’s occupation is “farm” like above.</p>
</section></section><section id="section-1" class="level2" data-number="7.2"><h2 data-number="7.2" class="anchored" data-anchor-id="section-1">
<span class="header-section-number">7.2</span> 2.2</h2>
<section id="a-1" class="level3" data-number="7.2.1"><h3 data-number="7.2.1" class="anchored" data-anchor-id="a-1">
<span class="header-section-number">7.2.1</span> a</h3>
<p>Since expectation is linear, <span class="math display">\[\begin{align}
\mathbb{E}(a_1 Y_1 + a_2 Y_2) &amp;= a_1\mathbb{E}(Y_1) + a_2\mathbb{E}(Y_2) \\
&amp;= a_1 \mu_1 + a_2 \mu_2
\end{align}\]</span></p>
<p>Since <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are independent, <span class="math inline">\(\text{Cov}(Y_1, Y_2) = 0\)</span>. When adding variances, it’s necessary to combine terms like below: <span class="math display">\[\begin{align}
\text{Var}(a_1 Y_1 + a_2 Y_2) &amp;= a^2\text{Var}(Y_1) + b^2\text{Var}(Y_2) + 2ab\text{Cov}(Y_1, Y_2) \\
&amp;= a^2\sigma_1^2 + b^2\sigma_2^2
\end{align}\]</span></p>
</section><section id="b-1" class="level3" data-number="7.2.2"><h3 data-number="7.2.2" class="anchored" data-anchor-id="b-1">
<span class="header-section-number">7.2.2</span> b</h3>
<p><span class="math display">\[\begin{align}
\mathbb{E}(a_1 Y_1 - a_2 Y_2) &amp;= a_1\mathbb{E}(Y_1) - a_2\mathbb{E}(Y_2) \\
&amp;= a_1 \mu_1 - a_2 \mu_2
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\text{Var}(a_1 Y_1 - a_2 Y_2) &amp;= a^2\text{Var}(Y_1) + b^2\text{Var}(Y_2) - 2ab\text{Cov}(Y_1, Y_2) \\
&amp;= a^2\sigma_1^2 + b^2\sigma_2^2 \\
&amp;= \text{Var}(a_1 Y_1 + a_2 Y_2)
\end{align}\]</span></p>
</section></section><section id="section-2" class="level2" data-number="7.3"><h2 data-number="7.3" class="anchored" data-anchor-id="section-2">
<span class="header-section-number">7.3</span> 2.3</h2>
<p>Let <span class="math inline">\(X, Y, Z\)</span> be random variables with joint density <span class="math inline">\(p(x, y, z) \propto f(x, z) g(y, z) h(z)\)</span>.</p>
<section id="a-2" class="level3" data-number="7.3.1"><h3 data-number="7.3.1" class="anchored" data-anchor-id="a-2">
<span class="header-section-number">7.3.1</span> a</h3>
<p><span class="math display">\[\begin{align}
p(x \mid y, z) &amp;= \frac{p(x, y, z)}{p(y, z)} \\
&amp;= \frac{p(x, y, z)}{\int p(x, y, z) \; dx} \\
&amp;\propto \frac{f(x, z) g(y, z) h(z)}{\int f(x, z) g(y, z) h(z) \; dx} \\
&amp;\propto \frac{f(x, z) g(y, z) h(z)}{g(y, z) h(z) \int f(x, z) \; dx} \\
&amp;\propto \frac{f(x, z)}{\int f(x, z) \; dx}
\end{align}\]</span></p>
</section><section id="b-2" class="level3" data-number="7.3.2"><h3 data-number="7.3.2" class="anchored" data-anchor-id="b-2">
<span class="header-section-number">7.3.2</span> b</h3>
<p><span class="math display">\[\begin{align}
p(y \mid x, z) &amp;= \frac{p(x, y, z)}{p(x, z)} \\
&amp;\propto \frac{f(x, z) g(y, z) h(z)}{\int p(x, y, z) \; dy} \\
&amp;\propto \frac{f(x, z) g(y, z) h(z)}{\int f(x, z) g(y, z) h(z) \; dy} \\
&amp;\propto \frac{f(x, z) g(y, z) h(z)}{f(x, z) h(z) \int g(y, z) \; dy}
&amp;\propto \frac{g(y, z)}{\int g(y, z) \; dy}
\end{align}\]</span></p>
</section><section id="c-1" class="level3" data-number="7.3.3"><h3 data-number="7.3.3" class="anchored" data-anchor-id="c-1">
<span class="header-section-number">7.3.3</span> c</h3>
<p>It is sufficient to show that <span class="math inline">\(p(x \mid y, z) = p(x \mid z)\)</span>:</p>
<p>From (a), <span class="math inline">\(p(x \mid y, z) \propto \frac{f(x, z)}{\int f(x, z) \; dx}\)</span>. Now</p>
<p><span class="math display">\[\begin{align}
p(x \mid z) &amp;= \frac{p(x, z)}{p(z)} \\
&amp;\propto \frac{\int p(x, y, z) \; dy}{\int \int p(x, y, z) \; dy \; dx} \\
&amp;\propto \frac{\int f(x, z) g(y, z) h(z) \; dy}{\int \int f(x, z) g(y, z) h(z) \; dy \; dx} \\
&amp;\propto \frac{f(x, z) h(z) \int g(y, z) \; dy}{h(z) \left( \int g(y, z) \; dy \right) \left( \int f(x, z) \; dx \right)} \\
&amp;\propto \frac{f(x, z)}{\int f(x, z) \; dx} \\
&amp;\propto p(x \mid y, z)
\end{align}\]</span></p>
<p>Now, since <span class="math inline">\(p(x \mid z) \propto p(x \mid y, z)\)</span> but additionally <span class="math inline">\(\int p(x \mid
z) = \int p(x \mid y, z) = 1\)</span>, <span class="math inline">\(p(x \mid z) = p(x \mid y, z)\)</span>, so <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are conditionally independent given <span class="math inline">\(z\)</span>.</p>
</section></section><section id="section-3" class="level2" data-number="7.4"><h2 data-number="7.4" class="anchored" data-anchor-id="section-3">
<span class="header-section-number">7.4</span> 2.4</h2>
<section id="a-3" class="level3" data-number="7.4.1"><h3 data-number="7.4.1" class="anchored" data-anchor-id="a-3">
<span class="header-section-number">7.4.1</span> a</h3>
<p>Use <strong>P3</strong> and condition on an always true event:</p>
<p><span class="math display">\[\begin{align}
P(H_j \cap E \mid 1 = 1) &amp;= P(H_j \mid 1 = 1) P(E \mid H_j \cap 1 = 1) = P(H_j)P(E \mid H_j) \\
P(H_j \cap E \mid 1 = 1) &amp;= P(E \mid 1 = 1) P(H_j \mid E \cap 1 = 1) = p(E)p(H_j \mid E) \\
&amp;= P(H_j) P(E \mid H_j)
\end{align}\]</span></p>
</section><section id="b-3" class="level3" data-number="7.4.2"><h3 data-number="7.4.2" class="anchored" data-anchor-id="b-3">
<span class="header-section-number">7.4.2</span> b</h3>
<p>If we can assume that <span class="math inline">\(p(\mathcal{H}) = 1\)</span>, then</p>
<p><span class="math display">\[\begin{align}
P(E) &amp;= P(E \cap H) \\
&amp;= P(E \cap (H_1 \cup H_2 \cup \dots \cup H_k)) \\
&amp;= P((E \cap (H_1)) \cup (E \cap (H_2 \cup \dots \cup H_k))) &amp; \text{Distributing} \\
&amp;= P(E \cap H_1) + P(E \cap (H_2 \cup \dots \cup H_k)) &amp; \text{P2; Implied condition on a true event}
\end{align}\]</span></p>
</section><section id="c-2" class="level3" data-number="7.4.3"><h3 data-number="7.4.3" class="anchored" data-anchor-id="c-2">
<span class="header-section-number">7.4.3</span> c</h3>
<p>Proceed inductively from b.</p>
</section><section id="d-1" class="level3" data-number="7.4.4"><h3 data-number="7.4.4" class="anchored" data-anchor-id="d-1">
<span class="header-section-number">7.4.4</span> d</h3>
<p><span class="math display">\[\begin{align}
&amp; P(H_j)P(E \mid H_j) = P(E)P(H_j \mid E) \\
\implies&amp; P(H_j \mid E) = \frac{P(H_j)P(E \mid H_j)}{P(E)} \\
\implies&amp; P(H_j \mid E) = \frac{P(H_j)P(E \mid H_j)}{\sum_k P(E \cap H_k)} &amp; \text{From c} \\
\implies&amp; P(H_j \mid E) = \frac{P(H_j)P(E \mid H_j)}{\sum_k P(E \mid H_k) P(H_k)} &amp; \text{P3} \\
\end{align}\]</span></p>
</section></section><section id="section-4" class="level2" data-number="7.5"><h2 data-number="7.5" class="anchored" data-anchor-id="section-4">
<span class="header-section-number">7.5</span> 2.5</h2>
<section id="a-4" class="level3" data-number="7.5.1"><h3 data-number="7.5.1" class="anchored" data-anchor-id="a-4">
<span class="header-section-number">7.5.1</span> a</h3>
<div class="cell" data-layout-align="center">
<details class="code-fold"><summary>Show R code</summary><div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">x</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span></span>
<span>  <span class="co"># Y = 0 Y = 1</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.5</span> <span class="op">*</span> <span class="fl">.4</span>, <span class="fl">.5</span> <span class="op">*</span> <span class="fl">.6</span><span class="op">)</span>, <span class="co"># X = 0</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.5</span> <span class="op">*</span> <span class="fl">.6</span>, <span class="fl">.5</span> <span class="op">*</span> <span class="fl">.4</span><span class="op">)</span> <span class="co"># X = 1</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"X = 0"</span>, <span class="st">"X = 1"</span><span class="op">)</span></span>
<span><span class="fu">kable</span><span class="op">(</span><span class="va">x</span>, col.names <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Y = 0"</span>, <span class="st">"Y = 1"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details><div class="cell-output-display">
<table class="caption-top table table-sm table-striped small">
<thead><tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">Y = 0</th>
<th style="text-align: right;">Y = 1</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">X = 0</td>
<td style="text-align: right;">0.2</td>
<td style="text-align: right;">0.3</td>
</tr>
<tr class="even">
<td style="text-align: left;">X = 1</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">0.2</td>
</tr>
</tbody>
</table>
</div>
</div>
</section><section id="b-4" class="level3" data-number="7.5.2"><h3 data-number="7.5.2" class="anchored" data-anchor-id="b-4">
<span class="header-section-number">7.5.2</span> b</h3>
<p><span class="math display">\[\mathbb{E}(Y) = 0.5\]</span>. Probability the ball is green is <span class="math inline">\(0.5\)</span>.</p>
</section><section id="c-3" class="level3" data-number="7.5.3"><h3 data-number="7.5.3" class="anchored" data-anchor-id="c-3">
<span class="header-section-number">7.5.3</span> c</h3>
<p><span class="math display">\[\begin{align}
\text{Var}(Y \mid X = 0) &amp;= \mathbb{E}((Y \mid X = 0)^2) - \mathbb{E}(Y \mid X = 0)^2 \\
&amp;= 1^2 p(Y = 1 \mid X = 0) + 0^2 p(Y = 0 \mid X = 0) - \\ &amp; \quad (1 p(Y = 1 \mid X = 0) + 0 p(Y = 0 \mid X = 0))^2 \\
&amp;= .6 - (.6)^2 \\
&amp;= 0.24 \\
&amp;= \text{Var}(Y \mid X = 1) &amp; \text{Not going to do this one}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\text{Var}(Y) &amp;= \mathbb{E}(Y^2) - \mathbb{E}(Y)^2 \\
&amp;= 1^2 p(Y = 1) + 0^2 p(Y = 0) - (0.5)^2 \\
&amp;= 0.5 - (0.5)^2 = 0.25
\end{align}\]</span></p>
<p><span class="math inline">\(\text{Var}(Y)\)</span> is slightly larger since we are more uncertain about the value of <span class="math inline">\(Y\)</span> when we have not yet flipped the coin. Knowing which urn we will draw from clarifies our probabilites of obtaining a green or red ball.</p>
</section><section id="d-2" class="level3" data-number="7.5.4"><h3 data-number="7.5.4" class="anchored" data-anchor-id="d-2">
<span class="header-section-number">7.5.4</span> d</h3>
<p><span class="math display">\[\begin{align}
P(X = 0 \mid Y = 1) &amp;= .6
\end{align}\]</span></p>
</section></section><section id="section-5" class="level2" data-number="7.6"><h2 data-number="7.6" class="anchored" data-anchor-id="section-5">
<span class="header-section-number">7.6</span> 2.6</h2>
<p>If <span class="math inline">\(A \perp B \mid C\)</span> then</p>
<p><span class="math display">\[\begin{align}
P(A, B \mid C) &amp;= P(A \mid C) P(B \mid C) \\
  &amp;= (1 - P(A^c \mid C))P(B \mid C) \\
  &amp;= P(B \mid C) - P(B \mid C)P(A^c \mid C)
\end{align}\]</span></p>
<p>so <span class="math display">\[P(B \mid C)P(A^c \mid C) = P(B \mid C) - P(A, B \mid C)\]</span>.</p>
<p>Also notice <span class="math display">\[\begin{align}
&amp; P(B \mid C) = P(A, B \mid C) + P(A^c, B \mid C) &amp; \text{LTP} \\
\implies &amp; P(A^c, B \mid C) = P(B \mid C) - P(A, B \mid C)
\end{align}\]</span></p>
<p>Equating the two equations above, we have <span class="math display">\[
P(A^c, B \mid C) = P(B \mid C) P(A^c \mid C)
\]</span></p>
<p>We do this similarly for the other events.</p>
<p>For a case where <span class="math inline">\(A \perp B \mid C\)</span> holds but <span class="math inline">\(A \perp B \mid C^c\)</span> does not, consider a Bayesian network where knowing <span class="math inline">\(C\)</span> results in 100% belief in the values of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> (e.g.&nbsp;<span class="math inline">\(P(A, B \mid C) = 1\)</span>), but absent of <span class="math inline">\(C\)</span>, values <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> have some probability of manifesting that affect each other.</p>
</section><section id="section-6" class="level2" data-number="7.7"><h2 data-number="7.7" class="anchored" data-anchor-id="section-6">
<span class="header-section-number">7.7</span> 2.7</h2>
<section id="a-5" class="level3" data-number="7.7.1"><h3 data-number="7.7.1" class="anchored" data-anchor-id="a-5">
<span class="header-section-number">7.7.1</span> a</h3>
<p>No matter how likely the event, the maximum money you will obtain is $1. Thus no rational person would give more than $1 for the possibility of obtaining $1, as such an action would always result in lost money.</p>
</section><section id="b-5" class="level3" data-number="7.7.2"><h3 data-number="7.7.2" class="anchored" data-anchor-id="b-5">
<span class="header-section-number">7.7.2</span> b</h3>
<p>Since either <span class="math inline">\(E\)</span> or <span class="math inline">\(E^c\)</span> will definitively happen, we would want someone who is betting on the occurrences of either <span class="math inline">\(E\)</span> or <span class="math inline">\(E^c\)</span> to even out.</p>
</section></section><section id="section-7" class="level2" data-number="7.8"><h2 data-number="7.8" class="anchored" data-anchor-id="section-7">
<span class="header-section-number">7.8</span> 2.8</h2>
<section id="a-6" class="level3" data-number="7.8.1"><h3 data-number="7.8.1" class="anchored" data-anchor-id="a-6">
<span class="header-section-number">7.8.1</span> a</h3>
<section id="i" class="level4"><h4 class="anchored" data-anchor-id="i">i</h4>
<p>A frequentist idea: if I were to pick at random many, many <span class="math inline">\(x\)</span> sampled from the census roll, how many of them would be Hindu?</p>
<p>A subjectivist interpretation: how strongly do I believe that a random person from this census roll is Hindu?</p>
<p>(Should be <span class="math inline">\(0.15\)</span>)</p>
</section><section id="ii" class="level4"><h4 class="anchored" data-anchor-id="ii">ii</h4>
<p>If I were to pick at random many, many <span class="math inline">\(x\)</span> from the census roll, in the long run average, what proportion of them would be <span class="math inline">\(6452859\)</span>?</p>
<p>How strongly do I believe that this <span class="math inline">\(x\)</span> I am going to sample is <span class="math inline">\(6452859\)</span>?</p>
</section><section id="iii" class="level4"><h4 class="anchored" data-anchor-id="iii">iii</h4>
<p><strong>An interesting one</strong></p>
<p>Assume person <span class="math inline">\(6452589\)</span> is a random person sampled from Sri Lanka that is either Hindu or not. In the long run, if I observed many persons <span class="math inline">\(6452589\)</span>, how many of them would be Hindu?</p>
<p>Person <span class="math inline">\(6452589\)</span> is <em>one</em> person who may or may not be Hindu. How strongly do I believe that he/she is Hindu?</p>
</section></section><section id="b-6" class="level3" data-number="7.8.2"><h3 data-number="7.8.2" class="anchored" data-anchor-id="b-6">
<span class="header-section-number">7.8.2</span> b</h3>
<section id="i-1" class="level4"><h4 class="anchored" data-anchor-id="i-1">i</h4>
</section><section id="ii-1" class="level4"><h4 class="anchored" data-anchor-id="ii-1">ii</h4>
</section><section id="iii-1" class="level4"><h4 class="anchored" data-anchor-id="iii-1">iii</h4>
<p>Skipping</p>
</section></section><section id="c-4" class="level3" data-number="7.8.3"><h3 data-number="7.8.3" class="anchored" data-anchor-id="c-4">
<span class="header-section-number">7.8.3</span> c</h3>


<!-- -->

</section></section></section><a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> 'Chapter 2: Belief, probability, and exchangeability'</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Jesse Mu"</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "September 8, 2016"</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Setup --&gt;</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="in">```{r echo=FALSE, message=FALSE}</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span>opts_chunk<span class="sc">$</span><span class="fu">set</span>(<span class="at">fig.align =</span> <span class="st">'center'</span>, <span class="at">message =</span> <span class="cn">FALSE</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(knitr)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cowplot)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(reshape)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Begin writing --&gt;</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>This section will be brief, as it is just a review of probability. However, as</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>exchangeability and de Finetti's theorem are important especially in Bayesian</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>statistics, I'll go into more depth there.</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="fu"># Belief functions and properties</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>Let $F, G,$ and $H$ be events. A "belief" function $\text{Be}(\cdot)$ should</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>correspond to certain intuitions about our beliefs about the likelihood of</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>events. Probabilitiy functions have axioms that satisfy our notions of belief:</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Contradictions and tautologies: $0 = P(\text{not } H \mid H) \leq P(F \mid H) \leq P(H \mid H) = 1$</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Addition rule: $P(F \cup G \mid H) = P(F \mid H) + P(G \mid H) \text{ if $F \cap G = \emptyset$}$</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Multiplication rule: $P(F \cap G \mid H) = P(G \mid H) P(F \mid G \cap H)$</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>Note that the axioms of probability and theorems discussed in this section are</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>the same whether you subscribe to a Bayesian or frequentist interpretation of</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>probability.</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="fu"># Events, partitions, and Bayes' rule</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>Consider a set $\mathcal{H}$, which is the "set of all possible truths." We can </span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>partition $\mathcal{H}$ into discrete subsets $<span class="sc">\{</span>H_1, \dots, H_k<span class="sc">\}</span>$,</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>where only one subset consists of the truth.</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>We can assign probabilities whether each of these sets contains the truth. </span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>First, some event in $\mathcal{H}$ is true, so $P(\mathcal{H}) = 1$. Let $E$ be </span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>some observation (in this case related to the truth of one $H_i$). Then,</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Rule of total probability: $\sum_i P(H_i) = 1$</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Marginal probability: $P(E) = \sum_i P(E \cap H_i) = \sum_i P(E \mid H_i) P(H_i)$</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The total probability of an event occurring is the sum of all of its</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>  probabilities under the possible partitions of truths</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bayes' rule: \begin{align}</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>P(H_i \mid E) = \frac{\overbrace{P(E \mid H_i)}^{\text{likelihood}} \overbrace{P(H_i)}^{\text{prior}}}{P(E)}</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a><span class="fu"># Independence</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>Two events $F$ and $G$ are *independent* if $P(F \cap G) = P(F) P(G)$.</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>Two events $F$ and $G$ are *conditionally independent* given $H$ if</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>P(F \cap G \mid H) = P(F \mid H) P(G \mid H)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>This implies $P(F \mid H \cap G) = P(F \mid H)$. That is, if we know about $H$,</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>and $F$ and $G$ are conditionally independent given $H$, then knowing $G$ does</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>not change our belief about $H$. This is a key property leveraged in Bayesian</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>networks.</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a><span class="fu"># Random variables and their distributions</span></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>A random variable $Y$ is *discrete* if the set of all its possible values </span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>$\mathcal{Y}$ is countable, i.e. they can be enumerated   $\mathcal{Y} = <span class="sc">\{</span>y_1, y_2, \dots <span class="sc">\}</span>$ Examples include the binomial and poisson </span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>distributions. Discrete random variables have a *probability mass function*</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>(PMF) $f(y) = P(Y = y)$ which assigns a certain probability to every discrete point in its sample</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>space. From this probability mass function, a *continuous distribution function*</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>(CDF) is also defined:</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>$$F(y) = P(Y \leq y) = \sum_{y_i \leq y} f(y_i)$$</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>$Y$ is *continuous* if $\mathcal{Y}$ can take *any* value in an interval. Therefore, the probability</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>of $Y$ taking a single value in the sample space is 0. So instead we describe</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>such distributions with *probability density functions* (PDFs) $f(y)$, which must be</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>integrated over an interval to obtain a probability: $P(a \leq y \leq b) =</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>\int_a^b f(y) \; dy$. These variables also have CDFs:</span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>F(y) = P(Y \leq y) = \int_{-\infty}^y f(x) \; dx</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a>Examples include the normal and beta distributions.</span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a><span class="fu">## Descriptions of distributions</span></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>In the same way that we use the mean, mode, and median to describe samples, we </span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>can use them to describe distributions. Notice that for many distributions, </span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>these quantities are *not* the same.</span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a>We also use variance and quantiles to measure the *spread* of distributions.</span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a><span class="fu">## Joint distributions</span></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a><span class="fu">### Discrete</span></span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>Let $Y_1$ and $Y_2$ be random variables with sample spaces $\mathcal{Y}_1$ and </span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>$\mathcal{Y}_2$. Then the joint pdf/density of $Y_1$ and $Y_2$ is defined as:</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a>p(y_1, y_2) = P(<span class="sc">\{</span>Y_1 = y_1<span class="sc">\}</span> \cap <span class="sc">\{</span>Y_2 = y_2 <span class="sc">\}</span>)</span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>and the *marginal density* of $Y_1$ is obtained by summing over all possible</span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a>values of $Y_2$:</span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a>p(y_1) &amp;= \sum_{y_2 \in \mathcal{Y}_2} p(y_1, y_2) <span class="sc">\\</span></span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a>       &amp;= \sum_{y_2 \in \mathcal{Y}_2} p(y_1 \mid y_2) p(y_2).</span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a>The *conditional density* is</span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a>p(y_2 \mid y_1) = \frac{p(y_1, y_2)}{p(y_1)}</span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a>Notice that given the joint density $p(y_1, y_2)$, we can calculate marginal and</span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a>conditional densities $<span class="sc">\{</span> p(y_1), p(y_2), p(y_1 \mid y_2), p(y_2 \mid y_1) <span class="sc">\}</span>$ </span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a>by simply summing up the relevant variables. Additionally, given $p(y_1)$ and </span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a>$p(y_2 \mid y_1)$, (or the reverse), we can reconstruct the joint distribution.</span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a>However, given only marginal densities $p(y_1)$ and $p(y_2)$, we can't</span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a>reconstruct the joint distribution, since we don't know whether the events are</span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a>independent.</span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a><span class="fu">### Continuous</span></span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a>In the continuous case, the probability density function is a function of $y_1$</span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a>and $y_2$ such that the CDF is</span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a>F(y_1, y_2) = \int_{-\infty}^{y_1} \int_{-\infty}^{y_2} p(y_1, y_2) \; dy_2 \; dy_1.</span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a>Obtaining the marginal densities can be done by integrating out the irrelevant variable:</span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a>p(y_1) &amp;= \int_{-\infty}^{\infty} p(y_1, y_2) \; dy_2 <span class="sc">\\</span></span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a>p(y_2) &amp;= \int_{-\infty}^{\infty} p(y_1, y_2) \; dy_1 <span class="sc">\\</span></span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a>With the marginal densities, you can compute the conditional densities $p(y_2</span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a>\mid y_1) = p(y_1, y_2) / p(y_1)$, etc.</span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a><span class="fu">## Independent random variables</span></span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a>Let $Y_1, \dots, Y_n$ be random variables dependent on a common parameter</span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a>$\theta$. Then $Y_1 \dots, Y_n$ are conditionally independent given $\theta$ if</span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a>p(y_1, \dots, y_n \mid \theta) = p(y_1 \mid \theta) \times \dots \times p(y_n \mid \theta).</span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a>Note this extends naturally from the definition of independent of two random</span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a>variables, $P(A \cap B) = P(A) P(B)$. Thus, knowing about any $Y_i$ does not </span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a>give any information about the other $Y_j$. Lastly, the joint density of these </span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a>variables can be defined as</span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a>p(y_1, \dots, y_n \mid \theta) = \prod_i p(y_i \mid \theta).</span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a>We say that $Y_1, \dots, Y_n$ are conditionallly independent and identically distributed (i.i.d.):</span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a>Y_1, \dots, Y_n \mid \theta \sim \text{i.i.d.} \; p(y \mid \theta).</span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a><span class="fu"># Exchangeability</span></span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a>In many situations with several random variables, we would intuit that the</span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a>specific order of observation of these random variables aren't important. For</span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a>example, consider a random sample of 3 participants from an infinite</span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a>population which may or may not have a property (1 or 0) It makes sense that</span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a>$$p(0, 0, 1) = p(1, 0, 0) = p(0, 1, 0).$$</span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a>since the likelihood of a person having the property or not is $\theta$,</span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a>regardless of the sample. This property is called exchangeability.</span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Exchangability. Let $Y_1, \dots, Y_n$ be random variables. If $p(y_1, \dots,</span></span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; y_n) = p(y_{\pi_1}, \dots, y_{\pi_n})$ for all permutations $\pi$ of $</span><span class="sc">\{</span><span class="at">1, \dots, n</span><span class="sc">\}</span><span class="at">$, then $Y_1, \dots, Y_n$ are exchangable.</span></span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a>If $Y_1, \dots Y_n$ are i.i.d., then they are exchangeable:</span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a>p(y_1, \dots, y_n) &amp;= \int p(y_1, \dots, y_n \mid \theta)\; d\theta &amp; <span class="sc">\\</span></span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a>&amp;= \int \left( \prod_i p(y_i \mid \theta) \right) p(\theta) \; d\theta &amp; \text{(i.i.d.)} <span class="sc">\\</span></span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a>&amp;= \int \left( \prod_i p(y_{\pi_i} \mid \theta) \right) p(\theta) \; d\theta &amp; \text{(order of product doesn't matter)} <span class="sc">\\</span></span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a>&amp;= p(y_{\pi_1}, \dots, y_{\pi_n}).</span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a>Classical assumption of Bernoulli variables $X_1, X_2, \dots, X_n$ as outcomes </span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a>of the same experiment (e.g. a coin flip): *independence*. But continuing to </span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a>observe $X_j$s should result in a change of opinion about the distribution of </span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a>coin flip outcomes (e.g. gradually learning coin bias). So Bayesian </span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a>statisticians should assume *exchangeability*, a weaker condition than </span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a>*independence*.</span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a><span class="fu"># de Finetti's theorem</span></span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; de Finetti's theorem. Let $Y_1, \dots, Y_n$ be a finite subset of an infinitely exchangeable but **not necessarily i.i.d.** random variables: $$p(y_1, \dots, y_n) = p(y_{\pi_1}, \dots, y_{\pi_n})$$ for all partitions $\pi$. Then our model can be written as</span></span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $$p(y_1, \dots, y_n) = \int \left( \prod_1^n p(y_i \mid \theta) \right) p(\theta) \; d\theta.$$</span></span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a>So, in general,</span>
<span id="cb2-214"><a href="#cb2-214" aria-hidden="true" tabindex="-1"></a>$$Y_1, \dots, Y_n \mid \theta \text{ are i.i.d.} \Leftrightarrow Y_1, \dots, Y_n \text{ are exchangable for all $n$}$$</span>
<span id="cb2-215"><a href="#cb2-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-216"><a href="#cb2-216" aria-hidden="true" tabindex="-1"></a>Importantly, if we sample from a sufficiently large population, then we can</span>
<span id="cb2-217"><a href="#cb2-217" aria-hidden="true" tabindex="-1"></a>model the sample variables as being approximately conditionally i.i.d.</span>
<span id="cb2-218"><a href="#cb2-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-219"><a href="#cb2-219" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof for {0, 1} random variables</span></span>
<span id="cb2-220"><a href="#cb2-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-221"><a href="#cb2-221" aria-hidden="true" tabindex="-1"></a>From Heath &amp; Sudderth (1976). De Finetti's Theorem on Exchangeable Variables. Also see this <span class="co">[</span><span class="ot">neat StackExchange answer</span><span class="co">](http://stats.stackexchange.com/questions/34465/what-is-so-cool-about-de-finettis-representation-theorem)</span>.</span>
<span id="cb2-222"><a href="#cb2-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-223"><a href="#cb2-223" aria-hidden="true" tabindex="-1"></a><span class="fu">### Alternative statement of theorem</span></span>
<span id="cb2-224"><a href="#cb2-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-225"><a href="#cb2-225" aria-hidden="true" tabindex="-1"></a>For every infinite sequence of exchangeable random variables $(X_n)$ having</span>
<span id="cb2-226"><a href="#cb2-226" aria-hidden="true" tabindex="-1"></a>values in $<span class="sc">\{</span>0, 1<span class="sc">\}</span>$ and a finite subsequence $X_1, \dots, X_n$, there is a probability distribution $F$ such that</span>
<span id="cb2-227"><a href="#cb2-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-228"><a href="#cb2-228" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-229"><a href="#cb2-229" aria-hidden="true" tabindex="-1"></a>P(X_1 = 1, \dots, X_k = 1, X_{k+1} = 0, \dots, X_n = 0) &amp;=  <span class="sc">\\</span></span>
<span id="cb2-230"><a href="#cb2-230" aria-hidden="true" tabindex="-1"></a>P\left(\underbrace{1, \dots, 1}_{\text{$k$ times}}, \underbrace{0, \dots, 0}_{\text{$n - k$ times}}\right) &amp;= \int_{0, 1} \theta^k (1 - \theta)^{n-k} F(d \theta)</span>
<span id="cb2-231"><a href="#cb2-231" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-232"><a href="#cb2-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-233"><a href="#cb2-233" aria-hidden="true" tabindex="-1"></a>Where $F$ is the prior over $\Theta$, i.e. the values that $\theta$ can take.</span>
<span id="cb2-234"><a href="#cb2-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-235"><a href="#cb2-235" aria-hidden="true" tabindex="-1"></a><span class="fu">### Proof</span></span>
<span id="cb2-236"><a href="#cb2-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-237"><a href="#cb2-237" aria-hidden="true" tabindex="-1"></a>*Lemma*. Let $p_{k, n} = P(X_1 = 1, \dots, X_k = 1, X_{k + 1} = 0, \dots, X_n =</span>
<span id="cb2-238"><a href="#cb2-238" aria-hidden="true" tabindex="-1"></a>0)$, the quantity on the left side of the above equation, and suppose $X_1,</span>
<span id="cb2-239"><a href="#cb2-239" aria-hidden="true" tabindex="-1"></a>\dots, X_m$ are exchangable Boolean random variables as before.</span>
<span id="cb2-240"><a href="#cb2-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-241"><a href="#cb2-241" aria-hidden="true" tabindex="-1"></a>Let $q_r = P\left( \sum_{j</span>
<span id="cb2-242"><a href="#cb2-242" aria-hidden="true" tabindex="-1"></a>= 1}^m X_j = r \right)$, that is, $q_i$ is the probability that exactly $i$ of</span>
<span id="cb2-243"><a href="#cb2-243" aria-hidden="true" tabindex="-1"></a>the $m$ variables are 1.</span>
<span id="cb2-244"><a href="#cb2-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-245"><a href="#cb2-245" aria-hidden="true" tabindex="-1"></a>Also denote $(x)_k = \prod_{j = 0}^{k - 1} (x - j) = \frac{x!}{(x - k)!}$.</span>
<span id="cb2-246"><a href="#cb2-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-247"><a href="#cb2-247" aria-hidden="true" tabindex="-1"></a>Then,</span>
<span id="cb2-248"><a href="#cb2-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-249"><a href="#cb2-249" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-250"><a href="#cb2-250" aria-hidden="true" tabindex="-1"></a>p_{k, n} = \sum_{r = 0}^{m} \frac{(r)_k (m - r)_{n - k}}{(m)_n} q_r \text{ for } 0 \leq k \leq n \leq m</span>
<span id="cb2-251"><a href="#cb2-251" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-252"><a href="#cb2-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-253"><a href="#cb2-253" aria-hidden="true" tabindex="-1"></a>where $\frac{(r)_k (m - r)_{n - k}}{(m)_n}$ is some complicated combinatorics </span>
<span id="cb2-254"><a href="#cb2-254" aria-hidden="true" tabindex="-1"></a>term for the number of ways to that $r$ ones and $m - r$ zeros can be placed in </span>
<span id="cb2-255"><a href="#cb2-255" aria-hidden="true" tabindex="-1"></a>a sequence of length $m$.</span>
<span id="cb2-256"><a href="#cb2-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-257"><a href="#cb2-257" aria-hidden="true" tabindex="-1"></a>With the sustitution $\frac{r}{m} = \theta$, we can re-package the summation of</span>
<span id="cb2-258"><a href="#cb2-258" aria-hidden="true" tabindex="-1"></a>discrete $r$ as an integral over a stepwise function $F_m$ with domain $<span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$</span>
<span id="cb2-259"><a href="#cb2-259" aria-hidden="true" tabindex="-1"></a>that jumps $q_r$ at every interval $0 \leq r / m \leq 1$:</span>
<span id="cb2-260"><a href="#cb2-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-261"><a href="#cb2-261" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-262"><a href="#cb2-262" aria-hidden="true" tabindex="-1"></a>p_{k, n} = \int_{0}^{1} \frac{(\theta m)_k ((1 - \theta)m)_{n - k}}{(m)_n} F_m(d\theta).</span>
<span id="cb2-263"><a href="#cb2-263" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-264"><a href="#cb2-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-265"><a href="#cb2-265" aria-hidden="true" tabindex="-1"></a>As $m$ goes to infinity, the steps of the stepwise function grow infinitely </span>
<span id="cb2-266"><a href="#cb2-266" aria-hidden="true" tabindex="-1"></a>small, such that the function converges to the continuous probability </span>
<span id="cb2-267"><a href="#cb2-267" aria-hidden="true" tabindex="-1"></a>distribution $F$. By Helly's theorem and some hand-waving in this explanation,</span>
<span id="cb2-268"><a href="#cb2-268" aria-hidden="true" tabindex="-1"></a>the components of the integrand converge to the exponential terms in the</span>
<span id="cb2-269"><a href="#cb2-269" aria-hidden="true" tabindex="-1"></a>theorem: $\theta^k (1 - \theta)^{n - k}$.</span>
<span id="cb2-270"><a href="#cb2-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-271"><a href="#cb2-271" aria-hidden="true" tabindex="-1"></a><span class="fu"># Exercises</span></span>
<span id="cb2-272"><a href="#cb2-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-273"><a href="#cb2-273" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2.1</span></span>
<span id="cb2-274"><a href="#cb2-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-275"><a href="#cb2-275" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb2-276"><a href="#cb2-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-277"><a href="#cb2-277" aria-hidden="true" tabindex="-1"></a>The marginal probability distribution of a father's occupation is found by</span>
<span id="cb2-278"><a href="#cb2-278" aria-hidden="true" tabindex="-1"></a>summing over the rows of the son's occupation. Formally, let $F$ be the father's</span>
<span id="cb2-279"><a href="#cb2-279" aria-hidden="true" tabindex="-1"></a>occupation and $S$ be the son's. Then $P(F = f) = \sum P(F = f, S = s)$.</span>
<span id="cb2-280"><a href="#cb2-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-281"><a href="#cb2-281" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Farm: <span class="in">`r 0.018 + 0.035 + 0.031 + 0.008 + 0.018`</span></span>
<span id="cb2-282"><a href="#cb2-282" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Operatives: <span class="in">`r 0.002 + 0.112 + 0.064 + 0.032 + 0.069`</span></span>
<span id="cb2-283"><a href="#cb2-283" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>etc...</span>
<span id="cb2-284"><a href="#cb2-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-285"><a href="#cb2-285" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb2-286"><a href="#cb2-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-287"><a href="#cb2-287" aria-hidden="true" tabindex="-1"></a>This time sum over columns.</span>
<span id="cb2-288"><a href="#cb2-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-289"><a href="#cb2-289" aria-hidden="true" tabindex="-1"></a><span class="fu">### c</span></span>
<span id="cb2-290"><a href="#cb2-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-291"><a href="#cb2-291" aria-hidden="true" tabindex="-1"></a>In the row of the table where the father's occupation is "farm", normalize the</span>
<span id="cb2-292"><a href="#cb2-292" aria-hidden="true" tabindex="-1"></a>values of the son's occupation such that they sum to one. This can be done by dividing each value by the sum of the row. Formally, this is</span>
<span id="cb2-293"><a href="#cb2-293" aria-hidden="true" tabindex="-1"></a>calculating</span>
<span id="cb2-294"><a href="#cb2-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-295"><a href="#cb2-295" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-296"><a href="#cb2-296" aria-hidden="true" tabindex="-1"></a>P(S = s \mid F = \text{farm}) &amp;= \frac{P(S = s, F = \text{farm})}{P(F = \text{farm})} <span class="sc">\\</span></span>
<span id="cb2-297"><a href="#cb2-297" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{P(S = s, F = \text{farm})}{\sum_{s' \in S} P(S = s', F = \text{farm})}.</span>
<span id="cb2-298"><a href="#cb2-298" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-299"><a href="#cb2-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-300"><a href="#cb2-300" aria-hidden="true" tabindex="-1"></a><span class="fu">### d</span></span>
<span id="cb2-301"><a href="#cb2-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-302"><a href="#cb2-302" aria-hidden="true" tabindex="-1"></a>Normalize the column where the son's occupation is "farm" like above.</span>
<span id="cb2-303"><a href="#cb2-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-304"><a href="#cb2-304" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2.2</span></span>
<span id="cb2-305"><a href="#cb2-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-306"><a href="#cb2-306" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb2-307"><a href="#cb2-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-308"><a href="#cb2-308" aria-hidden="true" tabindex="-1"></a>Since expectation is linear,</span>
<span id="cb2-309"><a href="#cb2-309" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-310"><a href="#cb2-310" aria-hidden="true" tabindex="-1"></a>\mathbb{E}(a_1 Y_1 + a_2 Y_2) &amp;= a_1\mathbb{E}(Y_1) + a_2\mathbb{E}(Y_2) <span class="sc">\\</span></span>
<span id="cb2-311"><a href="#cb2-311" aria-hidden="true" tabindex="-1"></a>&amp;= a_1 \mu_1 + a_2 \mu_2</span>
<span id="cb2-312"><a href="#cb2-312" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-313"><a href="#cb2-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-314"><a href="#cb2-314" aria-hidden="true" tabindex="-1"></a>Since $Y_1$ and $Y_2$ are independent, $\text{Cov}(Y_1, Y_2) = 0$. When adding</span>
<span id="cb2-315"><a href="#cb2-315" aria-hidden="true" tabindex="-1"></a>variances, it's necessary to combine terms like below:</span>
<span id="cb2-316"><a href="#cb2-316" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-317"><a href="#cb2-317" aria-hidden="true" tabindex="-1"></a>\text{Var}(a_1 Y_1 + a_2 Y_2) &amp;= a^2\text{Var}(Y_1) + b^2\text{Var}(Y_2) + 2ab\text{Cov}(Y_1, Y_2) <span class="sc">\\</span></span>
<span id="cb2-318"><a href="#cb2-318" aria-hidden="true" tabindex="-1"></a>&amp;= a^2\sigma_1^2 + b^2\sigma_2^2</span>
<span id="cb2-319"><a href="#cb2-319" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-320"><a href="#cb2-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-321"><a href="#cb2-321" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb2-322"><a href="#cb2-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-323"><a href="#cb2-323" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-324"><a href="#cb2-324" aria-hidden="true" tabindex="-1"></a>\mathbb{E}(a_1 Y_1 - a_2 Y_2) &amp;= a_1\mathbb{E}(Y_1) - a_2\mathbb{E}(Y_2) <span class="sc">\\</span></span>
<span id="cb2-325"><a href="#cb2-325" aria-hidden="true" tabindex="-1"></a>&amp;= a_1 \mu_1 - a_2 \mu_2</span>
<span id="cb2-326"><a href="#cb2-326" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-327"><a href="#cb2-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-328"><a href="#cb2-328" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-329"><a href="#cb2-329" aria-hidden="true" tabindex="-1"></a>\text{Var}(a_1 Y_1 - a_2 Y_2) &amp;= a^2\text{Var}(Y_1) + b^2\text{Var}(Y_2) - 2ab\text{Cov}(Y_1, Y_2) <span class="sc">\\</span></span>
<span id="cb2-330"><a href="#cb2-330" aria-hidden="true" tabindex="-1"></a>&amp;= a^2\sigma_1^2 + b^2\sigma_2^2 <span class="sc">\\</span></span>
<span id="cb2-331"><a href="#cb2-331" aria-hidden="true" tabindex="-1"></a>&amp;= \text{Var}(a_1 Y_1 + a_2 Y_2)</span>
<span id="cb2-332"><a href="#cb2-332" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-333"><a href="#cb2-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-334"><a href="#cb2-334" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2.3</span></span>
<span id="cb2-335"><a href="#cb2-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-336"><a href="#cb2-336" aria-hidden="true" tabindex="-1"></a>Let $X, Y, Z$ be random variables with joint density $p(x, y, z) \propto f(x, z) g(y, z) h(z)$.</span>
<span id="cb2-337"><a href="#cb2-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-338"><a href="#cb2-338" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb2-339"><a href="#cb2-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-340"><a href="#cb2-340" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-341"><a href="#cb2-341" aria-hidden="true" tabindex="-1"></a>p(x \mid y, z) &amp;= \frac{p(x, y, z)}{p(y, z)} <span class="sc">\\</span></span>
<span id="cb2-342"><a href="#cb2-342" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{p(x, y, z)}{\int p(x, y, z) \; dx} <span class="sc">\\</span></span>
<span id="cb2-343"><a href="#cb2-343" aria-hidden="true" tabindex="-1"></a>&amp;\propto \frac{f(x, z) g(y, z) h(z)}{\int f(x, z) g(y, z) h(z) \; dx} <span class="sc">\\</span></span>
<span id="cb2-344"><a href="#cb2-344" aria-hidden="true" tabindex="-1"></a>&amp;\propto \frac{f(x, z) g(y, z) h(z)}{g(y, z) h(z) \int f(x, z) \; dx} <span class="sc">\\</span></span>
<span id="cb2-345"><a href="#cb2-345" aria-hidden="true" tabindex="-1"></a>&amp;\propto \frac{f(x, z)}{\int f(x, z) \; dx}</span>
<span id="cb2-346"><a href="#cb2-346" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-347"><a href="#cb2-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-348"><a href="#cb2-348" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb2-349"><a href="#cb2-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-350"><a href="#cb2-350" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-351"><a href="#cb2-351" aria-hidden="true" tabindex="-1"></a>p(y \mid x, z) &amp;= \frac{p(x, y, z)}{p(x, z)} <span class="sc">\\</span></span>
<span id="cb2-352"><a href="#cb2-352" aria-hidden="true" tabindex="-1"></a>&amp;\propto \frac{f(x, z) g(y, z) h(z)}{\int p(x, y, z) \; dy} <span class="sc">\\</span></span>
<span id="cb2-353"><a href="#cb2-353" aria-hidden="true" tabindex="-1"></a>&amp;\propto \frac{f(x, z) g(y, z) h(z)}{\int f(x, z) g(y, z) h(z) \; dy} <span class="sc">\\</span></span>
<span id="cb2-354"><a href="#cb2-354" aria-hidden="true" tabindex="-1"></a>&amp;\propto \frac{f(x, z) g(y, z) h(z)}{f(x, z) h(z) \int g(y, z) \; dy}</span>
<span id="cb2-355"><a href="#cb2-355" aria-hidden="true" tabindex="-1"></a>&amp;\propto \frac{g(y, z)}{\int g(y, z) \; dy}</span>
<span id="cb2-356"><a href="#cb2-356" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-357"><a href="#cb2-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-358"><a href="#cb2-358" aria-hidden="true" tabindex="-1"></a><span class="fu">### c</span></span>
<span id="cb2-359"><a href="#cb2-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-360"><a href="#cb2-360" aria-hidden="true" tabindex="-1"></a>It is sufficient to show that $p(x \mid y, z) = p(x \mid z)$:</span>
<span id="cb2-361"><a href="#cb2-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-362"><a href="#cb2-362" aria-hidden="true" tabindex="-1"></a>From (a), $p(x \mid y, z) \propto \frac{f(x, z)}{\int f(x, z) \; dx}$. Now</span>
<span id="cb2-363"><a href="#cb2-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-364"><a href="#cb2-364" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-365"><a href="#cb2-365" aria-hidden="true" tabindex="-1"></a>p(x \mid z) &amp;= \frac{p(x, z)}{p(z)} <span class="sc">\\</span></span>
<span id="cb2-366"><a href="#cb2-366" aria-hidden="true" tabindex="-1"></a>&amp;\propto \frac{\int p(x, y, z) \; dy}{\int \int p(x, y, z) \; dy \; dx} <span class="sc">\\</span></span>
<span id="cb2-367"><a href="#cb2-367" aria-hidden="true" tabindex="-1"></a>&amp;\propto \frac{\int f(x, z) g(y, z) h(z) \; dy}{\int \int f(x, z) g(y, z) h(z) \; dy \; dx} <span class="sc">\\</span></span>
<span id="cb2-368"><a href="#cb2-368" aria-hidden="true" tabindex="-1"></a>&amp;\propto \frac{f(x, z) h(z) \int g(y, z) \; dy}{h(z) \left( \int g(y, z) \; dy \right) \left( \int f(x, z) \; dx \right)} <span class="sc">\\</span></span>
<span id="cb2-369"><a href="#cb2-369" aria-hidden="true" tabindex="-1"></a>&amp;\propto \frac{f(x, z)}{\int f(x, z) \; dx} <span class="sc">\\</span></span>
<span id="cb2-370"><a href="#cb2-370" aria-hidden="true" tabindex="-1"></a>&amp;\propto p(x \mid y, z)</span>
<span id="cb2-371"><a href="#cb2-371" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-372"><a href="#cb2-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-373"><a href="#cb2-373" aria-hidden="true" tabindex="-1"></a>Now, since $p(x \mid z) \propto p(x \mid y, z)$ but additionally $\int p(x \mid</span>
<span id="cb2-374"><a href="#cb2-374" aria-hidden="true" tabindex="-1"></a>z) = \int p(x \mid y, z) = 1$, $p(x \mid z) = p(x \mid y, z)$, so $x$ and $y$ are conditionally independent given $z$.</span>
<span id="cb2-375"><a href="#cb2-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-376"><a href="#cb2-376" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2.4</span></span>
<span id="cb2-377"><a href="#cb2-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-378"><a href="#cb2-378" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb2-379"><a href="#cb2-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-380"><a href="#cb2-380" aria-hidden="true" tabindex="-1"></a>Use **P3** and condition on an always true event:</span>
<span id="cb2-381"><a href="#cb2-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-382"><a href="#cb2-382" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-383"><a href="#cb2-383" aria-hidden="true" tabindex="-1"></a>P(H_j \cap E \mid 1 = 1) &amp;= P(H_j \mid 1 = 1) P(E \mid H_j \cap 1 = 1) = P(H_j)P(E \mid H_j) <span class="sc">\\</span></span>
<span id="cb2-384"><a href="#cb2-384" aria-hidden="true" tabindex="-1"></a>P(H_j \cap E \mid 1 = 1) &amp;= P(E \mid 1 = 1) P(H_j \mid E \cap 1 = 1) = p(E)p(H_j \mid E) <span class="sc">\\</span></span>
<span id="cb2-385"><a href="#cb2-385" aria-hidden="true" tabindex="-1"></a>&amp;= P(H_j) P(E \mid H_j)</span>
<span id="cb2-386"><a href="#cb2-386" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-387"><a href="#cb2-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-388"><a href="#cb2-388" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb2-389"><a href="#cb2-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-390"><a href="#cb2-390" aria-hidden="true" tabindex="-1"></a>If we can assume that $p(\mathcal{H}) = 1$, then</span>
<span id="cb2-391"><a href="#cb2-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-392"><a href="#cb2-392" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-393"><a href="#cb2-393" aria-hidden="true" tabindex="-1"></a>P(E) &amp;= P(E \cap H) <span class="sc">\\</span></span>
<span id="cb2-394"><a href="#cb2-394" aria-hidden="true" tabindex="-1"></a>&amp;= P(E \cap (H_1 \cup H_2 \cup \dots \cup H_k)) <span class="sc">\\</span></span>
<span id="cb2-395"><a href="#cb2-395" aria-hidden="true" tabindex="-1"></a>&amp;= P((E \cap (H_1)) \cup (E \cap (H_2 \cup \dots \cup H_k))) &amp; \text{Distributing} <span class="sc">\\</span></span>
<span id="cb2-396"><a href="#cb2-396" aria-hidden="true" tabindex="-1"></a>&amp;= P(E \cap H_1) + P(E \cap (H_2 \cup \dots \cup H_k)) &amp; \text{P2; Implied condition on a true event}</span>
<span id="cb2-397"><a href="#cb2-397" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-398"><a href="#cb2-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-399"><a href="#cb2-399" aria-hidden="true" tabindex="-1"></a><span class="fu">### c</span></span>
<span id="cb2-400"><a href="#cb2-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-401"><a href="#cb2-401" aria-hidden="true" tabindex="-1"></a>Proceed inductively from b.</span>
<span id="cb2-402"><a href="#cb2-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-403"><a href="#cb2-403" aria-hidden="true" tabindex="-1"></a><span class="fu">### d</span></span>
<span id="cb2-404"><a href="#cb2-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-405"><a href="#cb2-405" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-406"><a href="#cb2-406" aria-hidden="true" tabindex="-1"></a>&amp; P(H_j)P(E \mid H_j) = P(E)P(H_j \mid E) <span class="sc">\\</span></span>
<span id="cb2-407"><a href="#cb2-407" aria-hidden="true" tabindex="-1"></a>\implies&amp; P(H_j \mid E) = \frac{P(H_j)P(E \mid H_j)}{P(E)} <span class="sc">\\</span></span>
<span id="cb2-408"><a href="#cb2-408" aria-hidden="true" tabindex="-1"></a>\implies&amp; P(H_j \mid E) = \frac{P(H_j)P(E \mid H_j)}{\sum_k P(E \cap H_k)} &amp; \text{From c} <span class="sc">\\</span></span>
<span id="cb2-409"><a href="#cb2-409" aria-hidden="true" tabindex="-1"></a>\implies&amp; P(H_j \mid E) = \frac{P(H_j)P(E \mid H_j)}{\sum_k P(E \mid H_k) P(H_k)} &amp; \text{P3} <span class="sc">\\</span></span>
<span id="cb2-410"><a href="#cb2-410" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-411"><a href="#cb2-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-412"><a href="#cb2-412" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2.5</span></span>
<span id="cb2-413"><a href="#cb2-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-414"><a href="#cb2-414" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb2-415"><a href="#cb2-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-416"><a href="#cb2-416" aria-hidden="true" tabindex="-1"></a><span class="in">```{r kable}</span></span>
<span id="cb2-417"><a href="#cb2-417" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rbind</span>(</span>
<span id="cb2-418"><a href="#cb2-418" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Y = 0 Y = 1</span></span>
<span id="cb2-419"><a href="#cb2-419" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(.<span class="dv">5</span> <span class="sc">*</span> .<span class="dv">4</span>, .<span class="dv">5</span> <span class="sc">*</span> .<span class="dv">6</span>), <span class="co"># X = 0</span></span>
<span id="cb2-420"><a href="#cb2-420" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(.<span class="dv">5</span> <span class="sc">*</span> .<span class="dv">6</span>, .<span class="dv">5</span> <span class="sc">*</span> .<span class="dv">4</span>) <span class="co"># X = 1</span></span>
<span id="cb2-421"><a href="#cb2-421" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-422"><a href="#cb2-422" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(x) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"X = 0"</span>, <span class="st">"X = 1"</span>)</span>
<span id="cb2-423"><a href="#cb2-423" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(x, <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">"Y = 0"</span>, <span class="st">"Y = 1"</span>))</span>
<span id="cb2-424"><a href="#cb2-424" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-425"><a href="#cb2-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-426"><a href="#cb2-426" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb2-427"><a href="#cb2-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-428"><a href="#cb2-428" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}(Y) = 0.5$$. Probability the ball is green is $0.5$.</span>
<span id="cb2-429"><a href="#cb2-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-430"><a href="#cb2-430" aria-hidden="true" tabindex="-1"></a><span class="fu">### c</span></span>
<span id="cb2-431"><a href="#cb2-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-432"><a href="#cb2-432" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-433"><a href="#cb2-433" aria-hidden="true" tabindex="-1"></a>\text{Var}(Y \mid X = 0) &amp;= \mathbb{E}((Y \mid X = 0)^2) - \mathbb{E}(Y \mid X = 0)^2 <span class="sc">\\</span></span>
<span id="cb2-434"><a href="#cb2-434" aria-hidden="true" tabindex="-1"></a>&amp;= 1^2 p(Y = 1 \mid X = 0) + 0^2 p(Y = 0 \mid X = 0) - <span class="sc">\\</span> &amp; \quad (1 p(Y = 1 \mid X = 0) + 0 p(Y = 0 \mid X = 0))^2 <span class="sc">\\</span></span>
<span id="cb2-435"><a href="#cb2-435" aria-hidden="true" tabindex="-1"></a>&amp;= .6 - (.6)^2 <span class="sc">\\</span></span>
<span id="cb2-436"><a href="#cb2-436" aria-hidden="true" tabindex="-1"></a>&amp;= 0.24 <span class="sc">\\</span></span>
<span id="cb2-437"><a href="#cb2-437" aria-hidden="true" tabindex="-1"></a>&amp;= \text{Var}(Y \mid X = 1) &amp; \text{Not going to do this one}</span>
<span id="cb2-438"><a href="#cb2-438" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-439"><a href="#cb2-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-440"><a href="#cb2-440" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-441"><a href="#cb2-441" aria-hidden="true" tabindex="-1"></a>\text{Var}(Y) &amp;= \mathbb{E}(Y^2) - \mathbb{E}(Y)^2 <span class="sc">\\</span></span>
<span id="cb2-442"><a href="#cb2-442" aria-hidden="true" tabindex="-1"></a>&amp;= 1^2 p(Y = 1) + 0^2 p(Y = 0) - (0.5)^2 <span class="sc">\\</span></span>
<span id="cb2-443"><a href="#cb2-443" aria-hidden="true" tabindex="-1"></a>&amp;= 0.5 - (0.5)^2 = 0.25</span>
<span id="cb2-444"><a href="#cb2-444" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-445"><a href="#cb2-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-446"><a href="#cb2-446" aria-hidden="true" tabindex="-1"></a>$\text{Var}(Y)$ is slightly larger since we are more uncertain about the value</span>
<span id="cb2-447"><a href="#cb2-447" aria-hidden="true" tabindex="-1"></a>of $Y$ when we have not yet flipped the coin. Knowing which urn we will draw</span>
<span id="cb2-448"><a href="#cb2-448" aria-hidden="true" tabindex="-1"></a>from clarifies our probabilites of obtaining a green or red ball.</span>
<span id="cb2-449"><a href="#cb2-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-450"><a href="#cb2-450" aria-hidden="true" tabindex="-1"></a><span class="fu">### d</span></span>
<span id="cb2-451"><a href="#cb2-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-452"><a href="#cb2-452" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-453"><a href="#cb2-453" aria-hidden="true" tabindex="-1"></a>P(X = 0 \mid Y = 1) &amp;= .6</span>
<span id="cb2-454"><a href="#cb2-454" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-455"><a href="#cb2-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-456"><a href="#cb2-456" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2.6</span></span>
<span id="cb2-457"><a href="#cb2-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-458"><a href="#cb2-458" aria-hidden="true" tabindex="-1"></a>If $A \perp B \mid C$ then</span>
<span id="cb2-459"><a href="#cb2-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-460"><a href="#cb2-460" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-461"><a href="#cb2-461" aria-hidden="true" tabindex="-1"></a>P(A, B \mid C) &amp;= P(A \mid C) P(B \mid C) <span class="sc">\\</span></span>
<span id="cb2-462"><a href="#cb2-462" aria-hidden="true" tabindex="-1"></a>  &amp;= (1 - P(A^c \mid C))P(B \mid C) <span class="sc">\\</span></span>
<span id="cb2-463"><a href="#cb2-463" aria-hidden="true" tabindex="-1"></a>  &amp;= P(B \mid C) - P(B \mid C)P(A^c \mid C)</span>
<span id="cb2-464"><a href="#cb2-464" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-465"><a href="#cb2-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-466"><a href="#cb2-466" aria-hidden="true" tabindex="-1"></a>so $$P(B \mid C)P(A^c \mid C) = P(B \mid C) - P(A, B \mid C)$$.</span>
<span id="cb2-467"><a href="#cb2-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-468"><a href="#cb2-468" aria-hidden="true" tabindex="-1"></a>Also notice</span>
<span id="cb2-469"><a href="#cb2-469" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb2-470"><a href="#cb2-470" aria-hidden="true" tabindex="-1"></a>&amp; P(B \mid C) = P(A, B \mid C) + P(A^c, B \mid C) &amp; \text{LTP} <span class="sc">\\</span></span>
<span id="cb2-471"><a href="#cb2-471" aria-hidden="true" tabindex="-1"></a>\implies &amp; P(A^c, B \mid C) = P(B \mid C) - P(A, B \mid C)</span>
<span id="cb2-472"><a href="#cb2-472" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb2-473"><a href="#cb2-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-474"><a href="#cb2-474" aria-hidden="true" tabindex="-1"></a>Equating the two equations above, we have</span>
<span id="cb2-475"><a href="#cb2-475" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-476"><a href="#cb2-476" aria-hidden="true" tabindex="-1"></a>P(A^c, B \mid C) = P(B \mid C) P(A^c \mid C)</span>
<span id="cb2-477"><a href="#cb2-477" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-478"><a href="#cb2-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-479"><a href="#cb2-479" aria-hidden="true" tabindex="-1"></a>We do this similarly for the other events.</span>
<span id="cb2-480"><a href="#cb2-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-481"><a href="#cb2-481" aria-hidden="true" tabindex="-1"></a>For a case where $A \perp B \mid C$ holds but $A \perp B \mid C^c$ does not,</span>
<span id="cb2-482"><a href="#cb2-482" aria-hidden="true" tabindex="-1"></a>consider a Bayesian network where knowing $C$ results in 100% belief in the</span>
<span id="cb2-483"><a href="#cb2-483" aria-hidden="true" tabindex="-1"></a>values of $A$ and $B$ (e.g. $P(A, B \mid C) = 1$), but absent of $C$, values $A$</span>
<span id="cb2-484"><a href="#cb2-484" aria-hidden="true" tabindex="-1"></a>and $B$ have some probability of manifesting that affect each other.</span>
<span id="cb2-485"><a href="#cb2-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-486"><a href="#cb2-486" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2.7</span></span>
<span id="cb2-487"><a href="#cb2-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-488"><a href="#cb2-488" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb2-489"><a href="#cb2-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-490"><a href="#cb2-490" aria-hidden="true" tabindex="-1"></a>No matter how likely the event, the maximum money you will obtain is $1. Thus no</span>
<span id="cb2-491"><a href="#cb2-491" aria-hidden="true" tabindex="-1"></a>rational person would give more than $1 for the possibility of obtaining $1, as </span>
<span id="cb2-492"><a href="#cb2-492" aria-hidden="true" tabindex="-1"></a>such an action would always result in lost money.</span>
<span id="cb2-493"><a href="#cb2-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-494"><a href="#cb2-494" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb2-495"><a href="#cb2-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-496"><a href="#cb2-496" aria-hidden="true" tabindex="-1"></a>Since either $E$ or $E^c$ will definitively happen, we would want someone who is</span>
<span id="cb2-497"><a href="#cb2-497" aria-hidden="true" tabindex="-1"></a>betting on the occurrences of either $E$ or $E^c$ to even out.</span>
<span id="cb2-498"><a href="#cb2-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-499"><a href="#cb2-499" aria-hidden="true" tabindex="-1"></a><span class="fu">## 2.8</span></span>
<span id="cb2-500"><a href="#cb2-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-501"><a href="#cb2-501" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb2-502"><a href="#cb2-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-503"><a href="#cb2-503" aria-hidden="true" tabindex="-1"></a><span class="fu">#### i</span></span>
<span id="cb2-504"><a href="#cb2-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-505"><a href="#cb2-505" aria-hidden="true" tabindex="-1"></a>A frequentist idea: if I were to pick at random many, many $x$ sampled from the</span>
<span id="cb2-506"><a href="#cb2-506" aria-hidden="true" tabindex="-1"></a>census roll, how many of them would be Hindu?</span>
<span id="cb2-507"><a href="#cb2-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-508"><a href="#cb2-508" aria-hidden="true" tabindex="-1"></a>A subjectivist interpretation: how strongly do I believe that a random person</span>
<span id="cb2-509"><a href="#cb2-509" aria-hidden="true" tabindex="-1"></a>from this census roll is Hindu?</span>
<span id="cb2-510"><a href="#cb2-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-511"><a href="#cb2-511" aria-hidden="true" tabindex="-1"></a>(Should be $0.15$)</span>
<span id="cb2-512"><a href="#cb2-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-513"><a href="#cb2-513" aria-hidden="true" tabindex="-1"></a><span class="fu">#### ii</span></span>
<span id="cb2-514"><a href="#cb2-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-515"><a href="#cb2-515" aria-hidden="true" tabindex="-1"></a>If I were to pick at random many, many $x$ from the census roll, in the long run</span>
<span id="cb2-516"><a href="#cb2-516" aria-hidden="true" tabindex="-1"></a>average, what proportion of them would be $6452859$?</span>
<span id="cb2-517"><a href="#cb2-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-518"><a href="#cb2-518" aria-hidden="true" tabindex="-1"></a>How strongly do I believe that this $x$ I am going to sample is $6452859$?</span>
<span id="cb2-519"><a href="#cb2-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-520"><a href="#cb2-520" aria-hidden="true" tabindex="-1"></a><span class="fu">#### iii</span></span>
<span id="cb2-521"><a href="#cb2-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-522"><a href="#cb2-522" aria-hidden="true" tabindex="-1"></a>**An interesting one**</span>
<span id="cb2-523"><a href="#cb2-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-524"><a href="#cb2-524" aria-hidden="true" tabindex="-1"></a>Assume person $6452589$ is a random person sampled from Sri Lanka that is either</span>
<span id="cb2-525"><a href="#cb2-525" aria-hidden="true" tabindex="-1"></a>Hindu or not. In the long run, if I observed many persons $6452589$, how many of</span>
<span id="cb2-526"><a href="#cb2-526" aria-hidden="true" tabindex="-1"></a>them would be Hindu?</span>
<span id="cb2-527"><a href="#cb2-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-528"><a href="#cb2-528" aria-hidden="true" tabindex="-1"></a>Person $6452589$ is *one* person who may or may not be Hindu. How strongly do I</span>
<span id="cb2-529"><a href="#cb2-529" aria-hidden="true" tabindex="-1"></a>believe that he/she is Hindu?</span>
<span id="cb2-530"><a href="#cb2-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-531"><a href="#cb2-531" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb2-532"><a href="#cb2-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-533"><a href="#cb2-533" aria-hidden="true" tabindex="-1"></a><span class="fu">#### i</span></span>
<span id="cb2-534"><a href="#cb2-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-535"><a href="#cb2-535" aria-hidden="true" tabindex="-1"></a><span class="fu">#### ii</span></span>
<span id="cb2-536"><a href="#cb2-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-537"><a href="#cb2-537" aria-hidden="true" tabindex="-1"></a><span class="fu">#### iii</span></span>
<span id="cb2-538"><a href="#cb2-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-539"><a href="#cb2-539" aria-hidden="true" tabindex="-1"></a>Skipping</span>
<span id="cb2-540"><a href="#cb2-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-541"><a href="#cb2-541" aria-hidden="true" tabindex="-1"></a><span class="fu">### c</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
<li class="nav-item">
 Copyright 2024, Jesse Mu
  </li>  
</ul>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/d-morrison/hoff-bayesian-statistics/edit/main/2.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/d-morrison/hoff-bayesian-statistics/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/d-morrison/hoff-bayesian-statistics/blob/main/2.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>


</body></html>