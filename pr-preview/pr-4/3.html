<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jesse Mu">
<meta name="dcterms.date" content="2016-09-17">

<title>Chapter 3: One-parameter models – Hoff Bayesian Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-68c8bffd90dad8f2b55c52d7b6410dc0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-26500bfc55c7891837a911d6d50a6255.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Hoff Bayesian Statistics</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-chapters" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Chapters</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-chapters">    
        <li>
    <a class="dropdown-item" href="./1.html">
 <span class="dropdown-text">Chapter 1: Introduction and examples</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./2.html">
 <span class="dropdown-text">Chapter 2: Belief, probability, and exchangeability</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./3.html">
 <span class="dropdown-text">Chapter 3: One-parameter models</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./4.html">
 <span class="dropdown-text">Chapter 4: Monte Carlo approximation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./5.html">
 <span class="dropdown-text">Chapter 5: The Normal Model</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./6.html">
 <span class="dropdown-text">Chapter 6: Posterior approximation with the Gibbs sampler</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./7.html">
 <span class="dropdown-text">Chapter 7: The multivariate normal model</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./8.html">
 <span class="dropdown-text">Chapter 8: Group comparisons and hierarchical modeling</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./9.html">
 <span class="dropdown-text">Chapter 9: Linear regression</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./10.html">
 <span class="dropdown-text">Chapter 10: Nonconjugate priors and Metropolis-Hastings algorithms</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./irm.html"> 
<span class="menu-text">Infinite Relational Model</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-binomial-model" id="toc-the-binomial-model" class="nav-link active" data-scroll-target="#the-binomial-model">The Binomial model</a>
  <ul class="collapse">
  <li><a href="#uniform-prior" id="toc-uniform-prior" class="nav-link" data-scroll-target="#uniform-prior">Uniform prior</a></li>
  <li><a href="#data-and-posterior-distribution" id="toc-data-and-posterior-distribution" class="nav-link" data-scroll-target="#data-and-posterior-distribution">Data and posterior distribution</a></li>
  <li><a href="#inference-for-exchangeable-binary-data-i.e.-more-generally" id="toc-inference-for-exchangeable-binary-data-i.e.-more-generally" class="nav-link" data-scroll-target="#inference-for-exchangeable-binary-data-i.e.-more-generally">Inference for exchangeable binary data (i.e.&nbsp;more generally)</a>
  <ul class="collapse">
  <li><a href="#conjugacy" id="toc-conjugacy" class="nav-link" data-scroll-target="#conjugacy">Conjugacy</a></li>
  <li><a href="#prediction" id="toc-prediction" class="nav-link" data-scroll-target="#prediction">Prediction</a></li>
  </ul></li>
  <li><a href="#confidence-regions" id="toc-confidence-regions" class="nav-link" data-scroll-target="#confidence-regions">Confidence regions</a></li>
  </ul></li>
  <li><a href="#the-poisson-model" id="toc-the-poisson-model" class="nav-link" data-scroll-target="#the-poisson-model">The Poisson model</a>
  <ul class="collapse">
  <li><a href="#posterior-inference" id="toc-posterior-inference" class="nav-link" data-scroll-target="#posterior-inference">Posterior inference</a>
  <ul class="collapse">
  <li><a href="#conjugate-prior" id="toc-conjugate-prior" class="nav-link" data-scroll-target="#conjugate-prior">Conjugate prior</a></li>
  <li><a href="#posterior-predictive-distribution" id="toc-posterior-predictive-distribution" class="nav-link" data-scroll-target="#posterior-predictive-distribution">Posterior predictive distribution</a></li>
  </ul></li>
  <li><a href="#example-birth-rates" id="toc-example-birth-rates" class="nav-link" data-scroll-target="#example-birth-rates">Example: Birth Rates</a></li>
  </ul></li>
  <li><a href="#exponential-families-and-conjugate-priors" id="toc-exponential-families-and-conjugate-priors" class="nav-link" data-scroll-target="#exponential-families-and-conjugate-priors">Exponential families and conjugate priors</a>
  <ul class="collapse">
  <li><a href="#example-binomial-model" id="toc-example-binomial-model" class="nav-link" data-scroll-target="#example-binomial-model">Example: Binomial model</a>
  <ul class="collapse">
  <li><a href="#parameterization" id="toc-parameterization" class="nav-link" data-scroll-target="#parameterization">Parameterization</a></li>
  <li><a href="#prior" id="toc-prior" class="nav-link" data-scroll-target="#prior">Prior</a></li>
  </ul></li>
  <li><a href="#example-poisson-model" id="toc-example-poisson-model" class="nav-link" data-scroll-target="#example-poisson-model">Example: Poisson model</a>
  <ul class="collapse">
  <li><a href="#parameterization-1" id="toc-parameterization-1" class="nav-link" data-scroll-target="#parameterization-1">Parameterization</a></li>
  <li><a href="#prior-1" id="toc-prior-1" class="nav-link" data-scroll-target="#prior-1">Prior</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a>
  <ul class="collapse">
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section">3.1</a>
  <ul class="collapse">
  <li><a href="#a" id="toc-a" class="nav-link" data-scroll-target="#a">a</a></li>
  <li><a href="#b" id="toc-b" class="nav-link" data-scroll-target="#b">b</a></li>
  <li><a href="#c" id="toc-c" class="nav-link" data-scroll-target="#c">c</a></li>
  <li><a href="#d" id="toc-d" class="nav-link" data-scroll-target="#d">d</a></li>
  <li><a href="#e" id="toc-e" class="nav-link" data-scroll-target="#e">e</a></li>
  </ul></li>
  <li><a href="#section-1" id="toc-section-1" class="nav-link" data-scroll-target="#section-1">3.2</a></li>
  <li><a href="#section-2" id="toc-section-2" class="nav-link" data-scroll-target="#section-2">3.3</a>
  <ul class="collapse">
  <li><a href="#a-1" id="toc-a-1" class="nav-link" data-scroll-target="#a-1">a</a></li>
  <li><a href="#b-1" id="toc-b-1" class="nav-link" data-scroll-target="#b-1">b</a></li>
  <li><a href="#c-1" id="toc-c-1" class="nav-link" data-scroll-target="#c-1">c</a></li>
  </ul></li>
  <li><a href="#section-3" id="toc-section-3" class="nav-link" data-scroll-target="#section-3">3.4</a>
  <ul class="collapse">
  <li><a href="#a-2" id="toc-a-2" class="nav-link" data-scroll-target="#a-2">a</a></li>
  <li><a href="#b-2" id="toc-b-2" class="nav-link" data-scroll-target="#b-2">b</a></li>
  <li><a href="#c-2" id="toc-c-2" class="nav-link" data-scroll-target="#c-2">c</a></li>
  <li><a href="#d-1" id="toc-d-1" class="nav-link" data-scroll-target="#d-1">d</a></li>
  <li><a href="#e-1" id="toc-e-1" class="nav-link" data-scroll-target="#e-1">e</a></li>
  </ul></li>
  <li><a href="#section-4" id="toc-section-4" class="nav-link" data-scroll-target="#section-4">3.5</a>
  <ul class="collapse">
  <li><a href="#a-3" id="toc-a-3" class="nav-link" data-scroll-target="#a-3">a</a></li>
  <li><a href="#b-3" id="toc-b-3" class="nav-link" data-scroll-target="#b-3">b</a></li>
  </ul></li>
  <li><a href="#section-5" id="toc-section-5" class="nav-link" data-scroll-target="#section-5">3.9</a>
  <ul class="collapse">
  <li><a href="#a-4" id="toc-a-4" class="nav-link" data-scroll-target="#a-4">a</a></li>
  <li><a href="#b-4" id="toc-b-4" class="nav-link" data-scroll-target="#b-4">b</a></li>
  <li><a href="#c-3" id="toc-c-3" class="nav-link" data-scroll-target="#c-3">c</a></li>
  <li><a href="#d-2" id="toc-d-2" class="nav-link" data-scroll-target="#d-2">d</a></li>
  <li><a href="#e-2" id="toc-e-2" class="nav-link" data-scroll-target="#e-2">e</a></li>
  </ul></li>
  <li><a href="#section-6" id="toc-section-6" class="nav-link" data-scroll-target="#section-6">3.10</a>
  <ul class="collapse">
  <li><a href="#a-5" id="toc-a-5" class="nav-link" data-scroll-target="#a-5">a</a></li>
  <li><a href="#b-5" id="toc-b-5" class="nav-link" data-scroll-target="#b-5">b</a></li>
  </ul></li>
  <li><a href="#section-7" id="toc-section-7" class="nav-link" data-scroll-target="#section-7">3.12</a>
  <ul class="collapse">
  <li><a href="#a-6" id="toc-a-6" class="nav-link" data-scroll-target="#a-6">a</a></li>
  <li><a href="#b-6" id="toc-b-6" class="nav-link" data-scroll-target="#b-6">b</a></li>
  <li><a href="#c-4" id="toc-c-4" class="nav-link" data-scroll-target="#c-4">c</a></li>
  </ul></li>
  <li><a href="#section-8" id="toc-section-8" class="nav-link" data-scroll-target="#section-8">3.13</a>
  <ul class="collapse">
  <li><a href="#a-7" id="toc-a-7" class="nav-link" data-scroll-target="#a-7">a</a></li>
  <li><a href="#b-7" id="toc-b-7" class="nav-link" data-scroll-target="#b-7">b</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Chapter 3: One-parameter models</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Jesse Mu </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 17, 2016</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<!-- Setup -->
<!-- Begin writing -->
<section id="the-binomial-model" class="level1">
<h1>The Binomial model</h1>
<p>1998 General Social Survey: Females over age 65, <span class="math inline">\(1 = \text{happy}\)</span>, <span class="math inline">\(0 =
\text{unhappy}\)</span>. <span class="math inline">\(n = 129\)</span>. So let the survey be 129 exchangeable random variables <span class="math inline">\(Y_1, \dots, Y_{129}\)</span>.</p>
<p>Under our model, conditioned on some <span class="math inline">\(\theta\)</span>, <span class="math inline">\(Y_i\)</span> are i.i.d. binary random variables with probability <span class="math inline">\(\theta\)</span>. So the joint probability is</p>
<p><span class="math display">\[\begin{align}
p(y_1, \dots, y_{129} \mid \theta) = \theta^{\sum_{i} y_i} (1 - \theta)^{129 -
\sum_i y_i}
\end{align}\]</span></p>
<p>Now we need to specify our prior distribution</p>
<section id="uniform-prior" class="level2">
<h2 class="anchored" data-anchor-id="uniform-prior">Uniform prior</h2>
<p>Imagine our prior is <span class="math inline">\(\theta \sim \text{Uniform}(0, 1)\)</span>. What this means is <span class="math inline">\(P(a \leq \theta \leq b) = P(a + c \leq \theta \leq b + c)\)</span> for all compatible <span class="math inline">\(a, b, c\)</span>. In other words, the probability of theta falling in an interval of a given width is constant, regardless of where the interval is.</p>
<p>Then, notice</p>
<p><span class="math display">\[\begin{align}
p(\theta \mid y_1, \dots, y_{129}) &amp;= \frac{p(y_1, \dots, y_{129} \mid \theta) p(\theta)}{p(y_1, \dots, y_{129})} \\
&amp;= \frac{p(y_1, \dots, y_{129} \mid \theta)}{p(y_1, \dots, y_{129})} &amp; (\text{since $p(\theta)$ is constant for all $\theta$}) \\
&amp;\propto p(y_1, \dots, y_{129} \mid \theta)
\end{align}\]</span></p>
<p>so <span class="math inline">\(p(\theta \mid Y)\)</span> and <span class="math inline">\(p(y \mid \theta)\)</span> have the same shape (see MLE discussion in Chapter 1).</p>
</section>
<section id="data-and-posterior-distribution" class="level2">
<h2 class="anchored" data-anchor-id="data-and-posterior-distribution">Data and posterior distribution</h2>
<p>Say the observed proportion is 118 happy out of 129 (91%). Our sampling model for some fixed <span class="math inline">\(\theta\)</span> is</p>
<p><span class="math display">\[\begin{align}
p(y \mid \theta) = \theta^{118} (1 - \theta)^{11}
\end{align}\]</span></p>
<p>linking this back to Bayes’ rule above, we have the posterior probability</p>
<p><span class="math display">\[\begin{align}
p(\theta \mid y) = \frac{\theta^{118} (1 - \theta)^{11}}{p(y)}
\end{align}\]</span></p>
<p>We will often (WHEN would we not normalize?) want to be more precise than this and know about the scale of the posterior probability, not just the shape. This requires calculating <span class="math inline">\(p(y) = p(y_1, \dots, y_{129})\)</span>:</p>
<p><span class="math display">\[\begin{align}
1 &amp;= \int_0^1 p(\theta \mid y) \; d\theta &amp; (\text{Law of total probability}) \\
&amp;= \int_0^1 \theta^{111} (1 - \theta)^{11} / p(y) \; d\theta \\
&amp;= \frac{1}{p(y)} \int_0^1 \theta^{118} (1 - \theta)^{11} \; d\theta &amp; (\text{Note
$p(y)$ is constant for fixed $y$})\\
&amp;= \frac{1}{p(y)} \frac{\Gamma(119) \Gamma(12)}{\Gamma(131)} &amp; (\text{From calculus})\\
\end{align}\]</span></p>
<p>so <span class="math inline">\(p(y) = \frac{\Gamma(119) \Gamma(12)}{\Gamma(131)} \approx 2.89 \times
10^{-18}\)</span>. Since our <span class="math inline">\(y_i\)</span> are exchangeable, this holds true for any sequences of <span class="math inline">\(y_i\)</span> with 118 ones and 11 zeros.</p>
<p>So, finally, the posterior probability is</p>
<p><span class="math display">\[\begin{align}
p(\theta \mid y) &amp;= \frac{\Gamma(131)}{\Gamma(119) \Gamma(12)} \theta^{118} (1 -
\theta)^11 \\
&amp;= \frac{\Gamma(131)}{\Gamma(119) \Gamma(12)} \theta^{119 - 1} (1 - \theta)^{12 - 1} &amp; (\text{Beta parameterization})\\
\end{align}\]</span></p>
<p>which happens to be a <em>beta distribution</em> with parameters <span class="math inline">\(a = 119\)</span> and <span class="math inline">\(b = 12\)</span>.</p>
<p>If <span class="math inline">\(Y \sim \text{Beta}(a, b)\)</span>, then</p>
<ul>
<li>PDF: <span class="math inline">\(p(y) = \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)} y^{a - 1} (1 - y)^{b - 1}\)</span></li>
<li><span class="math inline">\(\mathbb{E}(Y) = \frac{a}{a + b}\)</span></li>
<li><span class="math inline">\(\text{Mode}(Y) = \frac{a - 1}{a + b - 2}\)</span></li>
<li><span class="math inline">\(\text{Var}(Y) = \frac{ab}{(a + b)^2 (a + b + 1)}\)</span></li>
</ul>
<p>In our case, our posterior looks like:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="3_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="inference-for-exchangeable-binary-data-i.e.-more-generally" class="level2">
<h2 class="anchored" data-anchor-id="inference-for-exchangeable-binary-data-i.e.-more-generally">Inference for exchangeable binary data (i.e.&nbsp;more generally)</h2>
<p>Recall for our binary data <span class="math inline">\(Y_1, \dots, Y_n\)</span> that</p>
<p><span class="math display">\[\begin{align}
p(\theta \mid y) &amp;= \frac{p(y \mid \theta) p(\theta)}{p(y)} \\
&amp;= \frac{\theta^{\sum y_i} (1 - \theta)^{n - \sum y_i} p(\theta)}{p(y)} &amp; \text{(Since i.i.d.)} \\
\end{align}\]</span></p>
<p>Importantly, the quantity <span class="math inline">\(\sum_{i = 1}^n Y_i\)</span> is the only statistic that is needed to calculate posterior probabilities of <span class="math inline">\(\theta\)</span>. So it is a <em>sufficient statistic</em> for making inference about <span class="math inline">\(\theta\)</span>. The statistic <span class="math inline">\(Y = \sum Y_i\)</span> has a binomial distribution with parameters <span class="math inline">\((n, \theta)\)</span>. Then,</p>
<p><span class="math display">\[\begin{align}
p(y \mid \theta) &amp;= {n \choose y} \theta^y (1 - \theta)^{n - y}
\end{align}\]</span></p>
<blockquote class="blockquote">
<p>Note: I skip the general derivation of posterior probabilities for uniform prior here, since a uniform prior is also a <span class="math inline">\(\text{Beta}(a, b)\)</span>.</p>
</blockquote>
<p>Now let’s calculate the posterior probability <span class="math inline">\(p(\theta \mid y)\)</span> when <span class="math inline">\(p(\theta)\)</span> is <em>not</em> uniform; in particular, when our prior on <span class="math inline">\(\theta\)</span> is a Beta distribution (<span class="math inline">\(\theta \sim \text{Beta}(a,
b)\)</span>):</p>
<p><span class="math display">\[\begin{align}
p(\theta \mid y) &amp;= \frac{p(\theta) p(y \mid \theta)}{p(y)} &amp; \\
&amp;= \frac{1}{p(y)} \times \underbrace{\frac{\Gamma(a + b)}{\Gamma(a)
\Gamma(b)}\theta^{a - 1}(1 - \theta)^{b - 1}}_{\text{PDF of $\text{Beta}(a,
b)$}} \times \underbrace{{n \choose y} \theta^y (1 - \theta)^{n -
y}}_{\text{$p(y \mid \theta)$ above}} &amp; \\
&amp;= c \times \theta^{a + y - 1} (1 - \theta)^{b + n - y - 1} &amp; \text{(Combine $\theta$s)} \
\end{align}\]</span>\end{align}</p>
<p>where <span class="math inline">\(c = f(n, y, a, b)\)</span> is just compressing the other stuff in the equation into a constant, since it doesn’t depend on <span class="math inline">\(\theta\)</span>. Now, notice that the term with <span class="math inline">\(\Gamma\)</span>s in the above equation is the PDF of <span class="math inline">\(\theta \sim \text{Beta}(a, b)\)</span>, which can also be expressed with a constant <span class="math inline">\(c = f(a, b)\)</span>: <span class="math inline">\(p(\theta) = c \theta^{a - 1}(1
- \theta)^{b - 1}\)</span>. Notice that this looks just like the equation above: thus, <span class="math inline">\(p(\theta)\)</span> and <span class="math inline">\(p(\theta \mid y)\)</span> are both proportional to <span class="math inline">\(\theta^{a - 1} (1 -
\theta)^{b - 1}\)</span> by some constant <span class="math inline">\(c\)</span>.</p>
<p>Now, since we know that as probability distributions, <span class="math inline">\(\int p(\theta) = \int
p(\theta \mid y) \; d\theta = 1\)</span>, we also know that the functions share the same <em>scale</em>, which means that <span class="math inline">\(p(\theta \mid y)\)</span> is actually a Beta PDF!</p>
<p><span class="math display">\[
(\theta \mid y) \sim \text{Beta}(a + y, b + n - y).
\]</span></p>
<p>We therefore call Beta distributions <strong>conjugate</strong> priors for Binomial distributions.</p>
<section id="conjugacy" class="level3">
<h3 class="anchored" data-anchor-id="conjugacy">Conjugacy</h3>
<blockquote class="blockquote">
<p><strong>Conjugacy</strong>. A class <span class="math inline">\(\mathcal{P}\)</span> of prior distributions for <span class="math inline">\(\theta\)</span> are <em>conjugate priors</em> of a sampling model <span class="math inline">\(p(y \mid \theta)\)</span> if <span class="math display">\[ p(\theta) \in
\mathcal{P} \implies p(\theta \mid y) \in \mathcal{P}.\]</span></p>
</blockquote>
<p>Conjugate priors are very convenient and making certain calculations easy, since there is some convenient closed form solution for the distribution of a model posterior given a model prior and observed data.</p>
<p>Now, if you remember from Chatper 1, since the expectation of a Beta distribution is <span class="math inline">\(\frac{a}{a + b}\)</span>,</p>
<p><span class="math display">\[\begin{align}
\mathbb{E}(\theta \mid y) &amp;= \frac{a + y}{a + b + n} \\
&amp;= \frac{a + b}{a + b + n} \frac{a}{a + b} + \frac{n}{a + b + n}\frac{y}{n} \\
\end{align}\]</span></p>
<p>Where <span class="math inline">\(\theta_0 = \frac{a}{a + b}\)</span> can be seen as our “prior expectation”, <span class="math inline">\(\frac{y}{n}\)</span> is the sample mean, and <span class="math inline">\(w = a + b\)</span> can be seen as the strength of belief in our prior. I leave the equation expressed above with the <span class="math inline">\(a\)</span>s and <span class="math inline">\(b\)</span>s expanded because another intuitive way of thinking about the problem is by thinking of <span class="math inline">\(a\)</span> as the “prior number of 1s”, <span class="math inline">\(b\)</span> as the “prior number of 0s”, and <span class="math inline">\(a + b\)</span> as the “prior sample size”. If you play around with the equation above, seeing how the values change with various <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> prior choices, you’ll notice it has several nice properties that intuitively balance the expectation and strength of our prior with the amount of data we receive.</p>
</section>
<section id="prediction" class="level3">
<h3 class="anchored" data-anchor-id="prediction">Prediction</h3>
<p>Assume we’ve already seen data <span class="math inline">\(y_1, \dots y_n\)</span> and we want to predict value of the next observation <span class="math inline">\(\tilde{Y}\)</span>. Intuitively we should predict <span class="math inline">\(\tilde{Y} = 1\)</span> with probability equal to the expectation of <span class="math inline">\(\theta\)</span> given our data (according to the Bayesian framework). These calculations confirm this:</p>
<p><span class="math display">\[\begin{align}
p(\tilde{Y} = 1 \mid y) &amp;= \int p(\tilde{Y} = 1, \theta \mid y) \; d\theta &amp; \text{(LTP)} \\
&amp;= \int p(\tilde{Y} = 1 \mid \theta, y) p(\theta \mid y) \; d\theta &amp; \text{(Chain rule)} \\
&amp;= \int \theta p(\theta \mid y) \; d\theta &amp; \text{(Since $\tilde{Y}$ is binary)} \\
&amp;= \mathbb{E}(\theta \mid y).
\end{align}\]</span></p>
<p>Note that in this case the posterior predictive distribution is very easy to predict and is (as it can only possibly be) a Bernoulli distribution with a certain probability <span class="math inline">\(p\)</span>. When posterior distributions become more complicated, however, (e.g.&nbsp;Poisson model), their posterior predictive distributions may require more complex calculations and may result in a different family of distributions. Still later, we will show how to simulate posterior predictive distributions with Monte Carlo sampling.</p>
<p>Also note that using the expectation instead of the mode is nice, for examples where we have a uniform <span class="math inline">\(\text{Beta}(1, 1)\)</span> prior and we observe <span class="math inline">\(Y = 0\)</span>. Then,</p>
<p><span class="math display">\[\begin{align}
\mathbb{E}(\theta \mid Y = 0) &amp;= \frac{2}{2 + n}\frac{1}{2} + \frac{n}{2 +
n}\frac{y}{n} = \frac{1}{2 + n}\\
\theta_{MAP} &amp;= \frac{y}{n} = 0 \\
\end{align}\]</span></p>
<p>And clearly the expectation is more sensible as a predictor of future <span class="math inline">\(\tilde{Y}\)</span>. But <span class="math inline">\(\theta_{MAP}\)</span> is not as unreasonable when there is a non-uniform prior…</p>
</section>
</section>
<section id="confidence-regions" class="level2">
<h2 class="anchored" data-anchor-id="confidence-regions">Confidence regions</h2>
<p>See <a href="http://stats.stackexchange.com/questions/2272/whats-the-difference-between-a-confidence-interval-and-a-credible-interval">this StackExchange question</a> for a really interesting discussiona bout the difference between Bayesian and frequentist confidence intervals.</p>
<p>Bayesian confidence interval is an interval <span class="math inline">\([l(y), u(y)]\)</span> with the following property:</p>
<p><span class="math display">\[\begin{align}
P(l(y) &lt; \theta &lt; u(y) \mid Y = y) = .95
\end{align}\]</span></p>
<p>Intuitively, a Bayesian confidence interval quantifies our uncertainty about the true value of the parameter we are estimating.</p>
<p>Frequentist confidence interval is an interval <span class="math inline">\([l(Y), u(Y)]\)</span> with the following property:</p>
<p><span class="math display">\[\begin{align}
P(l(Y) &lt; \theta &lt; u(Y) \mid theta) = .95
\end{align}\]</span></p>
<p>Intuitively, a Frequentist confidence interval quantifies our uncertainty about the measurement we have made of the parameter with a <em>fixed</em> true value.</p>
<p>Which interval is better is of course debated heavily.</p>
<p>To construct Bayesian confidence intervals, an easy way of doing so is to select quantiles of the posterior probability distribution such that the area in the interval under the curve is <span class="math inline">\(1 - \alpha\)</span>, where <span class="math inline">\(\alpha\)</span> is the desired confidence level:</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Uninformative prior, observe 10 variables with 2 1s</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>quantiles <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">q =</span> <span class="fu">qbeta</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), a <span class="sc">+</span> y, b <span class="sc">+</span> n <span class="sc">-</span> y))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.001</span>),</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">p =</span> <span class="fu">dbeta</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.001</span>), a <span class="sc">+</span> y, b <span class="sc">+</span> n <span class="sc">-</span> y)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> p)) <span class="sc">+</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">data =</span> quantiles, <span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">xintercept =</span> q))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="3_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>However, since some of the values for theta <em>outside</em> of the interval have higher density than values of theta <em>inside</em> the interval, another option is to force “symmetry” of heights by searching for the <strong>highest posterior density</strong> region, which can be intuitively found by drawing a horizontal line down the density until the region contained by the line is <span class="math inline">\(1 - \alpha\)</span>% of the entire curve. It’s not entirely clear to me how to calculate these analytically; computationally, using a discretized density, you can use a “trial and error” approach or a neat procedure detailed in Exercise 4.7 (c).</p>
</section>
</section>
<section id="the-poisson-model" class="level1">
<h1>The Poisson model</h1>
<p>This is for measurements that have values that are whole numbers, rather than just 1 or 0 (example: number of children).</p>
<p>Recall the PDF of a Poisson distribution - if <span class="math inline">\(Y \sim \text{Poisson}(\theta)\)</span> then</p>
<ul>
<li><span class="math inline">\(p(Y = y \mid \theta) = \theta^y e^{-\theta} / y!\)</span></li>
<li><span class="math inline">\(\mathbb{E}(Y) = \text{Var}(Y) = \theta\)</span></li>
</ul>
<section id="posterior-inference" class="level2">
<h2 class="anchored" data-anchor-id="posterior-inference">Posterior inference</h2>
<p>Just like how <span class="math inline">\(Y = \sum Y_i\)</span> is a sufficient statistic for <span class="math inline">\(n\)</span> independent Bernoulli trials, there exists a sufficient statistic for a sample of <span class="math inline">\(n\)</span> i.i.d. Poisson variables. Notice</p>
<p><span class="math display">\[\begin{align}
P(Y_1 = y_1, \dots, Y_n = y_n \mid \theta) &amp;=
\prod_i p(y_i \mid \theta) \\
&amp;= \prod_i \frac{\theta^{y_i} e^{-\theta}}{y_i !} \\
&amp;= \theta^{\sum_i y_i} e^{-n \theta} \prod_i \frac{1}{y_i !} &amp; \\
\end{align}\]</span></p>
<p>When we compare two values of <span class="math inline">\(\theta\)</span>,</p>
<p><span class="math display">\[\begin{align}
\frac{p(\theta_a \mid y_1, \dots, y_n)}{p(\theta_b \mid y_1, \dots, y_n)} &amp;= \dots
\end{align}\]</span></p>
<p>notice that the <span class="math inline">\(\prod_i 1 / y_i !\)</span> term is the same in that ratio, and thus cancels out. Then the <span class="math inline">\(\theta_{a, b}^{\sum_i y_i}\)</span> terms are the only terms remaining in the fraction, and thus <span class="math inline">\(\sum_i y_i\)</span> is a sufficient statistic. This sufficient statistic is a little more interesting than the Bernoulli case - consider that it doesn’t matter what the individual values of <span class="math inline">\(y_i\)</span> are (if one is very large, one is very small, or they all look the same) - inference on <span class="math inline">\(\theta\)</span> is possible with just the aggregate sum.</p>
<section id="conjugate-prior" class="level3">
<h3 class="anchored" data-anchor-id="conjugate-prior">Conjugate prior</h3>
<p>Using Bayes’ rule and the sampling model above, we have</p>
<p><span class="math display">\[\begin{align}
p(\theta \mid y) &amp;= \frac{p(\theta)p(y \mid \theta)}{p(y)} \\
&amp;\propto p(\theta) p(y \mid \theta) \\
&amp;\propto p(\theta) \times \theta^{\sum y_i} e^{-n\theta} \\
\end{align}\]</span></p>
<p>We are looking for some distribution of <span class="math inline">\(p(\theta)\)</span> that makes the posterior as calculated using the above the same distribution as <span class="math inline">\(p(\theta)\)</span> itself. That family of distributions is the Gamma family. If <span class="math inline">\(\theta \sim \text{Gamma}(a, b)\)</span>, then</p>
<ul>
<li><span class="math inline">\(p(\theta) = \frac{b^a}{\Gamma(a)}\theta^{a - 1} e^{-b \theta}\)</span></li>
<li><span class="math inline">\(\mathbb{E}(\theta) = a/b\)</span></li>
<li><span class="math inline">\(\text{Var}(\theta) = a/b^2\)</span></li>
</ul>
<p>To prove conjugacy, we have</p>
<p><span class="math display">\[\begin{align}
p(\theta \mid y) &amp;= \frac{p(\theta) p(y \mid \theta)}{p(y)} \\
&amp;= \underbrace{\left( \frac{b^a}{\Gamma(a)} \theta^{a - 1} e^{-b \theta}
\right)}_{p(\theta)} \underbrace{\left( \theta^{\sum y_i} e^{-n \theta} \prod_i
\frac{1}{y_i !} \right)}_{p(y \mid \theta)} \left( \frac{1}{p(y)} \right) \\
&amp;= c \times \left( \theta^{a - 1 + \sum y_i} e^{-(b + n) \theta} \right)
\end{align}\]</span></p>
<p>Where <span class="math inline">\(c = f(y, a, b)\)</span> is throwing all of the stuff that doesn’t depend on <span class="math inline">\(\theta\)</span> into some normalizing constant such that <span class="math inline">\(\int p(\theta \mid y) \; d\theta = 1\)</span>. Like the Binomial model, we recognize from the proportionality to <span class="math inline">\(\theta^{a - 1} e^{-b \theta}\)</span> that the posterior distribution of <span class="math inline">\(\theta\)</span> is Gamma distributed. Specifically</p>
<p><span class="math display">\[\begin{align}
\theta \mid y_1, \dots, y_n \sim \text{Gamma}(a + \sum_i y_i, b + n)
\end{align}\]</span></p>
<p>and the expectation is</p>
<p><span class="math display">\[\begin{align}
\mathbb{E}(\theta \mid y_1, \dots, y_n) &amp;= \frac{a + \sum y_i}{b + n} \\
&amp;= \frac{b}{b + n} \frac{a}{b} + \frac{n}{b + n}\frac{\sum y_i}{n} \\
\end{align}\]</span></p>
</section>
<section id="posterior-predictive-distribution" class="level3">
<h3 class="anchored" data-anchor-id="posterior-predictive-distribution">Posterior predictive distribution</h3>
<p><span class="math display">\[\begin{align}
p(\tilde{y} \mid y_1, \dots, y_n) &amp;= \int_0^{\infty} p(\tilde{y} \mid \theta, y_1, \dots, y_n) p(\theta \mid y_1, \dots, y_n) \; d\theta &amp; \text{Marginalization} \\
&amp;= \int_0^{\infty} p(\tilde{y} \mid \theta) p(\theta \mid y_1, \dots, y_n) \; d\theta &amp; \text{Since $\tilde{y}$ and $y_i$s c.i. given $\theta$} \\
&amp;= \int_0^{\infty} \left[ \frac{\theta^{\tilde{y}} e^{-\theta}}{\tilde{y}!} \right] \times \left[ \frac{(b + n)^{a + \sum y_i}}{\Gamma(a + \sum y_i)} \theta^{a + \sum y_i - 1} e^{-(b + n)\theta} \right] \; d\theta \\
&amp;= \int_0^{\infty} \left[ \frac{\theta^{\tilde{y} + a + \sum y_i - 1} e^{-(b + n + 1)\theta}}{\tilde{y}!} \right] \times \left[ \frac{(b + n)^{a + \sum y_i}}{\Gamma(a + \sum y_i)} \right] \; d\theta &amp; \text{Combine $\theta$s, $e$s} \\
&amp;= \frac{(b + n)^{a + \sum y_i}}{\Gamma(\tilde{y} + 1) \Gamma(a + \sum y_i)} \int_0^{\infty} \theta^{\tilde{y} + a + \sum y_i - 1} e^{-(b + n + 1)\theta} \; d\theta \\
\end{align}\]</span></p>
<p>Notice that the integrand is proportional to a Gamma density:</p>
<p><span class="math display">\[\begin{align}
&amp; \int_0^{\infty} \frac{b^a}{\Gamma(a)} \theta^{a - 1} e^{-b \theta} \; d\theta = 1\\
\implies&amp; \int_0^{\infty} \theta^{a - 1} e^{-b\theta} \; d\theta = \frac{\Gamma(a)}{b^a} \\
\end{align}\]</span></p>
<p>So</p>
<p><span class="math display">\[\begin{align}
p(\tilde{y} \mid y_1, \dots, y_n) &amp;= \dots \\
&amp;= \left( \frac{(b + n)^{a + \sum y_i}}{\Gamma(\tilde{y} + 1) \Gamma(a + \sum y_i)} \right) \left( \frac{\Gamma(\tilde{y} + a + \sum y_i)}{(b + n + 1)^{\tilde{y} + a + \sum y_i}} \right) \\
&amp;= \left( \frac{\Gamma(\tilde{y} + a + \sum y_i)}{\Gamma(\tilde{y} + 1) \Gamma(a + \sum y_i)} \right) \left( \frac{(b + n)^{a + \sum y_i}}{(b + n + 1)^{\tilde{y} + a + \sum y_i}} \right) &amp; \text{Swapping numerators} \\
&amp;= \left( \frac{\Gamma(\tilde{y} + a + \sum y_i)}{\Gamma(\tilde{y} + 1) \Gamma(a + \sum y_i)} \right) \left( \frac{b + n}{b + n + 1} \right)^{a + \sum y_i} \left( \frac{1}{b + n + 1} \right)^{\tilde{y}} \\
&amp;= \text{dnbinom}(\tilde{y}, a + \sum y_i, b + n) \\
\end{align}\]</span></p>
<p>The negative binomial distribution here, whose parameters look just like the Gamma posterior on theta, can be thought of as a predictive Poisson distribution with increased variance owing to the increased uncertainty on the value of <span class="math inline">\(\theta\)</span>. Notice</p>
<p><span class="math display">\[\begin{align}
\mathbb{E}(\tilde{Y} \mid y_1, \dots, y_n) &amp;= \frac{a + \sum y_i}{b + n} = \mathbb{E}(\theta \mid y_1, \dots, y_n) \\
\text{Var}(\tilde{Y} \mid y_1, \dots, y_n) &amp;= \frac{a + \sum y_i}{b + n} \frac{b
+ n + 1}{b + n} = \mathbb{E}(\theta \mid y_1, \dots, y_n) \times \frac{b + n + 1}{b + n}
\\
\end{align}\]</span></p>
<p>So as <span class="math inline">\(n\)</span> grows large, the variance on <span class="math inline">\(\tilde{Y}\)</span> approaches the variance of its expectation <span class="math inline">\(\mathbb{E}(\tilde{Y})\)</span>.</p>
</section>
</section>
<section id="example-birth-rates" class="level2">
<h2 class="anchored" data-anchor-id="example-birth-rates">Example: Birth Rates</h2>
<p>In the 1990s, did women with college degrees have different numbers of children than women without college degrees? We sample <span class="math inline">\(n_1\)</span> women without college degrees, denoted <span class="math inline">\(Y_{1,1}, \dots, Y_{n_1, 1}\)</span>, and <span class="math inline">\(n_2\)</span> women with college degrees, <span class="math inline">\(Y_{1, 2}, \dots, Y_{n_2, 2}\)</span>. For some parameter <span class="math inline">\(\theta\)</span> we can model the number of children for each woman as being i.i.d <span class="math inline">\(\text{Poisson}(\theta_1)\)</span> and <span class="math inline">\(\text{Poisson}(\theta_2)\)</span>, respectively. We may be interested in conducting hypothesis tests to see whether or not these <span class="math inline">\(\theta\)</span> are different.</p>
<p>Recall that, in this two samples, the <em>sufficient</em> statistic is simply the sum of all of the <span class="math inline">\(Y_i\)</span>. Say we observe</p>
<ul>
<li>No college: <span class="math inline">\(n_1 = 111\)</span>, <span class="math inline">\(\sum Y_i = 217\)</span>, <span class="math inline">\(\sum Y_i / n_1 = 1.95\)</span> (this is just useful for intuition)</li>
<li>College: <span class="math inline">\(n_2 = 44\)</span>, <span class="math inline">\(\sum Y_i = 66\)</span>, <span class="math inline">\(\sum Y_i / n_2 = 1.50\)</span></li>
</ul>
<p>Say our prior on both <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\text{Gamma(a = 2, 1)}\)</span>, which is lightly centered on ~1. You can toy around with choices of different priors by changing <code>a</code> and <code>b</code> below:</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>n1 <span class="ot">=</span> <span class="dv">111</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>sy1 <span class="ot">=</span> <span class="dv">217</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>n2 <span class="ot">=</span> <span class="dv">44</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>sy2 <span class="ot">=</span> <span class="dv">66</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.01</span>),</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">prior =</span> <span class="fu">dgamma</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.01</span>), a, b),</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">pos.theta1 =</span> <span class="fu">dgamma</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.01</span>), a <span class="sc">+</span> sy1, b <span class="sc">+</span> n1),</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">pos.theta2 =</span> <span class="fu">dgamma</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.01</span>), a <span class="sc">+</span> sy2, b <span class="sc">+</span> n2)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>df.long <span class="ot">=</span> <span class="fu">melt</span>(df, <span class="at">id.vars =</span> <span class="st">'theta'</span>, <span class="at">variable_name =</span> <span class="st">'dist'</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df.long, <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> value, <span class="at">group =</span> dist, <span class="at">color =</span> dist)) <span class="sc">+</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">'probability'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="3_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Notice that we can calculate the probability that <span class="math inline">\(\theta_1 &gt; \theta_2\)</span> by integrating over the joint density <span class="math inline">\(p(\theta_1, \theta_2)\)</span> over the region where <span class="math inline">\(\theta_1 &gt; \theta_2\)</span>. This is calculated at the beginning of Chapter 4, and it is about 0.97.</p>
</section>
</section>
<section id="exponential-families-and-conjugate-priors" class="level1">
<h1>Exponential families and conjugate priors</h1>
<p>Binomial and Poisson models are <em>exponential family models</em>.</p>
<blockquote class="blockquote">
<p><strong>Exponential family model</strong>: a model whose densities can be expressed as <span class="math display">\[p(y \mid \phi) = h(y)c(\phi)e^{\phi t(y)}\]</span> where <span class="math inline">\(\phi\)</span> is the unknown parameter and <span class="math inline">\(t(y)\)</span> is the sufficient statistic.</p>
</blockquote>
<p>Diaconis and Ylvisaker (1979) showed that the above class of models has conjugate prior densities</p>
<p><span class="math display">\[p(\phi \mid n_0, t_0) = \kappa (n_0, t_0) c(\phi)^{n_0} e^{n_0 t_0 \phi}\]</span></p>
<p>Where the posterior density is therefore</p>
<p><span class="math display">\[\begin{align}
p(\phi \mid y_1, \dots, y_n) &amp;\propto p(\phi) \times p(y_1, \dots, y_n \mid \phi) \\

&amp;\propto \left[ \kappa (n_0, t_0) c(\phi)^{n_0} \text{exp}(n_0 t_0 \phi) \right] \times \left[ \prod_{i = 1}^n \left( h(y_i) c(\phi) \text{exp}(\phi t(y)) \right) \right] \\
&amp;\propto \left[ \kappa (n_0, t_0) c(\phi)^{n_0} \text{exp}(n_0 t_0 \phi) \right] \times \left[ c(\phi)^n \text{exp}\left(\phi \sum_{i = 1}^n t(y_i)\right) \prod_{i = 1}^n h(y_i) \right] \\
&amp;\propto \left[ c(\phi)^{n_0} \text{exp}(n_0 t_0 \phi) \right] \times \left[ c(\phi)^n \text{exp}\left(\phi \sum_{i = 1}^n t(y_i)\right) \right] &amp; \text{Discard constants} \\
&amp;\propto c(\phi)^{n_0 + n} \text{exp}\left(\phi \times \left[n_0 t_0 + \sum_{i = 1}^n t(y_i) \right] \right) \\
&amp;\propto p\left( \phi \; \middle| \; n_0 + n, \; n_0 t_0 + \sum_{i = 1}^n t(y_i) \right)
\end{align}\]</span></p>
<p>For this class of priors, we can interpret <span class="math inline">\(n_0\)</span> as a “prior sample size” and <span class="math inline">\(t_0\)</span> is the prior expectation of the sufficient statistic <span class="math inline">\(t(Y)\)</span>.</p>
<section id="example-binomial-model" class="level2">
<h2 class="anchored" data-anchor-id="example-binomial-model">Example: Binomial model</h2>
<section id="parameterization" class="level3">
<h3 class="anchored" data-anchor-id="parameterization">Parameterization</h3>
<p>We can obtain the representation from a single binary random variable (why?)</p>
<p><span class="math display">\[p(y \mid \theta) = \theta^{y} (1 - \theta)^{1 - y}\]</span></p>
<p>We can express this as an exponential family model by reparameterizing by the log odds <span class="math inline">\(\phi = \log \frac{\theta}{1 - \theta}\)</span>, so that <span class="math inline">\(\theta = \frac{e^\phi}{e^\phi + 1}\)</span>:</p>
<p><span class="math display">\[\begin{align}
p(y \mid \phi) &amp;= \left( \frac{e^\phi}{e^\phi + 1} \right)^y \left(\frac{1}{e^\phi + 1} \right)^{1 - y} \\
&amp;= \frac{e^{\phi y}}{(e^\phi + 1)^y} \frac{1}{e^\phi + 1} \left( e^\phi + 1\right)^y \\
&amp;= e^{\phi y} (1 + e^\phi)^{-1} \\
\end{align}\]</span></p>
<p>Here,</p>
<ul>
<li><span class="math inline">\(h(y) = 1\)</span></li>
<li><span class="math inline">\(c(\phi) = (1 + e^\phi)^{-1}\)</span></li>
<li><span class="math inline">\(t(y) = y\)</span></li>
</ul>
<p>(It’s intuitive that the sufficient statistic is <span class="math inline">\(y\)</span>)</p>
</section>
<section id="prior" class="level3">
<h3 class="anchored" data-anchor-id="prior">Prior</h3>
<p>The conjugate prior on <span class="math inline">\(\phi\)</span> (discarding the constant <span class="math inline">\(\kappa\)</span>) is</p>
<p><span class="math display">\[\begin{align}
p(\phi \mid n_0, t_0) &amp;\propto (1 + e^{\phi})^{-n_0} e^{n_0 t_0 \phi}
\end{align}\]</span></p>
<p>To return to a density on <span class="math inline">\(\theta\)</span>, let <span class="math inline">\(\theta = g(\phi) = \frac{e^\phi}{e^\phi
+ 1}\)</span> and <span class="math inline">\(\phi = h(\theta) = \log \frac{\theta}{1 - \theta}\)</span>. Then, using the change of variables formula (Exercise 3.10),</p>
<p><span class="math display">\[\begin{align}
p_{\theta}(\theta \mid n_0, t_0) &amp;= p_{\phi}(h(\theta) \mid n_0, t_0) \times \left| \frac{dh}{d\theta} \right| \\
&amp;\propto \left(1 + \text{exp}\left(\log \left(\frac{\theta}{1 - \theta}\right)\right)\right)^{-n_0} \text{exp}\left(n_0 t_0 \log \left(\frac{\theta}{1 - \theta}\right) \right) \times \frac{1}{\theta - \theta^2} \\
&amp;\propto \left(1 + \frac{\theta}{1 - \theta} \right)^{-n_0} \left(\frac{\theta}{1 - \theta}\right)^{n_0 t_0} \times \frac{1}{\theta - \theta^2} \\
&amp;\propto \left(\frac{1}{1 - \theta} \right)^{-n_0} \left(\frac{\theta}{1 - \theta}\right)^{n_0 t_0} \times \frac{1}{\theta (1 - \theta)} \\
&amp;\propto (1 - \theta)^{n_0} \theta^{n_0 t_0} (1 - \theta)^{-n_0 t_0} \theta^{-1} (1 - \theta)^{-1} \\
&amp;\propto \theta^{n_0 t_0 - 1} (1 - \theta)^{n_0 - n_0t_0 - 1} \\
&amp;\propto \theta^{n_0 t_0 - 1} (1 - \theta)^{n_0 (1 - t_0) - 1} \\
&amp;= \text{dbeta}(\theta, n_0 t_0, n_0 (1 - t_0))
\end{align}\]</span></p>
<p>So the posterior is</p>
<p><span class="math display">\[\begin{align}
p(\theta \mid y_1, \dots, y_n) &amp;= \text{dbeta}\left(\theta,\; (n_0 + n)\left(n_0 t_0 + \sum t(y_i) \right),\; (n_0 + n) \left(1 - n_0 t_0 - \sum t(y_i) \right) \right)
\end{align}\]</span></p>
</section>
</section>
<section id="example-poisson-model" class="level2">
<h2 class="anchored" data-anchor-id="example-poisson-model">Example: Poisson model</h2>
<section id="parameterization-1" class="level3">
<h3 class="anchored" data-anchor-id="parameterization-1">Parameterization</h3>
<p><span class="math display">\[\begin{align}
p(y \mid \theta) &amp;= \frac{1}{y!} \theta^{y} e^{-\theta} \\
&amp;= \frac{1}{y!} e^{y \log \theta} \text{exp}(-e^{\log \theta}) \\
&amp;= h(y) c(\phi) e^{\phi t(y)} \\
\end{align}\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\phi = \log \theta\)</span></li>
<li><span class="math inline">\(h(y) = \frac{1}{y!}\)</span></li>
<li><span class="math inline">\(t(y) = y\)</span></li>
<li><span class="math inline">\(c(\phi) = \text{exp}(-e^\phi)\)</span> (The book has a typo here).</li>
</ul>
</section>
<section id="prior-1" class="level3">
<h3 class="anchored" data-anchor-id="prior-1">Prior</h3>
<p>Then the prior is <span class="math inline">\(p(\phi \mid n_0, t_0) = \text{exp}(n_0 e^{-\phi}) e^{n_0 t_0
\phi}\)</span>. For time, I will do change of variables to show how this induces a <span class="math inline">\(\text{Gamma}(n_0 t_0, n_0)\)</span> density on <span class="math inline">\(\theta\)</span>.</p>
</section>
</section>
</section>
<section id="exercises" class="level1">
<h1>Exercises</h1>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section">3.1</h2>
<section id="a" class="level3">
<h3 class="anchored" data-anchor-id="a">a</h3>
<p><span class="math display">\[\begin{align}
P(Y_1 = y_1, \dots, Y_{100} &amp;= y_100 \mid \theta) &amp;= \theta^{\sum y_i} (1 - \theta)^{100 - \sum y_i} \\
P(\sum Y_i = y \mid \theta) &amp;= {100 \choose y} \theta^{y}(1 - \theta)^{100 - y}
\end{align}\]</span></p>
</section>
<section id="b" class="level3">
<h3 class="anchored" data-anchor-id="b">b</h3>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Hand-implementing this, but you can use dbinom easily</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>my_dbinom <span class="ot">=</span> <span class="cf">function</span>(y, n, theta) <span class="fu">choose</span>(n, y) <span class="sc">*</span> theta<span class="sc">^</span>y <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> theta)<span class="sc">^</span>(n <span class="sc">-</span> y)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>theta.discrete <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> <span class="dv">57</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>ps <span class="ot">=</span> <span class="fu">sapply</span>(theta.discrete, <span class="cf">function</span>(theta) <span class="fu">my_dbinom</span>(Y, N, theta))</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">theta =</span> theta.discrete, <span class="at">p =</span> ps)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(df, <span class="dv">3</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>   theta     p
1    0.0 0.000
2    0.1 0.000
3    0.2 0.000
4    0.3 0.000
5    0.4 0.000
6    0.5 0.030
7    0.6 0.067
8    0.7 0.002
9    0.8 0.000
10   0.9 0.000
11   1.0 0.000</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> p)) <span class="sc">+</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">'identity'</span>) <span class="sc">+</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> theta.discrete)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="3_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="c" class="level3">
<h3 class="anchored" data-anchor-id="c">c</h3>
<p>If we have a uniform prior on beliefs of <span class="math inline">\(\theta \in \{0, 0.1, \dots, 1.0\}\)</span>, then <span class="math inline">\(P(\theta = 0.0) = P(\theta = 0.1) = \dots = 1/11\)</span>.</p>
<p><span class="math display">\[\begin{align}
p(\theta \mid \sum Y_i = 57) &amp;= \frac{p(\sum Y_i = 57 \mid \theta)p(\theta)}{p(\sum Y_i = 57)} \\
&amp;= \frac{p(\sum Y_i = 57 \mid \theta)p(\theta)}{\sum_{\theta'} p(\sum Y_i = 57 \mid \theta') p(\theta')}
\end{align}\]</span></p>
<p>Notice that <span class="math inline">\(p(\theta)\)</span> is constant for all <span class="math inline">\(\theta\)</span>, so it can be pulled out of the sum at the bottom and cancelled with the numerator:</p>
<p><span class="math display">\[\begin{align}
p(\theta \mid \sum Y_i = 57)
&amp;= \frac{p(\sum Y_i = 57 \mid \theta)}{\sum_{\theta'} p(\sum Y_i = 57 \mid \theta')} \\
&amp;\propto p(\sum Y_i = 57 \mid \theta)
\end{align}\]</span></p>
<p>since the denominator is the constant</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>denom <span class="ot">=</span> <span class="fu">sum</span>(ps)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(denom, <span class="dv">3</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.099</code></pre>
</div>
</div>
<p>So the posterior distribution has the same shape, but different scale.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>posteriors <span class="ot">=</span> <span class="fu">sapply</span>(theta.discrete, <span class="cf">function</span>(theta) <span class="fu">my_dbinom</span>(Y, N, theta) <span class="sc">/</span> denom)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">theta =</span> theta.discrete, <span class="at">p =</span> posteriors)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(df, <span class="dv">3</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>   theta     p
1    0.0 0.000
2    0.1 0.000
3    0.2 0.000
4    0.3 0.000
5    0.4 0.002
6    0.5 0.304
7    0.6 0.675
8    0.7 0.019
9    0.8 0.000
10   0.9 0.000
11   1.0 0.000</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> p)) <span class="sc">+</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">'identity'</span>) <span class="sc">+</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> theta.discrete)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="3_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Notice the denominator calculation could have been ignored - we could simply normalize the proportional prior densities <span class="math inline">\(p(\sum Y_i = 57 \mid \theta)\)</span> to 1.</p>
</section>
<section id="d" class="level3">
<h3 class="anchored" data-anchor-id="d">d</h3>
<p>Since <span class="math inline">\(p(\theta) = 1\)</span>, the density <span class="math inline">\(p(\theta) \times P(\sum Y_i = 57 \mid
\theta) = P(\sum Y_i = 57 \mid \theta)\)</span>. From (a),</p>
<p><span class="math display">\[
P(\sum Y_i = 57 \mid \theta) = {100 \choose 57} \theta^{57} (1 - \theta)^{43}
\]</span></p>
<p>which is implemented in the <code>my_dbinom</code> function. So</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>theta.continuous <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.001</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(theta.continuous, <span class="fu">sapply</span>(theta.continuous, <span class="cf">function</span>(theta) <span class="fu">my_dbinom</span>(Y, N, theta)),</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>      <span class="at">geom =</span> <span class="st">'line'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: `qplot()` was deprecated in ggplot2 3.4.0.</code></pre>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="3_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="e" class="level3">
<h3 class="anchored" data-anchor-id="e">e</h3>
<p>Treat the uniform prior as a <span class="math inline">\(\text{Beta}(1, 1)\)</span> distribution. Then <span class="math inline">\(\left(
\theta \mid \sum Y_i = y \right) \sim \text{Beta}(58, 44)\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(theta.continuous, <span class="fu">dbeta</span>(theta.continuous, <span class="dv">58</span>, <span class="dv">44</span>), <span class="at">geom =</span> <span class="st">'line'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="3_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<ol start="4" type="a">
<li>is the posterior density before normalization by the constant <span class="math inline">\(p(\sum Y_i = 57) = 0.099\)</span>. (e) is the posterior density fully, after normalization. Notice that (d) and (e) have the same shape, due to the lack of influence of <span class="math inline">\(p(\theta)\)</span> on posterior calculation.</li>
</ol>
</section>
</section>
<section id="section-1" class="level2">
<h2 class="anchored" data-anchor-id="section-1">3.2</h2>
<p>For consistency, I will rewrite these as done in Chapter 1, where <span class="math inline">\(\theta_0 = a
/ (a + b)\)</span> is the initial guess of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(w = a + b\)</span> is the strength of that guess. Then,</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># What is the expected value of theta after observing result y, given a Beta</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># prior parameterized by theta0 and w?</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>exp.posterior <span class="ot">=</span> <span class="cf">function</span>(w, theta0, y) {</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>  (N <span class="sc">/</span> (w <span class="sc">+</span> N)) <span class="sc">*</span> (y <span class="sc">/</span> N) <span class="sc">+</span> (w <span class="sc">/</span> (w <span class="sc">+</span> N)) <span class="sc">*</span> theta0</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>Theta0 <span class="ot">=</span> <span class="fu">rev</span>(<span class="fu">seq</span>(<span class="fl">0.0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.1</span>))</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>W <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">32</span>, <span class="at">by =</span> <span class="fl">0.5</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="dv">57</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">outer</span>(Theta0, W, <span class="at">FUN =</span> <span class="cf">function</span>(theta0, w) <span class="fu">exp.posterior</span>(w, theta0, <span class="dv">57</span>))</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(d) <span class="ot">=</span> Theta0</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(d) <span class="ot">=</span> W</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">melt</span>(d)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by
the caller; using TRUE
Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by
the caller; using TRUE</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(df) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">'theta0'</span>, <span class="st">'w'</span>, <span class="st">'theta'</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> w, <span class="at">y =</span> theta0, <span class="at">z =</span> theta)) <span class="sc">+</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_contour</span>(<span class="fu">aes</span>(<span class="at">colour =</span> ..level..)) <span class="sc">+</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>), <span class="at">labels =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>)) <span class="sc">+</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">breaks =</span> Theta0)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(directlabels)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="fu">direct.label</span>(p, <span class="at">method =</span> <span class="st">'bottom.pieces'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: `aes_string()` was deprecated in ggplot2 3.0.0.
ℹ Please use tidy evaluation idioms with `aes()`.
ℹ See also `vignette("ggplot2-in-packages")` for more information.
ℹ The deprecated feature was likely used in the directlabels package.
  Please report the issue at &lt;https://github.com/tdhock/directlabels/issues&gt;.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: The dot-dot notation (`..level..`) was deprecated in ggplot2 3.4.0.
ℹ Please use `after_stat(level)` instead.</code></pre>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="3_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>One can use this plot to determine whether or not they should believe that <span class="math inline">\(\theta &gt; 0.5\)</span> by quantifying their prior beliefs about the proportion with two factors: <span class="math inline">\(\theta_0\)</span>, an initial estimate of the true proportion, and <span class="math inline">\(w\)</span>, the “sample size” of observed individuals that contributed to the initial estimate. Then, viewing the corresponding contour on this plot would give an estimate of the posterior proportion given these two variables. It is shown here that <span class="math inline">\(\theta &gt; 0.5\)</span> for most prior beliefs except for those with relatively low and very strong estimates about <span class="math inline">\(\theta_0\)</span>.</p>
</section>
<section id="section-2" class="level2">
<h2 class="anchored" data-anchor-id="section-2">3.3</h2>
<section id="a-1" class="level3">
<h3 class="anchored" data-anchor-id="a-1">a</h3>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>ya <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">12</span>, <span class="dv">9</span>, <span class="dv">12</span>, <span class="dv">14</span>, <span class="dv">13</span>, <span class="dv">13</span>, <span class="dv">15</span>, <span class="dv">8</span>, <span class="dv">15</span>, <span class="dv">6</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>yb <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">11</span>, <span class="dv">11</span>, <span class="dv">10</span>, <span class="dv">9</span>, <span class="dv">9</span>, <span class="dv">8</span>, <span class="dv">7</span>, <span class="dv">10</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">7</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Notice that <span class="math inline">\(\sum y_a = 117, n_a = 10, \sum y_b = 113, n_b = 13\)</span>.</p>
<p>If</p>
<p><span class="math display">\[\begin{align}
\theta_A &amp;\sim \text{Gamma}(120, 10) \\
\theta_B &amp;\sim \text{Gamma}(12, 1)
\end{align}\]</span></p>
<p>then</p>
<p><span class="math display">\[\begin{align}
\theta_A \mid \mathbf{y}_a &amp;\sim \text{Gamma}(120 + 117, 10 + 10) = \text{Gamma}(237, 20) \\
\theta_B \mid \mathbf{y}_b &amp;\sim \text{Gamma}(12 + 113, 1 + 13) = \text{Gamma}(125, 14) \\
\end{align}\]</span></p>
<p>So</p>
<p><span class="math display">\[\begin{align}
\mathbb{E}(\theta_A) &amp;=  237/20 = 11.85\\
\mathbb{E}(\theta_A) &amp;= 125/14 = 8.92\\
\text{Var}(\theta_A) &amp;= 237/400 = 0.593\\
\text{Var}(\theta_B) &amp;= 125/196 = 0.638\\
\end{align}\]</span></p>
<p>95% quantile-based confidence intervals can be solved by setting the CDF of the Gammas to <span class="math inline">\(p\)</span>, and solving for <span class="math inline">\(\theta\)</span>. Alternatively,</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qgamma</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="dv">237</span>, <span class="dv">20</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 10.38924 13.40545</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qgamma</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="dv">125</span>, <span class="dv">14</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  7.432064 10.560308</code></pre>
</div>
</div>
</section>
<section id="b-1" class="level3">
<h3 class="anchored" data-anchor-id="b-1">b</h3>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>ya <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">12</span>, <span class="dv">9</span>, <span class="dv">12</span>, <span class="dv">14</span>, <span class="dv">13</span>, <span class="dv">13</span>, <span class="dv">15</span>, <span class="dv">8</span>, <span class="dv">15</span>, <span class="dv">6</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>yb <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">11</span>, <span class="dv">11</span>, <span class="dv">10</span>, <span class="dv">9</span>, <span class="dv">9</span>, <span class="dv">8</span>, <span class="dv">7</span>, <span class="dv">10</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">7</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>n0 <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>exps <span class="ot">=</span> (<span class="dv">12</span> <span class="sc">*</span> n0 <span class="sc">+</span> <span class="fu">sum</span>(yb)) <span class="sc">/</span> (n0 <span class="sc">+</span> <span class="fu">length</span>(yb))</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(n0, exps, <span class="at">geom =</span> <span class="fu">c</span>(<span class="st">'point'</span>, <span class="st">'smooth'</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="3_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Very strong prior beliefs that the expected value of <span class="math inline">\(\theta \approx 12\)</span> would be necessary, because <span class="math inline">\(\theta_{ML} = 8.69\)</span> which is quite low. According to the graph <span class="math inline">\(n_0\)</span> values of close to 50 are required.</p>
</section>
<section id="c-1" class="level3">
<h3 class="anchored" data-anchor-id="c-1">c</h3>
<p>We have existing knowledge about population A. Knowing that B is related, we have incorporated these beliefs into our prior on population B. However, nothing more than a weak prior expectation of B to be similar to A should be encoded in our analysis, since it is entirely possible that the parameter of B is quite different from A. So we should view the populations as independent.</p>
</section>
</section>
<section id="section-3" class="level2">
<h2 class="anchored" data-anchor-id="section-3">3.4</h2>
<section id="a-2" class="level3">
<h3 class="anchored" data-anchor-id="a-2">a</h3>
<p>This is fairly straightforward like 3.1, skipping</p>
</section>
<section id="b-2" class="level3">
<h3 class="anchored" data-anchor-id="b-2">b</h3>
<p>This is fairly straightforward like 3.1, skipping</p>
</section>
<section id="c-2" class="level3">
<h3 class="anchored" data-anchor-id="c-2">c</h3>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> <span class="cf">function</span>(theta) {</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">1</span><span class="sc">/</span><span class="dv">4</span>) <span class="sc">*</span> (<span class="fu">gamma</span>(<span class="dv">10</span>) <span class="sc">/</span> (<span class="fu">gamma</span>(<span class="dv">2</span>) <span class="sc">*</span> <span class="fu">gamma</span>(<span class="dv">8</span>))) <span class="sc">*</span> </span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">3</span> <span class="sc">*</span> theta <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> theta)<span class="sc">^</span><span class="dv">7</span> <span class="sc">+</span> theta<span class="sc">^</span><span class="dv">7</span> <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> theta))</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>thetas <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.005</span>)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(thetas, <span class="fu">prior</span>(thetas), <span class="at">geom =</span> <span class="st">'line'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="3_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Some kind of bimodal prior on rates of teen recidivism - for example, you believe that in some regions, teen recidivism is rather low, but there are some regions where teen recidivism is high.</p>
</section>
<section id="d-1" class="level3">
<h3 class="anchored" data-anchor-id="d-1">d</h3>
<section id="i" class="level4">
<h4 class="anchored" data-anchor-id="i">i</h4>
<p><span class="math display">\[\begin{align}
p(\theta) \times p(y \mid \theta) &amp;= \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2) \Gamma(8)} \left[ 3\theta (1 - \theta)^7 + \theta^7 (1 - \theta) \right] \times \left[ {43 \choose 15} \theta^{15} (1 - \theta)^{28} \right] \\
&amp;= \frac{1}{4} \frac{\Gamma(44)}{\Gamma(16) \Gamma(29)} \frac{\Gamma(10)}{\Gamma(2) \Gamma(8)} \left[3\theta^{16} (1 - \theta)^{35} + \theta^{22} (1 - \theta)^{29} \right] \\
\end{align}\]</span></p>
</section>
<section id="ii" class="level4">
<h4 class="anchored" data-anchor-id="ii">ii</h4>
<p>This is proportional to some mixture with unknown weights of a <span class="math inline">\(\text{Beta}(17,
36)\)</span> and a <span class="math inline">\(\text{Beta}(23, 30)\)</span> which intuitively are the posterior densities in parts a and b.</p>
</section>
<section id="iii" class="level4">
<h4 class="anchored" data-anchor-id="iii">iii</h4>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">=</span> <span class="cf">function</span>(theta) {</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">prior</span>(theta) <span class="sc">*</span> <span class="fu">choose</span>(<span class="dv">43</span>, <span class="dv">15</span>) <span class="sc">*</span> theta<span class="sc">^</span><span class="dv">15</span> <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> theta)<span class="sc">^</span><span class="dv">28</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(thetas, <span class="fu">posterior</span>(thetas), <span class="at">geom =</span> <span class="st">'line'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="3_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Mode:"</span>, thetas[<span class="fu">which.max</span>(<span class="fu">posterior</span>(thetas))], <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mode: 0.315 </code></pre>
</div>
</div>
<p>Notice that</p>
<p><span class="math display">\[\begin{align}
\text{mode}(\text{Beta}(17, 36)) &amp;= (17 - 1) / (17 + 36 - 2) = 0.313 \\
\text{mode}(\text{Beta}(23, 30)) &amp;= (23 - 1) / (23 + 30 - 2) = 0.431
\end{align}\]</span></p>
<p>So the mode is between these two modes, although closer to that of the <span class="math inline">\(\text{Beta}(17, 36)\)</span>.</p>
</section>
</section>
<section id="e-1" class="level3">
<h3 class="anchored" data-anchor-id="e-1">e</h3>
<p>Unclear: general formula for any beta mixture with any weights and parameters? Or general formula for any observed result for this model, given then prior in c)?</p>
</section>
</section>
<section id="section-4" class="level2">
<h2 class="anchored" data-anchor-id="section-4">3.5</h2>
<section id="a-3" class="level3">
<h3 class="anchored" data-anchor-id="a-3">a</h3>
<p>First,</p>
<p><span class="math display">\[\begin{align}
\tilde{p} &amp;= \sum_{k = 1}^K w_k p_k (\theta \mid n_{0, k}, t_{0, k}) \\
&amp;= \sum_{k = 1}^K \left( w_k \kappa (n_{0, k}, t_{0, k}) c(\phi)^{n_{0, k}} \text{exp}(n_{0, k} t_{0, k} \phi) \right) \\
\end{align}\]</span></p>
<p>Now</p>
<p><span class="math display">\[\begin{align}
p(\phi \mid y_1, \dots, y_n) &amp;\propto p(\phi) p(y_1, \dots, y_n \mid \phi) \\

&amp;\propto \left[ \sum_{k = 1}^K \left( w_k \kappa (n_{0, k}, t_{0, k}) c(\phi)^{n_{0, k}} \text{exp}(n_{0, k} t_{0, k} \phi) \right)  \right] \times \left[ \prod_{i = 1}^n h(y_i) c(\phi) \text{exp}(\phi t(y_i)) \right] \\
&amp;\propto \left[ \sum_{k = 1}^K \left( w_k \kappa (n_{0, k}, t_{0, k}) c(\phi)^{n_{0, k}} \text{exp}(n_{0, k} t_{0, k} \phi) \right)  \right] \times \left[ \text{exp}\left(\phi \sum_{i = 1}^n t(y_i)\right) c(\phi)^n \prod_{i = 1}^n h(y_i) \right] \\
&amp;\propto \left[ \sum_{k = 1}^K \left( w_k \kappa (n_{0, k}, t_{0, k}) c(\phi)^{n_{0, k}} \text{exp}(n_{0, k} t_{0, k} \phi) \right)  \right] \times \left[ \text{exp}\left(\phi \sum_{i = 1}^n t(y_i)\right) c(\phi)^n \right] \\
&amp;\propto \sum_{k = 1}^K \left( w_k \kappa (n_{0, k}, t_{0, k}) c(\phi)^{n_{0, k} + n} \text{exp}\left(\phi \times \left[ n_{0, k} t_{0, k}+ \sum_{i = 1}^n t(y_i) \right] \right) \right) \\
&amp;\propto \sum_{k = 1}^K w_k p\left(\theta \; \middle| \; n_0 + n, \; n_0 t_0 + \sum_{i = 1}^{n} t(y_i)\right)
\end{align}\]</span></p>
<p>So the posterior is another weighted mixture. However I don’t believe the weights of the relative components are preserved (neither are they in the Beta mixtures above).</p>
</section>
<section id="b-3" class="level3">
<h3 class="anchored" data-anchor-id="b-3">b</h3>
<p>Not specified whether we need to use the exponential family parameterization or the standard parameterization. I will use the standard parameterization.</p>
<p>Let <span class="math display">\[\begin{align}
\tilde{p} &amp;= \sum_{k = 1}^K w_k p_k (\theta \mid a_k, b_k) \\
&amp;= \sum_{k = 1}^K \left( w_k \frac{b_k^{a_k}}{\Gamma(a_k)} x^{a_k - 1} e^{-b_k x} \right) \\
\end{align}\]</span></p>
<p>Then <span class="math display">\[\begin{align}
p(\theta \mid y_1, \dots, y_n) &amp;\propto p(\theta) p(y_1, \dots, y_n \mid \theta) \\
&amp;\propto \left[ \sum_{k = 1}^K \left( w_k \frac{b_k^{a_k}}{\Gamma(a_k)} \theta^{a_k - 1} e^{-b_k \theta} \right) \right] \times \left[ \prod_{i = 1}^n \frac{1}{y_i !} \theta^{y_i} e^{-\theta} \right] \\
&amp;\propto \left[ \sum_{k = 1}^K \left( w_k \frac{b_k^{a_k}}{\Gamma(a_k)} \theta^{a_k - 1} e^{-b_k \theta} \right) \right] \times \left[ \theta^{\sum y_i} e^{-n\theta} \right] \\
&amp;\propto \sum_{k = 1}^K \left( w_k \frac{b_k^{a_k}}{\Gamma(a_k)} \theta^{a_k + \sum y_i - 1} e^{-(b_k + n)\theta} \right) \\
&amp;\propto \sum_{k = 1}^K w_k p\left(\theta \; \middle| \; a_k + \sum y_i, \; b_k + n \right) \\
\end{align}\]</span></p>
</section>
</section>
<section id="section-5" class="level2">
<h2 class="anchored" data-anchor-id="section-5">3.9</h2>
<section id="a-4" class="level3">
<h3 class="anchored" data-anchor-id="a-4">a</h3>
<p>We take advantage of the fact that the Galenshore distribution can be viewed as an exponential model</p>
<p><span class="math display">\[\begin{align}
p(y \mid \theta) &amp;= \frac{2}{\Gamma(a)} \theta^{2a} y^{2a - 1} e^{-\theta^2 y^2} \\
&amp;= \left( \frac{2}{\Gamma(a)} y^{2a - 1} \right) \times \left(\theta^{2a} \right) \times \left( e^{-\theta^2 y^2} \right) \\
&amp;= \left( \frac{2}{\Gamma(a)} y^{2a - 1} \right) \times \left(\theta^2 \right)^a \times \left( e^{\left(\theta^2\right) -y^2} \right) \\
&amp;= h(y) c(\phi) e^{\phi t(y)}
\end{align}\]</span></p>
<p>Where</p>
<ul>
<li><span class="math inline">\(h(y) = \frac{2}{\Gamma(a)} y^{2a - 1}\)</span></li>
<li><span class="math inline">\(c(\phi) = \phi^a\)</span></li>
<li><span class="math inline">\(t(y) = -(y^2)\)</span></li>
<li><span class="math inline">\(\phi = \theta^2\)</span></li>
</ul>
<p>Then, the conjugate priors for the <span class="math inline">\(\phi\)</span> parameterization are given by</p>
<p><span class="math display">\[\begin{align}
p(\phi \mid n_0, t_0) &amp;= \kappa (n_0, t_0) \phi^{a n_0} \text{exp}(n_0 t_0 \phi) \\
&amp;\propto \phi^{a n_0} \text{exp}(n_0 t_0 \phi) \\
\end{align}\]</span></p>
<p>To obtain the priors for <span class="math inline">\(\theta\)</span>, let <span class="math inline">\(\theta = g(\phi) = \sqrt{\phi}\)</span> and <span class="math inline">\(\phi = h(\theta) = \theta^2\)</span>. Notice <span class="math inline">\(dh/d\theta = 2\theta\)</span>. By the change of variables formula,</p>
<p><span class="math display">\[\begin{align}
p(\theta \mid n_0, t_0) &amp;= p(h(\theta) \mid n_0, t_0) \times \left| \frac{dh}{d\theta} \right| \\
&amp;\propto \kappa(n_0, t_0) \theta^{2a n_0} \text{exp}\left( n_0 t_0 \theta^2 \right) \times 2 \theta \\
&amp;\propto \theta^{2a n_0 + 1} \text{exp}\left( n_0 t_0 \theta^2 \right) \\
&amp;\propto \text{dgalenshore}\left(\theta, \underbrace{a n_0 + 1}_{a_{\text{Galenshore}}}, \underbrace{\sqrt{-n_0 t_0}}_{\theta_{\text{Galenshore}}} \right)
\end{align}\]</span></p>
<p>Notice this is true since <span class="math inline">\(\text{dgalenshore}(y, a, \theta) \propto y^{2a - 1}
e^{- \theta^2 y^2}\)</span> - the rest of the PDF is a constant that doesn’t depend on <span class="math inline">\(y\)</span>. Also notice that <span class="math inline">\(\sqrt{-n_0 t_0}\)</span> is defined since <span class="math inline">\(n_0 &gt; 0\)</span> and <span class="math inline">\(t_0\)</span> is the initial “guess” of <span class="math inline">\(t(y) = - (y^2) &lt; 0\)</span>, so <span class="math inline">\(-n_0 t_0 &gt; 0\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>dgalenshore <span class="ot">=</span> <span class="cf">function</span>(y, a, theta) {</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">2</span> <span class="sc">/</span> <span class="fu">gamma</span>(a)) <span class="sc">*</span> theta<span class="sc">^</span>(<span class="dv">2</span> <span class="sc">*</span> a) <span class="sc">*</span> y<span class="sc">^</span>(<span class="dv">2</span> <span class="sc">*</span> a <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="dv">1</span> <span class="sc">*</span> (theta<span class="sc">^</span><span class="dv">2</span>) <span class="sc">*</span> y<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">seq</span>(<span class="fl">0.02</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.02</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">rbind</span>(</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">y =</span> y, <span class="at">density =</span> <span class="fu">dgalenshore</span>(y, <span class="dv">1</span>, <span class="dv">1</span>), <span class="at">dist =</span> <span class="st">'alpha = 1, theta = 1'</span>),</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">y =</span> y, <span class="at">density =</span> <span class="fu">dgalenshore</span>(y, <span class="dv">1</span>, <span class="dv">3</span>), <span class="at">dist =</span> <span class="st">'alpha = 1, theta = 3'</span>),</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">y =</span> y, <span class="at">density =</span> <span class="fu">dgalenshore</span>(y, <span class="dv">3</span>, <span class="dv">1</span>), <span class="at">dist =</span> <span class="st">'alpha = 3, theta = 1'</span>),</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">y =</span> y, <span class="at">density =</span> <span class="fu">dgalenshore</span>(y, <span class="dv">4</span>, <span class="dv">2</span>), <span class="at">dist =</span> <span class="st">'alpha = 4, theta = 2'</span>)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> y, <span class="at">y =</span> density, <span class="at">group =</span> dist, <span class="at">color =</span> dist)) <span class="sc">+</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="3_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="b-4" class="level3">
<h3 class="anchored" data-anchor-id="b-4">b</h3>
<p>Since this is an exponential family, the posterior of <span class="math inline">\(\phi\)</span> is given by <span class="math inline">\(p(\phi \mid n_0 + n, n_0 t_0 + n\bar{t}(\mathbf{y}))\)</span>. This means that</p>
<p><span class="math display">\[\begin{align}
\theta \mid y_1, \dots, y_n &amp;\sim \text{Galenshore}\left(a (n_0 + n) + 1, \sqrt{- (n_0 + n) (n_0 t_0 + n \bar{t}(\mathbf{y}))} \right) \\
\end{align}\]</span></p>
</section>
<section id="c-3" class="level3">
<h3 class="anchored" data-anchor-id="c-3">c</h3>
<p>By taking advantage of the exponential family we already know <span class="math inline">\(t(y) = -(y^2)\)</span> is our sufficient statistic. Specifically when comparing multiple <span class="math inline">\(Y_1, \dots, Y_n\)</span>, we have <span class="math inline">\(\sum_{i = 1}^n
- (y^2)\)</span> as our sufficient statistic.</p>
<p>Note it was a technicality that I picked <span class="math inline">\(t(y) = -(y^2)\)</span>. I could have parameterized the exponential family such that <span class="math inline">\(t(y) = y^2\)</span>, and the negative is distributed in the other functions. Also notice that since <span class="math inline">\(y^2 &gt; 0\)</span>, the <span class="math inline">\(t(y)\)</span>s provide the same information.</p>
</section>
<section id="d-2" class="level3">
<h3 class="anchored" data-anchor-id="d-2">d</h3>
<p>From the formula for the expectation of a Galenshore distribution, if we have <span class="math display">\[\theta \mid y_1, \dots, y_n \sim \text{Galenshore}\left(a (n_0 + n) + 1, \sqrt{- (n_0 + n) (n_0 t_0 + n \bar{t}(\mathbf{y}))} \right)\]</span></p>
<p>then</p>
<p><span class="math display">\[\begin{align}
\mathbb{E}(\theta \mid y_1, \dots, y_n) = \frac{\Gamma\left(\frac{1}{2} a(n_0 + n) + 2 \right)}{ \sqrt{- (n_0 + n) (n_0 t_0 + n \bar{t}(\mathbf{y}))} \Gamma\left( a(n_0 + n) + 1 \right)}
\end{align}\]</span></p>
</section>
<section id="e-2" class="level3">
<h3 class="anchored" data-anchor-id="e-2">e</h3>
<p>This one looks tedious…</p>
</section>
</section>
<section id="section-6" class="level2">
<h2 class="anchored" data-anchor-id="section-6">3.10</h2>
<p>Change of variables</p>
<section id="a-5" class="level3">
<h3 class="anchored" data-anchor-id="a-5">a</h3>
<p>If <span class="math inline">\(\psi = g(\theta) = \log \frac{\theta}{1 - \theta}\)</span>, then let <span class="math inline">\(\theta = h(\psi) = \frac{e^\psi}{1 + e^\psi}\)</span>. Then, by the change of variables formula,</p>
<p><span class="math display">\[\begin{align}
p_{\psi}(\psi) &amp;= p_{\theta}(h(\psi)) \times \left| \frac{dh}{d\psi} \right| \\
&amp;= \left[ \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} \left( \frac{e^\psi}{1 + e^\psi}  \right)^{a - 1} \left( 1 - \frac{e^\psi}{1 + e^\psi} \right)^{b - 1} \right] \times \frac{e^\psi}{(e^\psi + 1)^2} \\
&amp;= \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)} \left[ \left( \frac{e^\psi}{1 + e^\psi}  \right)^{a - 1} \left( \frac{1}{1 + e^\psi} \right)^{b - 1} \right] \times \frac{e^\psi}{(e^\psi + 1)^2} \\
&amp;= \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)} \left[ \left( \frac{e^\psi}{1 + e^\psi}  \right)^a \left( \frac{1 + e^\psi}{e^\psi} \right) \left( \frac{1}{1 + e^\psi} \right)^b \left(\frac{1 + e^\psi}{1} \right) \right] \times \frac{e^\psi}{(e^\psi + 1)^2} \\
&amp;= \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)} \left[ \left( \frac{e^\psi}{1 + e^\psi}  \right)^a \left( \frac{1}{1 + e^\psi} \right)^b \frac{(e^\psi + 1)^2}{e^\psi} \right] \times \frac{e^\psi}{(e^\psi + 1)^2} \\
&amp;= \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)} \left( \frac{e^\psi}{1 + e^\psi}  \right)^a \left( \frac{1}{1 + e^\psi} \right)^b
\end{align}\]</span></p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>h <span class="ot">=</span> <span class="cf">function</span>(psi) {</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">exp</span>(psi) <span class="sc">/</span> (<span class="fu">exp</span>(psi) <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>dh <span class="ot">=</span> <span class="cf">function</span>(psi) {</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">exp</span>(psi) <span class="sc">/</span> (<span class="fu">exp</span>(psi) <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>dpsi <span class="ot">=</span> <span class="cf">function</span>(psi, a, b) {</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>  (<span class="fu">gamma</span>(a <span class="sc">+</span> b) <span class="sc">/</span> (<span class="fu">gamma</span>(a) <span class="sc">*</span> <span class="fu">gamma</span>(b))) <span class="sc">*</span> <span class="fu">h</span>(psi)<span class="sc">^</span>(a <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">h</span>(psi))<span class="sc">^</span>(b <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> <span class="fu">dh</span>(psi)</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="co"># To verify this is a valid PDF - with various a and b, integral should be approximately 1</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(<span class="cf">function</span>(p) <span class="fu">dpsi</span>(p, <span class="dv">2</span>, <span class="dv">2</span>), <span class="sc">-</span><span class="dv">100</span>, <span class="dv">100</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>1 with absolute error &lt; 9e-07</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(<span class="cf">function</span>(p) <span class="fu">dpsi</span>(p, <span class="dv">4</span>, <span class="dv">8</span>), <span class="sc">-</span><span class="dv">100</span>, <span class="dv">100</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>1 with absolute error &lt; 1.2e-08</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(<span class="cf">function</span>(p) <span class="fu">dpsi</span>(p, <span class="dv">10</span>, <span class="dv">1</span>), <span class="sc">-</span><span class="dv">100</span>, <span class="dv">100</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>1 with absolute error &lt; 5.6e-05</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>psi <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.05</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>density <span class="ot">=</span> <span class="fu">dpsi</span>(psi, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(psi, density, <span class="at">geom =</span> <span class="st">'line'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="3_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>This prior is also visualized via Monte-Carlo simulation in exercise 4.6. One perhaps counterintuitive result here is that for a Beta-Binomial model, an uninformative (uniform) prior on <span class="math inline">\(\theta\)</span> <em>does</em> result in an informative prior on the log-odds. This was apparently a “major criticism of Bayesian inference” led by R.A. Fisher. For more information about this (and an introduction into the Jeffreys’ prior exercises next up), see <a href="https://www2.stat.duke.edu/courses/Fall11/sta114/jeffreys.pdf">these lecture notes</a>.</p>
<p>Point: it is impossible to have a totally diffuse prior on a random variable with infinite support such as the log-odds, though…</p>
</section>
<section id="b-5" class="level3">
<h3 class="anchored" data-anchor-id="b-5">b</h3>
<p>If <span class="math inline">\(\psi = g(\theta) = \log \theta\)</span>, then let <span class="math inline">\(\theta = h(\psi) = e^\psi\)</span>. Then, by the change of variables formula,</p>
<p><span class="math display">\[\begin{align}
p_{\psi}(\psi) &amp;= p_{\theta}(h(\psi)) \times \left| \frac{dh}{d\psi} \right| \\
&amp;= \left[ \frac{b^a}{\Gamma(a)} \text{exp}\left(\psi (a - 1)\right) \text{exp}\left( -b e^\psi \right)  \right] \times \text{exp}\left( \psi \right) \\
&amp;= \frac{b^a}{\Gamma(a)} \text{exp}\left(a\psi - \psi - b e^\psi + \psi \right) \\
&amp;= \frac{b^a}{\Gamma(a)} \text{exp}\left(a\psi - \psi - b e^\psi + \psi \right) \\
&amp;= \frac{b^a}{\Gamma(a)} \text{exp}\left(a\psi - b e^\psi \right)
\end{align}\]</span></p>
<div class="cell" data-layout-align="center">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>dpsi <span class="ot">=</span> <span class="cf">function</span>(psi, a, b) {</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>  (b<span class="sc">^</span>a) <span class="sc">/</span> (<span class="fu">gamma</span>(a)) <span class="sc">*</span> <span class="fu">exp</span>(a <span class="sc">*</span> psi <span class="sc">-</span> b <span class="sc">*</span> <span class="fu">exp</span>(psi))</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="co"># To verify this is a valid PDF - with various a and b, integral should be approximately 1</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(<span class="cf">function</span>(p) <span class="fu">dpsi</span>(p, <span class="dv">2</span>, <span class="dv">2</span>), <span class="sc">-</span><span class="dv">100</span>, <span class="dv">100</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>1 with absolute error &lt; 3.3e-06</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(<span class="cf">function</span>(p) <span class="fu">dpsi</span>(p, <span class="dv">4</span>, <span class="dv">8</span>), <span class="sc">-</span><span class="dv">100</span>, <span class="dv">100</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>1 with absolute error &lt; 2.3e-05</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(<span class="cf">function</span>(p) <span class="fu">dpsi</span>(p, <span class="dv">10</span>, <span class="dv">1</span>), <span class="sc">-</span><span class="dv">100</span>, <span class="dv">100</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>1 with absolute error &lt; 3.1e-05</code></pre>
</div>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>psi <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">10</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.05</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>density <span class="ot">=</span> <span class="fu">dpsi</span>(psi, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(psi, density, <span class="at">geom =</span> <span class="st">'line'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="3_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="section-7" class="level2">
<h2 class="anchored" data-anchor-id="section-7">3.12</h2>
<section id="a-6" class="level3">
<h3 class="anchored" data-anchor-id="a-6">a</h3>
<p>If <span class="math inline">\(Y\)</span> is Binomial, then <span class="math inline">\(p(y \mid \theta) = {n \choose y} \theta^y (1 - \theta)^{n - y}\)</span>.</p>
<p>So <span class="math inline">\(I(\theta) = -\mathbb{E}(\partial^2 \ell(y \mid \theta) / \partial \theta^2 )\)</span> where <span class="math inline">\(\ell(y \mid \theta) = \log p(y \mid \theta)\)</span>. Now,</p>
<p><span class="math display">\[\begin{align}
\ell(y \mid \theta) &amp;= \log p(y \mid \theta) \\
&amp;= \log \left( {n \choose y} \theta^y (1 - \theta)^{n - y} \right) \\
&amp;= \log \left( {n \choose y} \right) + y \log(\theta) + (n - y) \log(1 - \theta) \\
\ell_\theta(y \mid \theta) &amp;= \frac{y}{\theta}- \frac{n - y}{1 - \theta} \\
\ell_{\theta \theta}(y \mid \theta) &amp;= - \frac{y}{\theta^2} - \frac{n - y}{(1 - \theta)^2} \\
\end{align}\]</span></p>
<p>So</p>
<p><span class="math display">\[\begin{align}
I(\theta) &amp;= -\mathbb{E}\left( -\frac{y}{\theta^2} - \frac{n - y}{(1 - \theta)^2} \right) \\
&amp;= - \left( -\frac{1}{\theta^2} \mathbb{E}(y) - \frac{1}{(1 - \theta)^2} \mathbb{E}(n - y) \right) \\
&amp;= \frac{n\theta}{\theta^2} + \frac{n - n\theta}{(1 - \theta)^2} \\
&amp;= \frac{n}{\theta} + \frac{n}{1 - \theta} \\
&amp;= \frac{n}{\theta (1 - \theta)}
\end{align}\]</span></p>
<p>So Jeffreys’ prior distribution is</p>
<p><span class="math display">\[\begin{align}
p_J(\theta) &amp;= c \times \sqrt{\frac{n}{\theta (1 - \theta)}} \\
\end{align}\]</span></p>
<p>where <span class="math display">\[
c = \left( \int_0^1 \sqrt{\frac{n}{\theta(1 - \theta)}} \; d\theta \right)^{-1}.
\]</span></p>
</section>
<section id="b-6" class="level3">
<h3 class="anchored" data-anchor-id="b-6">b</h3>
<p><span class="math display">\[\begin{align}
\ell(y \mid \psi) &amp;= \log p(y \mid \psi) \\
&amp;= \log \left( {n \choose y} e^{\psi y} (1 + e^\psi)^{-n} \right) \\
&amp;= \log {n \choose y} + \psi y - n \log \left(1 + e^\psi \right) \\
\ell_\psi(y \mid \psi) &amp;= y - \frac{n e^\psi}{e^\psi + 1} \\
\ell_{\psi \psi}(y \mid \psi) &amp;= - \frac{n e^\psi}{\left( e^\psi + 1 \right)^2}
\end{align}\]</span></p>
<p>So</p>
<p><span class="math display">\[\begin{align}
I(\psi) &amp;= -\mathbb{E}\left( - \frac{n e^\psi}{\left( e^\psi + 1 \right)^2}\right) \\
&amp;= \frac{n e^\psi}{\left( e^\psi + 1 \right)^2} \\
\end{align}\]</span></p>
<p>Then Jeffreys prior is <span class="math display">\[\begin{align}
p_J(\psi) &amp;\propto \sqrt{ \frac{n e^\psi}{\left( e^\psi + 1 \right)^2} } \\
&amp;\propto \frac{\sqrt{n e^\psi}}{e^\psi + 1}
\end{align}\]</span></p>
</section>
<section id="c-4" class="level3">
<h3 class="anchored" data-anchor-id="c-4">c</h3>
<p>If <span class="math inline">\(\psi = g(\theta) = \log \frac{\theta}{1 - \theta}\)</span>, then let <span class="math inline">\(\theta = h(\psi) = \frac{e^\psi}{1 + e^\psi}\)</span>. Then, by the change of variables formula,</p>
<p><span class="math display">\[\begin{align}
p_{\psi}(\psi) &amp;\propto p_{\theta}(h(\psi)) \times \left| \frac{dh}{d\psi} \right| \\
&amp;\propto \sqrt{\frac{n}{\frac{e^\psi}{1 + e^\psi} \left(1 - \frac{e^\psi}{1 + e^\psi}\right)}} \times \frac{e^\psi}{(e^\psi + 1)^2} \\
&amp;\propto \sqrt{\frac{n(e^\psi + 1)^2}{e^\psi} } \times \frac{e^\psi}{(e^\psi + 1)^2} \\
&amp;\propto \frac{\sqrt{n}(e^\psi + 1)}{\sqrt{e^\psi}} \times \frac{e^\psi}{(e^\psi + 1)^2} \\
&amp;\propto \frac{\sqrt{n}\sqrt{e^\psi}}{e^\psi + 1} \\
&amp;\propto p_J(\psi).
\end{align}\]</span></p>
<p>In this case, it has been demonstrated that Jeffreys’ prior is invariant under monotone transformation.</p>
</section>
</section>
<section id="section-8" class="level2">
<h2 class="anchored" data-anchor-id="section-8">3.13</h2>
<p>Improper Jeffreys’ prior</p>
<section id="a-7" class="level3">
<h3 class="anchored" data-anchor-id="a-7">a</h3>
<p>If <span class="math inline">\(Y \sim \text{Poisson}(\theta)\)</span>, <span class="math inline">\(p(y) = \frac{\theta^y e^{-\theta}}{y!}\)</span>.</p>
<p><span class="math display">\[\begin{align}
\ell(y \mid \theta) &amp;= \log p(y \mid \theta) \\
&amp;= \log \left( \frac{\theta^y e^{-\theta}}{y!} \right) \\
&amp;= \log \left( \frac{1}{y!} \right) + y \log (\theta) - \theta \\
\ell_\theta(y \mid \theta) &amp;= \frac{y}{\theta} - 1\\
\ell_{\theta \theta}(y \mid \theta) &amp;= -\frac{y}{\theta^2} \\
\end{align}\]</span></p>
<p>So</p>
<p><span class="math display">\[\begin{align}
I(\psi) &amp;= -\mathbb{E}\left( - \frac{y}{\theta^2} \right) \\
&amp;= \frac{1}{\theta^2} \mathbb{E}(y) \\
&amp;= \frac{\theta}{\theta^2} \\
&amp;= \frac{1}{\theta}
\end{align}\]</span></p>
<p>and Jeffreys’ prior is</p>
<p><span class="math display">\[\begin{align}
p_J(\theta) &amp;= c \times \sqrt{\frac{1}{\theta}} \\
&amp;= c \times \frac{1}{\sqrt{\theta}}
\end{align}\]</span></p>
<p>Notice that, to be a proper probability distribution, it must be the case that</p>
<p><span class="math display">\[\begin{align}
\int_0^{\infty} c \times \frac{1}{\sqrt{\theta}} \; d\theta = c \int_0^{\infty} \frac{1}{\sqrt{\theta}} \; d\theta = 1
\end{align}\]</span></p>
<p>However, this integral does not converge (<span class="math inline">\(p\)</span>-test, <span class="math inline">\(p = 1/2\)</span>), so there is no value of <span class="math inline">\(c\)</span> such that this is a valid probability distribution. Thus (per the name of the exercise) this is an <em>improper</em> Jeffreys’ prior.</p>
</section>
<section id="b-7" class="level3">
<h3 class="anchored" data-anchor-id="b-7">b</h3>
<p>However, even if <span class="math inline">\(p_J(\theta)\)</span> is not a valid probability distribution, we can still imagine performing “inference” using this prior.</p>
<p><span class="math display">\[\begin{align}
f(\theta, y) &amp;= \sqrt{I(\theta)} \times p(y \mid \theta) \\
&amp;= \frac{1}{\sqrt{\theta}} \times  \frac{\theta^y e^{-\theta}}{y!} \\
&amp;= \theta^{-1/2} \times  \frac{\theta^y e^{-\theta}}{\Gamma(y + 1)} \\
&amp;= \theta^{y - 1/2} e^{-\theta} \frac{1}{\Gamma(y + 1)} \\
\end{align}\]</span></p>
<p>As a function of <span class="math inline">\(\theta\)</span> only, this is proportional to <span class="math display">\[\begin{align}
\dots &amp;\propto \theta^{y - 1/2} e^{-\theta} \\
&amp;\propto \text{dgamma}(\theta, y + \frac{1/2}, 1)
\end{align}\]</span></p>
<p>Since <span class="math inline">\(\Gamma(y + \frac{1/2}, 1)\)</span>, <span class="math inline">\(y &gt;= 0\)</span> <em>is</em> a valid parameterization of a Gamma density, it follows that we can normalize <span class="math inline">\(f(\theta, y)\)</span> such that it represents a valid posterior density for <span class="math inline">\(\theta\)</span>. If we actually calculate <span class="math inline">\(f(\theta, y) / \int f(\theta, y) \; d\theta\)</span> (which I won’t do here), we will get such a Gamma density.</p>
<p>Notes: the prior in a) can be thought of as an improper <span class="math inline">\(\text{Gamma}(1/2, 0)\)</span>. Since improper Jeffreys’ priors are not real probability densities, their usage is controversial for some. (Who?)</p>


<!-- -->

</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb44" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> 'Chapter 3: One-parameter models'</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Jesse Mu"</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "September 17, 2016"</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Setup --&gt;</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="in">```{r echo=FALSE, message=FALSE}</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span>opts_chunk<span class="sc">$</span><span class="fu">set</span>(<span class="at">fig.align =</span> <span class="st">'center'</span>, <span class="at">message =</span> <span class="cn">FALSE</span>)</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(knitr)</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cowplot)</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(reshape)</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Begin writing --&gt;</span></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="fu"># The Binomial model</span></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>1998 General Social Survey: Females over age 65, $1 = \text{happy}$, $0 = </span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>\text{unhappy}$. $n = 129$. So let the survey be 129 exchangeable random </span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>variables $Y_1, \dots, Y_{129}$.</span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>Under our model, conditioned on some $\theta$, $Y_i$ are i.i.d. binary random</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>variables with probability $\theta$. So the joint probability is</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a>p(y_1, \dots, y_{129} \mid \theta) = \theta^{\sum_{i} y_i} (1 - \theta)^{129 -</span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a>\sum_i y_i}</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a>Now we need to specify our prior distribution</span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a><span class="fu">## Uniform prior</span></span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a>Imagine our prior is $\theta \sim \text{Uniform}(0, 1)$. What this means is</span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a>$P(a \leq \theta \leq b) = P(a + c \leq \theta \leq b + c)$ for all compatible</span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a>$a, b, c$. In other words, the probability of theta falling in an interval of a</span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a>given width is constant, regardless of where the interval is.</span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-42"><a href="#cb44-42" aria-hidden="true" tabindex="-1"></a>Then, notice</span>
<span id="cb44-43"><a href="#cb44-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-44"><a href="#cb44-44" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-45"><a href="#cb44-45" aria-hidden="true" tabindex="-1"></a>p(\theta \mid y_1, \dots, y_{129}) &amp;= \frac{p(y_1, \dots, y_{129} \mid \theta) p(\theta)}{p(y_1, \dots, y_{129})} <span class="sc">\\</span></span>
<span id="cb44-46"><a href="#cb44-46" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{p(y_1, \dots, y_{129} \mid \theta)}{p(y_1, \dots, y_{129})} &amp; (\text{since $p(\theta)$ is constant for all $\theta$}) <span class="sc">\\</span></span>
<span id="cb44-47"><a href="#cb44-47" aria-hidden="true" tabindex="-1"></a>&amp;\propto p(y_1, \dots, y_{129} \mid \theta)</span>
<span id="cb44-48"><a href="#cb44-48" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-49"><a href="#cb44-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-50"><a href="#cb44-50" aria-hidden="true" tabindex="-1"></a>so $p(\theta \mid Y)$ and $p(y \mid \theta)$ have the same shape (see MLE</span>
<span id="cb44-51"><a href="#cb44-51" aria-hidden="true" tabindex="-1"></a>discussion in Chapter 1).</span>
<span id="cb44-52"><a href="#cb44-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-53"><a href="#cb44-53" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data and posterior distribution</span></span>
<span id="cb44-54"><a href="#cb44-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-55"><a href="#cb44-55" aria-hidden="true" tabindex="-1"></a>Say the observed proportion is 118 happy out of 129 (91%). Our sampling model</span>
<span id="cb44-56"><a href="#cb44-56" aria-hidden="true" tabindex="-1"></a>for some fixed $\theta$ is</span>
<span id="cb44-57"><a href="#cb44-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-58"><a href="#cb44-58" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-59"><a href="#cb44-59" aria-hidden="true" tabindex="-1"></a>p(y \mid \theta) = \theta^{118} (1 - \theta)^{11}</span>
<span id="cb44-60"><a href="#cb44-60" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-61"><a href="#cb44-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-62"><a href="#cb44-62" aria-hidden="true" tabindex="-1"></a>linking this back to Bayes' rule above, we have the posterior probability</span>
<span id="cb44-63"><a href="#cb44-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-64"><a href="#cb44-64" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-65"><a href="#cb44-65" aria-hidden="true" tabindex="-1"></a>p(\theta \mid y) = \frac{\theta^{118} (1 - \theta)^{11}}{p(y)}</span>
<span id="cb44-66"><a href="#cb44-66" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-67"><a href="#cb44-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-68"><a href="#cb44-68" aria-hidden="true" tabindex="-1"></a>We will often (WHEN would we not normalize?) want to be more precise than this and know about the scale</span>
<span id="cb44-69"><a href="#cb44-69" aria-hidden="true" tabindex="-1"></a>of the posterior probability, not just the shape. This requires calculating $p(y) = p(y_1, \dots, y_{129})$:</span>
<span id="cb44-70"><a href="#cb44-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-71"><a href="#cb44-71" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-72"><a href="#cb44-72" aria-hidden="true" tabindex="-1"></a>1 &amp;= \int_0^1 p(\theta \mid y) \; d\theta &amp; (\text{Law of total probability}) <span class="sc">\\</span></span>
<span id="cb44-73"><a href="#cb44-73" aria-hidden="true" tabindex="-1"></a>&amp;= \int_0^1 \theta^{111} (1 - \theta)^{11} / p(y) \; d\theta <span class="sc">\\</span></span>
<span id="cb44-74"><a href="#cb44-74" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{p(y)} \int_0^1 \theta^{118} (1 - \theta)^{11} \; d\theta &amp; (\text{Note</span>
<span id="cb44-75"><a href="#cb44-75" aria-hidden="true" tabindex="-1"></a>$p(y)$ is constant for fixed $y$})<span class="sc">\\</span></span>
<span id="cb44-76"><a href="#cb44-76" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{p(y)} \frac{\Gamma(119) \Gamma(12)}{\Gamma(131)} &amp; (\text{From calculus})<span class="sc">\\</span></span>
<span id="cb44-77"><a href="#cb44-77" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-78"><a href="#cb44-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-79"><a href="#cb44-79" aria-hidden="true" tabindex="-1"></a>so $p(y) = \frac{\Gamma(119) \Gamma(12)}{\Gamma(131)} \approx 2.89 \times</span>
<span id="cb44-80"><a href="#cb44-80" aria-hidden="true" tabindex="-1"></a>10^{-18}$. Since our $y_i$ are exchangeable, this holds true for any sequences</span>
<span id="cb44-81"><a href="#cb44-81" aria-hidden="true" tabindex="-1"></a>of $y_i$ with 118 ones and 11 zeros.</span>
<span id="cb44-82"><a href="#cb44-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-83"><a href="#cb44-83" aria-hidden="true" tabindex="-1"></a>So, finally, the posterior probability is </span>
<span id="cb44-84"><a href="#cb44-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-85"><a href="#cb44-85" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-86"><a href="#cb44-86" aria-hidden="true" tabindex="-1"></a>p(\theta \mid y) &amp;= \frac{\Gamma(131)}{\Gamma(119) \Gamma(12)} \theta^{118} (1 -</span>
<span id="cb44-87"><a href="#cb44-87" aria-hidden="true" tabindex="-1"></a>\theta)^11 <span class="sc">\\</span></span>
<span id="cb44-88"><a href="#cb44-88" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\Gamma(131)}{\Gamma(119) \Gamma(12)} \theta^{119 - 1} (1 - \theta)^{12 - 1} &amp; (\text{Beta parameterization})<span class="sc">\\</span></span>
<span id="cb44-89"><a href="#cb44-89" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-90"><a href="#cb44-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-91"><a href="#cb44-91" aria-hidden="true" tabindex="-1"></a>which happens to be a *beta distribution* with parameters $a = 119$ and</span>
<span id="cb44-92"><a href="#cb44-92" aria-hidden="true" tabindex="-1"></a>$b = 12$.</span>
<span id="cb44-93"><a href="#cb44-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-94"><a href="#cb44-94" aria-hidden="true" tabindex="-1"></a>If $Y \sim \text{Beta}(a, b)$, then</span>
<span id="cb44-95"><a href="#cb44-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-96"><a href="#cb44-96" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PDF: $p(y) = \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)} y^{a - 1} (1 - y)^{b - 1}$</span>
<span id="cb44-97"><a href="#cb44-97" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{E}(Y) = \frac{a}{a + b}$</span>
<span id="cb44-98"><a href="#cb44-98" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{Mode}(Y) = \frac{a - 1}{a + b - 2}$</span>
<span id="cb44-99"><a href="#cb44-99" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{Var}(Y) = \frac{ab}{(a + b)^2 (a + b + 1)}$</span>
<span id="cb44-100"><a href="#cb44-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-101"><a href="#cb44-101" aria-hidden="true" tabindex="-1"></a>In our case, our posterior looks like:</span>
<span id="cb44-102"><a href="#cb44-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-103"><a href="#cb44-103" aria-hidden="true" tabindex="-1"></a><span class="in">```{r echo=FALSE}</span></span>
<span id="cb44-104"><a href="#cb44-104" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggrepel)</span>
<span id="cb44-105"><a href="#cb44-105" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="dv">119</span></span>
<span id="cb44-106"><a href="#cb44-106" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">=</span> <span class="dv">12</span></span>
<span id="cb44-107"><a href="#cb44-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-108"><a href="#cb44-108" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb44-109"><a href="#cb44-109" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.001</span>),</span>
<span id="cb44-110"><a href="#cb44-110" aria-hidden="true" tabindex="-1"></a>  <span class="at">density =</span> <span class="fu">dbeta</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.001</span>), alpha, beta)</span>
<span id="cb44-111"><a href="#cb44-111" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-112"><a href="#cb44-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-113"><a href="#cb44-113" aria-hidden="true" tabindex="-1"></a>stats <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb44-114"><a href="#cb44-114" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="fu">round</span>(<span class="fu">c</span>(</span>
<span id="cb44-115"><a href="#cb44-115" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This formula is only for alpha, beta &gt; 1</span></span>
<span id="cb44-116"><a href="#cb44-116" aria-hidden="true" tabindex="-1"></a>    (alpha <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span>((alpha <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">+</span> (beta <span class="sc">-</span> <span class="dv">1</span>)),</span>
<span id="cb44-117"><a href="#cb44-117" aria-hidden="true" tabindex="-1"></a>    alpha <span class="sc">/</span> (alpha <span class="sc">+</span> beta)</span>
<span id="cb44-118"><a href="#cb44-118" aria-hidden="true" tabindex="-1"></a>  ), <span class="dv">3</span>),</span>
<span id="cb44-119"><a href="#cb44-119" aria-hidden="true" tabindex="-1"></a>  <span class="at">statistic =</span> <span class="fu">c</span>(<span class="st">'mode'</span>, <span class="st">'expectation'</span>)</span>
<span id="cb44-120"><a href="#cb44-120" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-121"><a href="#cb44-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-122"><a href="#cb44-122" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(posterior, <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> density)) <span class="sc">+</span></span>
<span id="cb44-123"><a href="#cb44-123" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb44-124"><a href="#cb44-124" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">data =</span> stats,</span>
<span id="cb44-125"><a href="#cb44-125" aria-hidden="true" tabindex="-1"></a>             <span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">xintercept =</span> theta, <span class="at">lty =</span> statistic)) <span class="sc">+</span></span>
<span id="cb44-126"><a href="#cb44-126" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_label_repel</span>(<span class="at">data =</span> stats,</span>
<span id="cb44-127"><a href="#cb44-127" aria-hidden="true" tabindex="-1"></a>                   <span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> <span class="dv">12</span>, <span class="at">label =</span> theta),</span>
<span id="cb44-128"><a href="#cb44-128" aria-hidden="true" tabindex="-1"></a>                   <span class="at">force =</span> <span class="dv">50</span>)</span>
<span id="cb44-129"><a href="#cb44-129" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-130"><a href="#cb44-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-131"><a href="#cb44-131" aria-hidden="true" tabindex="-1"></a><span class="fu">## Inference for exchangeable binary data (i.e. more generally)</span></span>
<span id="cb44-132"><a href="#cb44-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-133"><a href="#cb44-133" aria-hidden="true" tabindex="-1"></a>Recall for our binary data $Y_1, \dots, Y_n$ that</span>
<span id="cb44-134"><a href="#cb44-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-135"><a href="#cb44-135" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-136"><a href="#cb44-136" aria-hidden="true" tabindex="-1"></a>p(\theta \mid y) &amp;= \frac{p(y \mid \theta) p(\theta)}{p(y)} <span class="sc">\\</span></span>
<span id="cb44-137"><a href="#cb44-137" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\theta^{\sum y_i} (1 - \theta)^{n - \sum y_i} p(\theta)}{p(y)} &amp; \text{(Since i.i.d.)} <span class="sc">\\</span></span>
<span id="cb44-138"><a href="#cb44-138" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-139"><a href="#cb44-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-140"><a href="#cb44-140" aria-hidden="true" tabindex="-1"></a>Importantly, the quantity $\sum_{i = 1}^n Y_i$ is the only statistic that is </span>
<span id="cb44-141"><a href="#cb44-141" aria-hidden="true" tabindex="-1"></a>needed to calculate posterior probabilities of $\theta$. So it is a *sufficient</span>
<span id="cb44-142"><a href="#cb44-142" aria-hidden="true" tabindex="-1"></a>statistic* for making inference about $\theta$. The statistic $Y = \sum Y_i$ has</span>
<span id="cb44-143"><a href="#cb44-143" aria-hidden="true" tabindex="-1"></a>a binomial distribution with parameters $(n, \theta)$. Then,</span>
<span id="cb44-144"><a href="#cb44-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-145"><a href="#cb44-145" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-146"><a href="#cb44-146" aria-hidden="true" tabindex="-1"></a>p(y \mid \theta) &amp;= {n \choose y} \theta^y (1 - \theta)^{n - y}</span>
<span id="cb44-147"><a href="#cb44-147" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-148"><a href="#cb44-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-149"><a href="#cb44-149" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Note: I skip the general derivation of posterior probabilities for uniform</span></span>
<span id="cb44-150"><a href="#cb44-150" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; prior here, since a uniform prior is also a $\text{Beta}(a, b)$.</span></span>
<span id="cb44-151"><a href="#cb44-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-152"><a href="#cb44-152" aria-hidden="true" tabindex="-1"></a>Now let's calculate the posterior probability $p(\theta \mid y)$ when</span>
<span id="cb44-153"><a href="#cb44-153" aria-hidden="true" tabindex="-1"></a>$p(\theta)$ is *not* uniform; in particular, when our prior on $\theta$ is a</span>
<span id="cb44-154"><a href="#cb44-154" aria-hidden="true" tabindex="-1"></a>Beta distribution ($\theta \sim \text{Beta}(a,</span>
<span id="cb44-155"><a href="#cb44-155" aria-hidden="true" tabindex="-1"></a>b)$):</span>
<span id="cb44-156"><a href="#cb44-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-157"><a href="#cb44-157" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-158"><a href="#cb44-158" aria-hidden="true" tabindex="-1"></a>p(\theta \mid y) &amp;= \frac{p(\theta) p(y \mid \theta)}{p(y)} &amp; <span class="sc">\\</span></span>
<span id="cb44-159"><a href="#cb44-159" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{p(y)} \times \underbrace{\frac{\Gamma(a + b)}{\Gamma(a)</span>
<span id="cb44-160"><a href="#cb44-160" aria-hidden="true" tabindex="-1"></a>\Gamma(b)}\theta^{a - 1}(1 - \theta)^{b - 1}}_{\text{PDF of $\text{Beta}(a,</span>
<span id="cb44-161"><a href="#cb44-161" aria-hidden="true" tabindex="-1"></a>b)$}} \times \underbrace{{n \choose y} \theta^y (1 - \theta)^{n -</span>
<span id="cb44-162"><a href="#cb44-162" aria-hidden="true" tabindex="-1"></a>y}}_{\text{$p(y \mid \theta)$ above}} &amp; <span class="sc">\\</span></span>
<span id="cb44-163"><a href="#cb44-163" aria-hidden="true" tabindex="-1"></a>&amp;= c \times \theta^{a + y - 1} (1 - \theta)^{b + n - y - 1} &amp; \text{(Combine $\theta$s)} \</span>
<span id="cb44-164"><a href="#cb44-164" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-165"><a href="#cb44-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-166"><a href="#cb44-166" aria-hidden="true" tabindex="-1"></a>where $c = f(n, y, a, b)$ is just compressing the other stuff in the equation</span>
<span id="cb44-167"><a href="#cb44-167" aria-hidden="true" tabindex="-1"></a>into a constant, since it doesn't depend on $\theta$.</span>
<span id="cb44-168"><a href="#cb44-168" aria-hidden="true" tabindex="-1"></a>Now, notice that the term with $\Gamma$s in the above</span>
<span id="cb44-169"><a href="#cb44-169" aria-hidden="true" tabindex="-1"></a>equation is the PDF of $\theta \sim \text{Beta}(a, b)$, which can also be</span>
<span id="cb44-170"><a href="#cb44-170" aria-hidden="true" tabindex="-1"></a>expressed with a constant $c = f(a, b)$: $p(\theta) = c \theta^{a - 1}(1</span>
<span id="cb44-171"><a href="#cb44-171" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>\theta)^{b - 1}$. Notice that this looks just like the equation above: thus,</span>
<span id="cb44-172"><a href="#cb44-172" aria-hidden="true" tabindex="-1"></a>$p(\theta)$ and $p(\theta \mid y)$ are both proportional to $\theta^{a - 1} (1 -</span>
<span id="cb44-173"><a href="#cb44-173" aria-hidden="true" tabindex="-1"></a>\theta)^{b - 1}$ by some constant $c$.</span>
<span id="cb44-174"><a href="#cb44-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-175"><a href="#cb44-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-176"><a href="#cb44-176" aria-hidden="true" tabindex="-1"></a>Now, since we know that as probability distributions, $\int p(\theta) = \int</span>
<span id="cb44-177"><a href="#cb44-177" aria-hidden="true" tabindex="-1"></a>p(\theta \mid y) \; d\theta = 1$, we also know that the functions share the same *scale*,</span>
<span id="cb44-178"><a href="#cb44-178" aria-hidden="true" tabindex="-1"></a>which means that $p(\theta \mid y)$ is actually a Beta PDF!</span>
<span id="cb44-179"><a href="#cb44-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-180"><a href="#cb44-180" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb44-181"><a href="#cb44-181" aria-hidden="true" tabindex="-1"></a>(\theta \mid y) \sim \text{Beta}(a + y, b + n - y).</span>
<span id="cb44-182"><a href="#cb44-182" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb44-183"><a href="#cb44-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-184"><a href="#cb44-184" aria-hidden="true" tabindex="-1"></a>We therefore call Beta distributions **conjugate** priors for Binomial</span>
<span id="cb44-185"><a href="#cb44-185" aria-hidden="true" tabindex="-1"></a>distributions.</span>
<span id="cb44-186"><a href="#cb44-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-187"><a href="#cb44-187" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conjugacy</span></span>
<span id="cb44-188"><a href="#cb44-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-189"><a href="#cb44-189" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **Conjugacy**. A class $\mathcal{P}$ of prior distributions for $\theta$ are</span></span>
<span id="cb44-190"><a href="#cb44-190" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; *conjugate priors* of a sampling model $p(y \mid \theta)$ if $$ p(\theta) \in</span></span>
<span id="cb44-191"><a href="#cb44-191" aria-hidden="true" tabindex="-1"></a><span class="at">\mathcal{P} \implies p(\theta \mid y) \in \mathcal{P}.$$</span></span>
<span id="cb44-192"><a href="#cb44-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-193"><a href="#cb44-193" aria-hidden="true" tabindex="-1"></a>Conjugate priors are very convenient and making certain calculations easy, since</span>
<span id="cb44-194"><a href="#cb44-194" aria-hidden="true" tabindex="-1"></a>there is some convenient closed form solution for the distribution of a model</span>
<span id="cb44-195"><a href="#cb44-195" aria-hidden="true" tabindex="-1"></a>posterior given a model prior and observed data.</span>
<span id="cb44-196"><a href="#cb44-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-197"><a href="#cb44-197" aria-hidden="true" tabindex="-1"></a>Now, if you remember from Chatper 1, since the expectation of a Beta</span>
<span id="cb44-198"><a href="#cb44-198" aria-hidden="true" tabindex="-1"></a>distribution is $\frac{a}{a + b}$,</span>
<span id="cb44-199"><a href="#cb44-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-200"><a href="#cb44-200" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-201"><a href="#cb44-201" aria-hidden="true" tabindex="-1"></a>\mathbb{E}(\theta \mid y) &amp;= \frac{a + y}{a + b + n} <span class="sc">\\</span></span>
<span id="cb44-202"><a href="#cb44-202" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{a + b}{a + b + n} \frac{a}{a + b} + \frac{n}{a + b + n}\frac{y}{n} <span class="sc">\\</span></span>
<span id="cb44-203"><a href="#cb44-203" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-204"><a href="#cb44-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-205"><a href="#cb44-205" aria-hidden="true" tabindex="-1"></a>Where $\theta_0 = \frac{a}{a + b}$ can be seen as our "prior expectation",</span>
<span id="cb44-206"><a href="#cb44-206" aria-hidden="true" tabindex="-1"></a>$\frac{y}{n}$ is the sample mean, and $w = a + b$ can be seen as the strength of</span>
<span id="cb44-207"><a href="#cb44-207" aria-hidden="true" tabindex="-1"></a>belief in our prior. I leave the equation expressed above with the $a$s and $b$s</span>
<span id="cb44-208"><a href="#cb44-208" aria-hidden="true" tabindex="-1"></a>expanded because another intuitive way of thinking about the problem is by</span>
<span id="cb44-209"><a href="#cb44-209" aria-hidden="true" tabindex="-1"></a>thinking of $a$ as the "prior number of 1s", $b$ as the "prior number of 0s",</span>
<span id="cb44-210"><a href="#cb44-210" aria-hidden="true" tabindex="-1"></a>and $a + b$ as the "prior sample size". If you play around with the equation</span>
<span id="cb44-211"><a href="#cb44-211" aria-hidden="true" tabindex="-1"></a>above, seeing how the values change with various $a$ and $b$ prior choices,</span>
<span id="cb44-212"><a href="#cb44-212" aria-hidden="true" tabindex="-1"></a>you'll notice it has several nice properties that intuitively balance the</span>
<span id="cb44-213"><a href="#cb44-213" aria-hidden="true" tabindex="-1"></a>expectation and strength of our prior with the amount of data we receive.</span>
<span id="cb44-214"><a href="#cb44-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-215"><a href="#cb44-215" aria-hidden="true" tabindex="-1"></a><span class="fu">### Prediction</span></span>
<span id="cb44-216"><a href="#cb44-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-217"><a href="#cb44-217" aria-hidden="true" tabindex="-1"></a>Assume we've already seen data $y_1, \dots y_n$ and we want to predict value of</span>
<span id="cb44-218"><a href="#cb44-218" aria-hidden="true" tabindex="-1"></a>the next observation $\tilde{Y}$. Intuitively we should predict $\tilde{Y} = 1$</span>
<span id="cb44-219"><a href="#cb44-219" aria-hidden="true" tabindex="-1"></a>with probability equal to the expectation of $\theta$ given our data (according</span>
<span id="cb44-220"><a href="#cb44-220" aria-hidden="true" tabindex="-1"></a>to the Bayesian framework). These calculations confirm this:</span>
<span id="cb44-221"><a href="#cb44-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-222"><a href="#cb44-222" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-223"><a href="#cb44-223" aria-hidden="true" tabindex="-1"></a>p(\tilde{Y} = 1 \mid y) &amp;= \int p(\tilde{Y} = 1, \theta \mid y) \; d\theta &amp; \text{(LTP)} <span class="sc">\\</span></span>
<span id="cb44-224"><a href="#cb44-224" aria-hidden="true" tabindex="-1"></a>&amp;= \int p(\tilde{Y} = 1 \mid \theta, y) p(\theta \mid y) \; d\theta &amp; \text{(Chain rule)} <span class="sc">\\</span></span>
<span id="cb44-225"><a href="#cb44-225" aria-hidden="true" tabindex="-1"></a>&amp;= \int \theta p(\theta \mid y) \; d\theta &amp; \text{(Since $\tilde{Y}$ is binary)} <span class="sc">\\</span></span>
<span id="cb44-226"><a href="#cb44-226" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}(\theta \mid y).</span>
<span id="cb44-227"><a href="#cb44-227" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-228"><a href="#cb44-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-229"><a href="#cb44-229" aria-hidden="true" tabindex="-1"></a>Note that in this case the posterior predictive distribution is very easy to</span>
<span id="cb44-230"><a href="#cb44-230" aria-hidden="true" tabindex="-1"></a>predict and is (as it can only possibly be) a Bernoulli distribution with a</span>
<span id="cb44-231"><a href="#cb44-231" aria-hidden="true" tabindex="-1"></a>certain probability $p$. When posterior distributions become more complicated,</span>
<span id="cb44-232"><a href="#cb44-232" aria-hidden="true" tabindex="-1"></a>however, (e.g. Poisson model), their posterior predictive distributions may</span>
<span id="cb44-233"><a href="#cb44-233" aria-hidden="true" tabindex="-1"></a>require more complex calculations and may result in a different family of</span>
<span id="cb44-234"><a href="#cb44-234" aria-hidden="true" tabindex="-1"></a>distributions. Still later, we will show how to simulate posterior predictive</span>
<span id="cb44-235"><a href="#cb44-235" aria-hidden="true" tabindex="-1"></a>distributions with Monte Carlo sampling.</span>
<span id="cb44-236"><a href="#cb44-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-237"><a href="#cb44-237" aria-hidden="true" tabindex="-1"></a>Also note that using the expectation instead of the mode is nice, for examples where</span>
<span id="cb44-238"><a href="#cb44-238" aria-hidden="true" tabindex="-1"></a>we have a uniform $\text{Beta}(1, 1)$ prior and we observe $Y = 0$. Then,</span>
<span id="cb44-239"><a href="#cb44-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-240"><a href="#cb44-240" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-241"><a href="#cb44-241" aria-hidden="true" tabindex="-1"></a>\mathbb{E}(\theta \mid Y = 0) &amp;= \frac{2}{2 + n}\frac{1}{2} + \frac{n}{2 +</span>
<span id="cb44-242"><a href="#cb44-242" aria-hidden="true" tabindex="-1"></a>n}\frac{y}{n} = \frac{1}{2 + n}<span class="sc">\\</span></span>
<span id="cb44-243"><a href="#cb44-243" aria-hidden="true" tabindex="-1"></a>\theta_{MAP} &amp;= \frac{y}{n} = 0 <span class="sc">\\</span></span>
<span id="cb44-244"><a href="#cb44-244" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-245"><a href="#cb44-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-246"><a href="#cb44-246" aria-hidden="true" tabindex="-1"></a>And clearly the expectation is more sensible as a predictor of future $\tilde{Y}$. But $\theta_{MAP}$ is not as unreasonable when there is a non-uniform prior...</span>
<span id="cb44-247"><a href="#cb44-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-248"><a href="#cb44-248" aria-hidden="true" tabindex="-1"></a><span class="fu">## Confidence regions</span></span>
<span id="cb44-249"><a href="#cb44-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-250"><a href="#cb44-250" aria-hidden="true" tabindex="-1"></a>See <span class="co">[</span><span class="ot">this StackExchange question</span><span class="co">](http://stats.stackexchange.com/questions/2272/whats-the-difference-between-a-confidence-interval-and-a-credible-interval)</span> for a really interesting discussiona bout the difference between Bayesian and frequentist confidence intervals.</span>
<span id="cb44-251"><a href="#cb44-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-252"><a href="#cb44-252" aria-hidden="true" tabindex="-1"></a>Bayesian confidence interval is an interval $<span class="co">[</span><span class="ot">l(y), u(y)</span><span class="co">]</span>$ with the following property:</span>
<span id="cb44-253"><a href="#cb44-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-254"><a href="#cb44-254" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-255"><a href="#cb44-255" aria-hidden="true" tabindex="-1"></a>P(l(y) &lt; \theta &lt; u(y) \mid Y = y) = .95</span>
<span id="cb44-256"><a href="#cb44-256" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-257"><a href="#cb44-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-258"><a href="#cb44-258" aria-hidden="true" tabindex="-1"></a>Intuitively, a Bayesian confidence interval quantifies our uncertainty about the</span>
<span id="cb44-259"><a href="#cb44-259" aria-hidden="true" tabindex="-1"></a>true value of the parameter we are estimating.</span>
<span id="cb44-260"><a href="#cb44-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-261"><a href="#cb44-261" aria-hidden="true" tabindex="-1"></a>Frequentist confidence interval is an interval $<span class="co">[</span><span class="ot">l(Y), u(Y)</span><span class="co">]</span>$ with the following</span>
<span id="cb44-262"><a href="#cb44-262" aria-hidden="true" tabindex="-1"></a>property:</span>
<span id="cb44-263"><a href="#cb44-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-264"><a href="#cb44-264" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-265"><a href="#cb44-265" aria-hidden="true" tabindex="-1"></a>P(l(Y) &lt; \theta &lt; u(Y) \mid theta) = .95</span>
<span id="cb44-266"><a href="#cb44-266" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-267"><a href="#cb44-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-268"><a href="#cb44-268" aria-hidden="true" tabindex="-1"></a>Intuitively, a Frequentist confidence interval quantifies our uncertainty about </span>
<span id="cb44-269"><a href="#cb44-269" aria-hidden="true" tabindex="-1"></a>the measurement we have made of the parameter with a *fixed* true value.</span>
<span id="cb44-270"><a href="#cb44-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-271"><a href="#cb44-271" aria-hidden="true" tabindex="-1"></a>Which interval is better is of course debated heavily.</span>
<span id="cb44-272"><a href="#cb44-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-273"><a href="#cb44-273" aria-hidden="true" tabindex="-1"></a>To construct Bayesian confidence intervals, an easy way of doing so is to select</span>
<span id="cb44-274"><a href="#cb44-274" aria-hidden="true" tabindex="-1"></a>quantiles of the posterior probability distribution such that the area in the</span>
<span id="cb44-275"><a href="#cb44-275" aria-hidden="true" tabindex="-1"></a>interval under the curve is $1 - \alpha$, where $\alpha$ is the desired confidence level:</span>
<span id="cb44-276"><a href="#cb44-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-279"><a href="#cb44-279" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-280"><a href="#cb44-280" aria-hidden="true" tabindex="-1"></a><span class="co"># Uninformative prior, observe 10 variables with 2 1s</span></span>
<span id="cb44-281"><a href="#cb44-281" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb44-282"><a href="#cb44-282" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb44-283"><a href="#cb44-283" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb44-284"><a href="#cb44-284" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb44-285"><a href="#cb44-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-286"><a href="#cb44-286" aria-hidden="true" tabindex="-1"></a>quantiles <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">q =</span> <span class="fu">qbeta</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), a <span class="sc">+</span> y, b <span class="sc">+</span> n <span class="sc">-</span> y))</span>
<span id="cb44-287"><a href="#cb44-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-288"><a href="#cb44-288" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb44-289"><a href="#cb44-289" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.001</span>),</span>
<span id="cb44-290"><a href="#cb44-290" aria-hidden="true" tabindex="-1"></a>  <span class="at">p =</span> <span class="fu">dbeta</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.001</span>), a <span class="sc">+</span> y, b <span class="sc">+</span> n <span class="sc">-</span> y)</span>
<span id="cb44-291"><a href="#cb44-291" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-292"><a href="#cb44-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-293"><a href="#cb44-293" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> p)) <span class="sc">+</span></span>
<span id="cb44-294"><a href="#cb44-294" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb44-295"><a href="#cb44-295" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">data =</span> quantiles, <span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">xintercept =</span> q))</span>
<span id="cb44-296"><a href="#cb44-296" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-297"><a href="#cb44-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-298"><a href="#cb44-298" aria-hidden="true" tabindex="-1"></a>However, since some of the values for theta *outside* of the interval have </span>
<span id="cb44-299"><a href="#cb44-299" aria-hidden="true" tabindex="-1"></a>higher density than values of theta *inside* the interval, another option </span>
<span id="cb44-300"><a href="#cb44-300" aria-hidden="true" tabindex="-1"></a>is to force "symmetry" of heights by searching for the **highest posterior </span>
<span id="cb44-301"><a href="#cb44-301" aria-hidden="true" tabindex="-1"></a>density** region, which can be intuitively found by drawing a horizontal line </span>
<span id="cb44-302"><a href="#cb44-302" aria-hidden="true" tabindex="-1"></a>down the density until the region contained by the line is $1 - \alpha$% of the </span>
<span id="cb44-303"><a href="#cb44-303" aria-hidden="true" tabindex="-1"></a>entire curve. It's not entirely clear to me how to calculate these analytically; computationally, using a discretized density, you can use a "trial and error" approach or a neat procedure detailed in Exercise 4.7 (c).</span>
<span id="cb44-304"><a href="#cb44-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-305"><a href="#cb44-305" aria-hidden="true" tabindex="-1"></a><span class="fu"># The Poisson model</span></span>
<span id="cb44-306"><a href="#cb44-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-307"><a href="#cb44-307" aria-hidden="true" tabindex="-1"></a>This is for measurements that have values that are whole numbers, rather than just 1 or 0 (example: number of children).</span>
<span id="cb44-308"><a href="#cb44-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-309"><a href="#cb44-309" aria-hidden="true" tabindex="-1"></a>Recall the PDF of a Poisson distribution - if $Y \sim \text{Poisson}(\theta)$ then</span>
<span id="cb44-310"><a href="#cb44-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-311"><a href="#cb44-311" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$p(Y = y \mid \theta) = \theta^y e^{-\theta} / y!$</span>
<span id="cb44-312"><a href="#cb44-312" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{E}(Y) = \text{Var}(Y) = \theta$</span>
<span id="cb44-313"><a href="#cb44-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-314"><a href="#cb44-314" aria-hidden="true" tabindex="-1"></a><span class="fu">## Posterior inference</span></span>
<span id="cb44-315"><a href="#cb44-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-316"><a href="#cb44-316" aria-hidden="true" tabindex="-1"></a>Just like how $Y = \sum Y_i$ is a sufficient statistic for $n$ independent </span>
<span id="cb44-317"><a href="#cb44-317" aria-hidden="true" tabindex="-1"></a>Bernoulli trials, there exists a sufficient statistic for a sample of $n$ i.i.d.</span>
<span id="cb44-318"><a href="#cb44-318" aria-hidden="true" tabindex="-1"></a>Poisson variables. Notice</span>
<span id="cb44-319"><a href="#cb44-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-320"><a href="#cb44-320" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-321"><a href="#cb44-321" aria-hidden="true" tabindex="-1"></a>P(Y_1 = y_1, \dots, Y_n = y_n \mid \theta) &amp;=</span>
<span id="cb44-322"><a href="#cb44-322" aria-hidden="true" tabindex="-1"></a>\prod_i p(y_i \mid \theta) <span class="sc">\\</span></span>
<span id="cb44-323"><a href="#cb44-323" aria-hidden="true" tabindex="-1"></a>&amp;= \prod_i \frac{\theta^{y_i} e^{-\theta}}{y_i !} <span class="sc">\\</span></span>
<span id="cb44-324"><a href="#cb44-324" aria-hidden="true" tabindex="-1"></a>&amp;= \theta^{\sum_i y_i} e^{-n \theta} \prod_i \frac{1}{y_i !} &amp; <span class="sc">\\</span></span>
<span id="cb44-325"><a href="#cb44-325" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-326"><a href="#cb44-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-327"><a href="#cb44-327" aria-hidden="true" tabindex="-1"></a>When we compare two values of $\theta$,</span>
<span id="cb44-328"><a href="#cb44-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-329"><a href="#cb44-329" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-330"><a href="#cb44-330" aria-hidden="true" tabindex="-1"></a>\frac{p(\theta_a \mid y_1, \dots, y_n)}{p(\theta_b \mid y_1, \dots, y_n)} &amp;= \dots</span>
<span id="cb44-331"><a href="#cb44-331" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-332"><a href="#cb44-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-333"><a href="#cb44-333" aria-hidden="true" tabindex="-1"></a>notice that the $\prod_i 1 / y_i !$ term is the same in that ratio, and thus</span>
<span id="cb44-334"><a href="#cb44-334" aria-hidden="true" tabindex="-1"></a>cancels out. Then the $\theta_{a, b}^{\sum_i y_i}$ terms are the only terms remaining in the</span>
<span id="cb44-335"><a href="#cb44-335" aria-hidden="true" tabindex="-1"></a>fraction, and thus $\sum_i y_i$ is a sufficient statistic. This sufficient</span>
<span id="cb44-336"><a href="#cb44-336" aria-hidden="true" tabindex="-1"></a>statistic is a little more interesting than the Bernoulli case - consider that</span>
<span id="cb44-337"><a href="#cb44-337" aria-hidden="true" tabindex="-1"></a>it doesn't matter what the individual values of $y_i$ are (if one is very large,</span>
<span id="cb44-338"><a href="#cb44-338" aria-hidden="true" tabindex="-1"></a>one is very small, or they all look the same) - inference on $\theta$ is</span>
<span id="cb44-339"><a href="#cb44-339" aria-hidden="true" tabindex="-1"></a>possible with just the aggregate sum.</span>
<span id="cb44-340"><a href="#cb44-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-341"><a href="#cb44-341" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conjugate prior</span></span>
<span id="cb44-342"><a href="#cb44-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-343"><a href="#cb44-343" aria-hidden="true" tabindex="-1"></a>Using Bayes' rule and the sampling model above, we have</span>
<span id="cb44-344"><a href="#cb44-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-345"><a href="#cb44-345" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-346"><a href="#cb44-346" aria-hidden="true" tabindex="-1"></a>p(\theta \mid y) &amp;= \frac{p(\theta)p(y \mid \theta)}{p(y)} <span class="sc">\\</span></span>
<span id="cb44-347"><a href="#cb44-347" aria-hidden="true" tabindex="-1"></a>&amp;\propto p(\theta) p(y \mid \theta) <span class="sc">\\</span></span>
<span id="cb44-348"><a href="#cb44-348" aria-hidden="true" tabindex="-1"></a>&amp;\propto p(\theta) \times \theta^{\sum y_i} e^{-n\theta} <span class="sc">\\</span></span>
<span id="cb44-349"><a href="#cb44-349" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-350"><a href="#cb44-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-351"><a href="#cb44-351" aria-hidden="true" tabindex="-1"></a>We are looking for some distribution of $p(\theta)$ that makes the posterior as</span>
<span id="cb44-352"><a href="#cb44-352" aria-hidden="true" tabindex="-1"></a>calculated using the above the same distribution as $p(\theta)$ itself. That</span>
<span id="cb44-353"><a href="#cb44-353" aria-hidden="true" tabindex="-1"></a>family of distributions is the Gamma family. If $\theta \sim \text{Gamma}(a, b)$, then</span>
<span id="cb44-354"><a href="#cb44-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-355"><a href="#cb44-355" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$p(\theta) = \frac{b^a}{\Gamma(a)}\theta^{a - 1} e^{-b \theta}$</span>
<span id="cb44-356"><a href="#cb44-356" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbb{E}(\theta) = a/b$</span>
<span id="cb44-357"><a href="#cb44-357" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{Var}(\theta) = a/b^2$</span>
<span id="cb44-358"><a href="#cb44-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-359"><a href="#cb44-359" aria-hidden="true" tabindex="-1"></a>To prove conjugacy, we have</span>
<span id="cb44-360"><a href="#cb44-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-361"><a href="#cb44-361" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-362"><a href="#cb44-362" aria-hidden="true" tabindex="-1"></a>p(\theta \mid y) &amp;= \frac{p(\theta) p(y \mid \theta)}{p(y)} <span class="sc">\\</span></span>
<span id="cb44-363"><a href="#cb44-363" aria-hidden="true" tabindex="-1"></a>&amp;= \underbrace{\left( \frac{b^a}{\Gamma(a)} \theta^{a - 1} e^{-b \theta}</span>
<span id="cb44-364"><a href="#cb44-364" aria-hidden="true" tabindex="-1"></a>\right)}_{p(\theta)} \underbrace{\left( \theta^{\sum y_i} e^{-n \theta} \prod_i</span>
<span id="cb44-365"><a href="#cb44-365" aria-hidden="true" tabindex="-1"></a>\frac{1}{y_i !} \right)}_{p(y \mid \theta)} \left( \frac{1}{p(y)} \right) <span class="sc">\\</span></span>
<span id="cb44-366"><a href="#cb44-366" aria-hidden="true" tabindex="-1"></a>&amp;= c \times \left( \theta^{a - 1 + \sum y_i} e^{-(b + n) \theta} \right)</span>
<span id="cb44-367"><a href="#cb44-367" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-368"><a href="#cb44-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-369"><a href="#cb44-369" aria-hidden="true" tabindex="-1"></a>Where $c = f(y, a, b)$ is throwing all of the stuff that doesn't depend on</span>
<span id="cb44-370"><a href="#cb44-370" aria-hidden="true" tabindex="-1"></a>$\theta$ into some normalizing constant such that $\int p(\theta \mid y) \; d\theta = 1$.</span>
<span id="cb44-371"><a href="#cb44-371" aria-hidden="true" tabindex="-1"></a>Like the Binomial model, we recognize from the proportionality to $\theta^{a - 1} e^{-b \theta}$ that the posterior distribution of $\theta$ is Gamma distributed. Specifically</span>
<span id="cb44-372"><a href="#cb44-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-373"><a href="#cb44-373" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-374"><a href="#cb44-374" aria-hidden="true" tabindex="-1"></a>\theta \mid y_1, \dots, y_n \sim \text{Gamma}(a + \sum_i y_i, b + n)</span>
<span id="cb44-375"><a href="#cb44-375" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-376"><a href="#cb44-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-377"><a href="#cb44-377" aria-hidden="true" tabindex="-1"></a>and the expectation is</span>
<span id="cb44-378"><a href="#cb44-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-379"><a href="#cb44-379" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-380"><a href="#cb44-380" aria-hidden="true" tabindex="-1"></a>\mathbb{E}(\theta \mid y_1, \dots, y_n) &amp;= \frac{a + \sum y_i}{b + n} <span class="sc">\\</span></span>
<span id="cb44-381"><a href="#cb44-381" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{b}{b + n} \frac{a}{b} + \frac{n}{b + n}\frac{\sum y_i}{n} <span class="sc">\\</span></span>
<span id="cb44-382"><a href="#cb44-382" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-383"><a href="#cb44-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-384"><a href="#cb44-384" aria-hidden="true" tabindex="-1"></a><span class="fu">### Posterior predictive distribution</span></span>
<span id="cb44-385"><a href="#cb44-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-386"><a href="#cb44-386" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-387"><a href="#cb44-387" aria-hidden="true" tabindex="-1"></a>p(\tilde{y} \mid y_1, \dots, y_n) &amp;= \int_0^{\infty} p(\tilde{y} \mid \theta, y_1, \dots, y_n) p(\theta \mid y_1, \dots, y_n) \; d\theta &amp; \text{Marginalization} <span class="sc">\\</span></span>
<span id="cb44-388"><a href="#cb44-388" aria-hidden="true" tabindex="-1"></a>&amp;= \int_0^{\infty} p(\tilde{y} \mid \theta) p(\theta \mid y_1, \dots, y_n) \; d\theta &amp; \text{Since $\tilde{y}$ and $y_i$s c.i. given $\theta$} <span class="sc">\\</span></span>
<span id="cb44-389"><a href="#cb44-389" aria-hidden="true" tabindex="-1"></a>&amp;= \int_0^{\infty} \left<span class="co">[</span><span class="ot"> \frac{\theta^{\tilde{y}} e^{-\theta}}{\tilde{y}!} \right</span><span class="co">]</span> \times \left<span class="co">[</span><span class="ot"> \frac{(b + n)^{a + \sum y_i}}{\Gamma(a + \sum y_i)} \theta^{a + \sum y_i - 1} e^{-(b + n)\theta} \right</span><span class="co">]</span> \; d\theta <span class="sc">\\</span></span>
<span id="cb44-390"><a href="#cb44-390" aria-hidden="true" tabindex="-1"></a>&amp;= \int_0^{\infty} \left<span class="co">[</span><span class="ot"> \frac{\theta^{\tilde{y} + a + \sum y_i - 1} e^{-(b + n + 1)\theta}}{\tilde{y}!} \right</span><span class="co">]</span> \times \left<span class="co">[</span><span class="ot"> \frac{(b + n)^{a + \sum y_i}}{\Gamma(a + \sum y_i)} \right</span><span class="co">]</span> \; d\theta &amp; \text{Combine $\theta$s, $e$s} <span class="sc">\\</span></span>
<span id="cb44-391"><a href="#cb44-391" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{(b + n)^{a + \sum y_i}}{\Gamma(\tilde{y} + 1) \Gamma(a + \sum y_i)} \int_0^{\infty} \theta^{\tilde{y} + a + \sum y_i - 1} e^{-(b + n + 1)\theta} \; d\theta <span class="sc">\\</span></span>
<span id="cb44-392"><a href="#cb44-392" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-393"><a href="#cb44-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-394"><a href="#cb44-394" aria-hidden="true" tabindex="-1"></a>Notice that the integrand is proportional to a Gamma density:</span>
<span id="cb44-395"><a href="#cb44-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-396"><a href="#cb44-396" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-397"><a href="#cb44-397" aria-hidden="true" tabindex="-1"></a>&amp; \int_0^{\infty} \frac{b^a}{\Gamma(a)} \theta^{a - 1} e^{-b \theta} \; d\theta = 1<span class="sc">\\</span></span>
<span id="cb44-398"><a href="#cb44-398" aria-hidden="true" tabindex="-1"></a>\implies&amp; \int_0^{\infty} \theta^{a - 1} e^{-b\theta} \; d\theta = \frac{\Gamma(a)}{b^a} <span class="sc">\\</span></span>
<span id="cb44-399"><a href="#cb44-399" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-400"><a href="#cb44-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-401"><a href="#cb44-401" aria-hidden="true" tabindex="-1"></a>So</span>
<span id="cb44-402"><a href="#cb44-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-403"><a href="#cb44-403" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-404"><a href="#cb44-404" aria-hidden="true" tabindex="-1"></a>p(\tilde{y} \mid y_1, \dots, y_n) &amp;= \dots <span class="sc">\\</span></span>
<span id="cb44-405"><a href="#cb44-405" aria-hidden="true" tabindex="-1"></a>&amp;= \left( \frac{(b + n)^{a + \sum y_i}}{\Gamma(\tilde{y} + 1) \Gamma(a + \sum y_i)} \right) \left( \frac{\Gamma(\tilde{y} + a + \sum y_i)}{(b + n + 1)^{\tilde{y} + a + \sum y_i}} \right) <span class="sc">\\</span></span>
<span id="cb44-406"><a href="#cb44-406" aria-hidden="true" tabindex="-1"></a>&amp;= \left( \frac{\Gamma(\tilde{y} + a + \sum y_i)}{\Gamma(\tilde{y} + 1) \Gamma(a + \sum y_i)} \right) \left( \frac{(b + n)^{a + \sum y_i}}{(b + n + 1)^{\tilde{y} + a + \sum y_i}} \right) &amp; \text{Swapping numerators} <span class="sc">\\</span></span>
<span id="cb44-407"><a href="#cb44-407" aria-hidden="true" tabindex="-1"></a>&amp;= \left( \frac{\Gamma(\tilde{y} + a + \sum y_i)}{\Gamma(\tilde{y} + 1) \Gamma(a + \sum y_i)} \right) \left( \frac{b + n}{b + n + 1} \right)^{a + \sum y_i} \left( \frac{1}{b + n + 1} \right)^{\tilde{y}} <span class="sc">\\</span></span>
<span id="cb44-408"><a href="#cb44-408" aria-hidden="true" tabindex="-1"></a>&amp;= \text{dnbinom}(\tilde{y}, a + \sum y_i, b + n) <span class="sc">\\</span></span>
<span id="cb44-409"><a href="#cb44-409" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-410"><a href="#cb44-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-411"><a href="#cb44-411" aria-hidden="true" tabindex="-1"></a>The negative binomial distribution here, whose parameters look just like the</span>
<span id="cb44-412"><a href="#cb44-412" aria-hidden="true" tabindex="-1"></a>Gamma posterior on theta, can be thought of as a predictive Poisson distribution</span>
<span id="cb44-413"><a href="#cb44-413" aria-hidden="true" tabindex="-1"></a>with increased variance owing to the increased uncertainty on the value of</span>
<span id="cb44-414"><a href="#cb44-414" aria-hidden="true" tabindex="-1"></a>$\theta$. Notice</span>
<span id="cb44-415"><a href="#cb44-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-416"><a href="#cb44-416" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-417"><a href="#cb44-417" aria-hidden="true" tabindex="-1"></a>\mathbb{E}(\tilde{Y} \mid y_1, \dots, y_n) &amp;= \frac{a + \sum y_i}{b + n} = \mathbb{E}(\theta \mid y_1, \dots, y_n) <span class="sc">\\</span></span>
<span id="cb44-418"><a href="#cb44-418" aria-hidden="true" tabindex="-1"></a>\text{Var}(\tilde{Y} \mid y_1, \dots, y_n) &amp;= \frac{a + \sum y_i}{b + n} \frac{b</span>
<span id="cb44-419"><a href="#cb44-419" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>n + 1}{b + n} = \mathbb{E}(\theta \mid y_1, \dots, y_n) \times \frac{b + n + 1}{b + n}</span>
<span id="cb44-420"><a href="#cb44-420" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span></span>
<span id="cb44-421"><a href="#cb44-421" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-422"><a href="#cb44-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-423"><a href="#cb44-423" aria-hidden="true" tabindex="-1"></a>So as $n$ grows large, the variance on $\tilde{Y}$ approaches the variance of its expectation</span>
<span id="cb44-424"><a href="#cb44-424" aria-hidden="true" tabindex="-1"></a>$\mathbb{E}(\tilde{Y})$.</span>
<span id="cb44-425"><a href="#cb44-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-426"><a href="#cb44-426" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Birth Rates</span></span>
<span id="cb44-427"><a href="#cb44-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-428"><a href="#cb44-428" aria-hidden="true" tabindex="-1"></a>In the 1990s, did women with college degrees have different numbers of children </span>
<span id="cb44-429"><a href="#cb44-429" aria-hidden="true" tabindex="-1"></a>than women without college degrees? We sample $n_1$ women without college </span>
<span id="cb44-430"><a href="#cb44-430" aria-hidden="true" tabindex="-1"></a>degrees, denoted $Y_{1,1}, \dots, Y_{n_1, 1}$, and $n_2$ women with college </span>
<span id="cb44-431"><a href="#cb44-431" aria-hidden="true" tabindex="-1"></a>degrees, $Y_{1, 2}, \dots, Y_{n_2, 2}$. For some parameter $\theta$ we can model</span>
<span id="cb44-432"><a href="#cb44-432" aria-hidden="true" tabindex="-1"></a>the number of children for each woman as being i.i.d $\text{Poisson}(\theta_1)$ </span>
<span id="cb44-433"><a href="#cb44-433" aria-hidden="true" tabindex="-1"></a>and $\text{Poisson}(\theta_2)$, respectively. We may be interested in conducting</span>
<span id="cb44-434"><a href="#cb44-434" aria-hidden="true" tabindex="-1"></a>hypothesis tests to see whether or not these $\theta$ are different.</span>
<span id="cb44-435"><a href="#cb44-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-436"><a href="#cb44-436" aria-hidden="true" tabindex="-1"></a>Recall that, in this two samples, the *sufficient* statistic is simply the sum</span>
<span id="cb44-437"><a href="#cb44-437" aria-hidden="true" tabindex="-1"></a>of all of the $Y_i$. Say we observe</span>
<span id="cb44-438"><a href="#cb44-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-439"><a href="#cb44-439" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>No college: $n_1 = 111$, $\sum Y_i = 217$, $\sum Y_i / n_1 = 1.95$ (this is just useful for intuition)</span>
<span id="cb44-440"><a href="#cb44-440" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>College: $n_2 = 44$, $\sum Y_i = 66$, $\sum Y_i / n_2 = 1.50$</span>
<span id="cb44-441"><a href="#cb44-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-442"><a href="#cb44-442" aria-hidden="true" tabindex="-1"></a>Say our prior on both $\theta$ is $\text{Gamma(a = 2, 1)}$, which is lightly</span>
<span id="cb44-443"><a href="#cb44-443" aria-hidden="true" tabindex="-1"></a>centered on ~1. You can toy around with choices of different priors by changing</span>
<span id="cb44-444"><a href="#cb44-444" aria-hidden="true" tabindex="-1"></a><span class="in">`a`</span> and <span class="in">`b`</span> below:</span>
<span id="cb44-445"><a href="#cb44-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-448"><a href="#cb44-448" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-449"><a href="#cb44-449" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="dv">2</span></span>
<span id="cb44-450"><a href="#cb44-450" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb44-451"><a href="#cb44-451" aria-hidden="true" tabindex="-1"></a>n1 <span class="ot">=</span> <span class="dv">111</span></span>
<span id="cb44-452"><a href="#cb44-452" aria-hidden="true" tabindex="-1"></a>sy1 <span class="ot">=</span> <span class="dv">217</span></span>
<span id="cb44-453"><a href="#cb44-453" aria-hidden="true" tabindex="-1"></a>n2 <span class="ot">=</span> <span class="dv">44</span></span>
<span id="cb44-454"><a href="#cb44-454" aria-hidden="true" tabindex="-1"></a>sy2 <span class="ot">=</span> <span class="dv">66</span></span>
<span id="cb44-455"><a href="#cb44-455" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb44-456"><a href="#cb44-456" aria-hidden="true" tabindex="-1"></a>  <span class="at">theta =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.01</span>),</span>
<span id="cb44-457"><a href="#cb44-457" aria-hidden="true" tabindex="-1"></a>  <span class="at">prior =</span> <span class="fu">dgamma</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.01</span>), a, b),</span>
<span id="cb44-458"><a href="#cb44-458" aria-hidden="true" tabindex="-1"></a>  <span class="at">pos.theta1 =</span> <span class="fu">dgamma</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.01</span>), a <span class="sc">+</span> sy1, b <span class="sc">+</span> n1),</span>
<span id="cb44-459"><a href="#cb44-459" aria-hidden="true" tabindex="-1"></a>  <span class="at">pos.theta2 =</span> <span class="fu">dgamma</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.01</span>), a <span class="sc">+</span> sy2, b <span class="sc">+</span> n2)</span>
<span id="cb44-460"><a href="#cb44-460" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-461"><a href="#cb44-461" aria-hidden="true" tabindex="-1"></a>df.long <span class="ot">=</span> <span class="fu">melt</span>(df, <span class="at">id.vars =</span> <span class="st">'theta'</span>, <span class="at">variable_name =</span> <span class="st">'dist'</span>)</span>
<span id="cb44-462"><a href="#cb44-462" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df.long, <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> value, <span class="at">group =</span> dist, <span class="at">color =</span> dist)) <span class="sc">+</span></span>
<span id="cb44-463"><a href="#cb44-463" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb44-464"><a href="#cb44-464" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">'probability'</span>)</span>
<span id="cb44-465"><a href="#cb44-465" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-466"><a href="#cb44-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-467"><a href="#cb44-467" aria-hidden="true" tabindex="-1"></a>Notice that we can calculate the probability that $\theta_1 &gt; \theta_2$ by</span>
<span id="cb44-468"><a href="#cb44-468" aria-hidden="true" tabindex="-1"></a>integrating over the joint density $p(\theta_1, \theta_2)$ over the region where</span>
<span id="cb44-469"><a href="#cb44-469" aria-hidden="true" tabindex="-1"></a>$\theta_1 &gt; \theta_2$. This is calculated at the beginning of Chapter 4, and it</span>
<span id="cb44-470"><a href="#cb44-470" aria-hidden="true" tabindex="-1"></a>is about 0.97.</span>
<span id="cb44-471"><a href="#cb44-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-472"><a href="#cb44-472" aria-hidden="true" tabindex="-1"></a><span class="fu"># Exponential families and conjugate priors</span></span>
<span id="cb44-473"><a href="#cb44-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-474"><a href="#cb44-474" aria-hidden="true" tabindex="-1"></a>Binomial and Poisson models are *exponential family models*.</span>
<span id="cb44-475"><a href="#cb44-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-476"><a href="#cb44-476" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **Exponential family model**: a model whose densities can be expressed as</span></span>
<span id="cb44-477"><a href="#cb44-477" aria-hidden="true" tabindex="-1"></a><span class="at">$$p(y \mid \phi) = h(y)c(\phi)e^{\phi t(y)}$$ where $\phi$ is the unknown</span></span>
<span id="cb44-478"><a href="#cb44-478" aria-hidden="true" tabindex="-1"></a><span class="at">parameter and $t(y)$ is the sufficient statistic.</span></span>
<span id="cb44-479"><a href="#cb44-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-480"><a href="#cb44-480" aria-hidden="true" tabindex="-1"></a>Diaconis and Ylvisaker (1979) showed that the above class of models has conjugate prior densities</span>
<span id="cb44-481"><a href="#cb44-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-482"><a href="#cb44-482" aria-hidden="true" tabindex="-1"></a>$$p(\phi \mid n_0, t_0) = \kappa (n_0, t_0) c(\phi)^{n_0} e^{n_0 t_0 \phi}$$</span>
<span id="cb44-483"><a href="#cb44-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-484"><a href="#cb44-484" aria-hidden="true" tabindex="-1"></a>Where the posterior density is therefore</span>
<span id="cb44-485"><a href="#cb44-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-486"><a href="#cb44-486" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-487"><a href="#cb44-487" aria-hidden="true" tabindex="-1"></a>p(\phi \mid y_1, \dots, y_n) &amp;\propto p(\phi) \times p(y_1, \dots, y_n \mid \phi) <span class="sc">\\</span></span>
<span id="cb44-488"><a href="#cb44-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-489"><a href="#cb44-489" aria-hidden="true" tabindex="-1"></a>&amp;\propto \left<span class="co">[</span><span class="ot"> \kappa (n_0, t_0) c(\phi)^{n_0} \text{exp}(n_0 t_0 \phi) \right</span><span class="co">]</span> \times \left<span class="co">[</span><span class="ot"> \prod_{i = 1}^n \left( h(y_i) c(\phi) \text{exp}(\phi t(y)) \right) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb44-490"><a href="#cb44-490" aria-hidden="true" tabindex="-1"></a>&amp;\propto \left<span class="co">[</span><span class="ot"> \kappa (n_0, t_0) c(\phi)^{n_0} \text{exp}(n_0 t_0 \phi) \right</span><span class="co">]</span> \times \left<span class="co">[</span><span class="ot"> c(\phi)^n \text{exp}\left(\phi \sum_{i = 1}^n t(y_i)\right) \prod_{i = 1}^n h(y_i) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb44-491"><a href="#cb44-491" aria-hidden="true" tabindex="-1"></a>&amp;\propto \left<span class="co">[</span><span class="ot"> c(\phi)^{n_0} \text{exp}(n_0 t_0 \phi) \right</span><span class="co">]</span> \times \left<span class="co">[</span><span class="ot"> c(\phi)^n \text{exp}\left(\phi \sum_{i = 1}^n t(y_i)\right) \right</span><span class="co">]</span> &amp; \text{Discard constants} <span class="sc">\\</span></span>
<span id="cb44-492"><a href="#cb44-492" aria-hidden="true" tabindex="-1"></a>&amp;\propto c(\phi)^{n_0 + n} \text{exp}\left(\phi \times \left<span class="co">[</span><span class="ot">n_0 t_0 + \sum_{i = 1}^n t(y_i) \right</span><span class="co">]</span> \right) <span class="sc">\\</span></span>
<span id="cb44-493"><a href="#cb44-493" aria-hidden="true" tabindex="-1"></a>&amp;\propto p\left( \phi \; \middle| \; n_0 + n, \; n_0 t_0 + \sum_{i = 1}^n t(y_i) \right)</span>
<span id="cb44-494"><a href="#cb44-494" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-495"><a href="#cb44-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-496"><a href="#cb44-496" aria-hidden="true" tabindex="-1"></a>For this class of priors, we can interpret $n_0$ as a "prior sample size" and</span>
<span id="cb44-497"><a href="#cb44-497" aria-hidden="true" tabindex="-1"></a>$t_0$ is the prior expectation of the sufficient statistic $t(Y)$.</span>
<span id="cb44-498"><a href="#cb44-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-499"><a href="#cb44-499" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Binomial model</span></span>
<span id="cb44-500"><a href="#cb44-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-501"><a href="#cb44-501" aria-hidden="true" tabindex="-1"></a><span class="fu">### Parameterization</span></span>
<span id="cb44-502"><a href="#cb44-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-503"><a href="#cb44-503" aria-hidden="true" tabindex="-1"></a>We can obtain the representation from a single binary random variable (why?)</span>
<span id="cb44-504"><a href="#cb44-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-505"><a href="#cb44-505" aria-hidden="true" tabindex="-1"></a>$$p(y \mid \theta) = \theta^{y} (1 - \theta)^{1 - y}$$</span>
<span id="cb44-506"><a href="#cb44-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-507"><a href="#cb44-507" aria-hidden="true" tabindex="-1"></a>We can express this as an exponential family model by reparameterizing  by the log odds $\phi = \log \frac{\theta}{1 - \theta}$, so that $\theta = \frac{e^\phi}{e^\phi + 1}$:</span>
<span id="cb44-508"><a href="#cb44-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-509"><a href="#cb44-509" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-510"><a href="#cb44-510" aria-hidden="true" tabindex="-1"></a>p(y \mid \phi) &amp;= \left( \frac{e^\phi}{e^\phi + 1} \right)^y \left(\frac{1}{e^\phi + 1} \right)^{1 - y} <span class="sc">\\</span></span>
<span id="cb44-511"><a href="#cb44-511" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{e^{\phi y}}{(e^\phi + 1)^y} \frac{1}{e^\phi + 1} \left( e^\phi + 1\right)^y <span class="sc">\\</span></span>
<span id="cb44-512"><a href="#cb44-512" aria-hidden="true" tabindex="-1"></a>&amp;= e^{\phi y} (1 + e^\phi)^{-1} <span class="sc">\\</span></span>
<span id="cb44-513"><a href="#cb44-513" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-514"><a href="#cb44-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-515"><a href="#cb44-515" aria-hidden="true" tabindex="-1"></a>Here,</span>
<span id="cb44-516"><a href="#cb44-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-517"><a href="#cb44-517" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$h(y) = 1$</span>
<span id="cb44-518"><a href="#cb44-518" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$c(\phi) = (1 + e^\phi)^{-1}$</span>
<span id="cb44-519"><a href="#cb44-519" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$t(y) = y$</span>
<span id="cb44-520"><a href="#cb44-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-521"><a href="#cb44-521" aria-hidden="true" tabindex="-1"></a>(It's intuitive that the sufficient statistic is $y$)</span>
<span id="cb44-522"><a href="#cb44-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-523"><a href="#cb44-523" aria-hidden="true" tabindex="-1"></a><span class="fu">### Prior</span></span>
<span id="cb44-524"><a href="#cb44-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-525"><a href="#cb44-525" aria-hidden="true" tabindex="-1"></a>The conjugate prior on $\phi$ (discarding the constant $\kappa$) is</span>
<span id="cb44-526"><a href="#cb44-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-527"><a href="#cb44-527" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-528"><a href="#cb44-528" aria-hidden="true" tabindex="-1"></a>p(\phi \mid n_0, t_0) &amp;\propto (1 + e^{\phi})^{-n_0} e^{n_0 t_0 \phi}</span>
<span id="cb44-529"><a href="#cb44-529" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-530"><a href="#cb44-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-531"><a href="#cb44-531" aria-hidden="true" tabindex="-1"></a>To return to a density on $\theta$, let $\theta = g(\phi) = \frac{e^\phi}{e^\phi</span>
<span id="cb44-532"><a href="#cb44-532" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>1}$ and $\phi = h(\theta) = \log \frac{\theta}{1 - \theta}$. Then, using the</span>
<span id="cb44-533"><a href="#cb44-533" aria-hidden="true" tabindex="-1"></a>change of variables formula (Exercise 3.10),</span>
<span id="cb44-534"><a href="#cb44-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-535"><a href="#cb44-535" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-536"><a href="#cb44-536" aria-hidden="true" tabindex="-1"></a>p_{\theta}(\theta \mid n_0, t_0) &amp;= p_{\phi}(h(\theta) \mid n_0, t_0) \times \left| \frac{dh}{d\theta} \right| <span class="sc">\\</span></span>
<span id="cb44-537"><a href="#cb44-537" aria-hidden="true" tabindex="-1"></a>&amp;\propto \left(1 + \text{exp}\left(\log \left(\frac{\theta}{1 - \theta}\right)\right)\right)^{-n_0} \text{exp}\left(n_0 t_0 \log \left(\frac{\theta}{1 - \theta}\right) \right) \times \frac{1}{\theta - \theta^2} <span class="sc">\\</span></span>
<span id="cb44-538"><a href="#cb44-538" aria-hidden="true" tabindex="-1"></a>&amp;\propto \left(1 + \frac{\theta}{1 - \theta} \right)^{-n_0} \left(\frac{\theta}{1 - \theta}\right)^{n_0 t_0} \times \frac{1}{\theta - \theta^2} <span class="sc">\\</span></span>
<span id="cb44-539"><a href="#cb44-539" aria-hidden="true" tabindex="-1"></a>&amp;\propto \left(\frac{1}{1 - \theta} \right)^{-n_0} \left(\frac{\theta}{1 - \theta}\right)^{n_0 t_0} \times \frac{1}{\theta (1 - \theta)} <span class="sc">\\</span></span>
<span id="cb44-540"><a href="#cb44-540" aria-hidden="true" tabindex="-1"></a>&amp;\propto (1 - \theta)^{n_0} \theta^{n_0 t_0} (1 - \theta)^{-n_0 t_0} \theta^{-1} (1 - \theta)^{-1} <span class="sc">\\</span></span>
<span id="cb44-541"><a href="#cb44-541" aria-hidden="true" tabindex="-1"></a>&amp;\propto \theta^{n_0 t_0 - 1} (1 - \theta)^{n_0 - n_0t_0 - 1} <span class="sc">\\</span></span>
<span id="cb44-542"><a href="#cb44-542" aria-hidden="true" tabindex="-1"></a>&amp;\propto \theta^{n_0 t_0 - 1} (1 - \theta)^{n_0 (1 - t_0) - 1} <span class="sc">\\</span></span>
<span id="cb44-543"><a href="#cb44-543" aria-hidden="true" tabindex="-1"></a>&amp;= \text{dbeta}(\theta, n_0 t_0, n_0 (1 - t_0))</span>
<span id="cb44-544"><a href="#cb44-544" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-545"><a href="#cb44-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-546"><a href="#cb44-546" aria-hidden="true" tabindex="-1"></a>So the posterior is</span>
<span id="cb44-547"><a href="#cb44-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-548"><a href="#cb44-548" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-549"><a href="#cb44-549" aria-hidden="true" tabindex="-1"></a>p(\theta \mid y_1, \dots, y_n) &amp;= \text{dbeta}\left(\theta,\; (n_0 + n)\left(n_0 t_0 + \sum t(y_i) \right),\; (n_0 + n) \left(1 - n_0 t_0 - \sum t(y_i) \right) \right)</span>
<span id="cb44-550"><a href="#cb44-550" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-551"><a href="#cb44-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-552"><a href="#cb44-552" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example: Poisson model</span></span>
<span id="cb44-553"><a href="#cb44-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-554"><a href="#cb44-554" aria-hidden="true" tabindex="-1"></a><span class="fu">### Parameterization</span></span>
<span id="cb44-555"><a href="#cb44-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-556"><a href="#cb44-556" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-557"><a href="#cb44-557" aria-hidden="true" tabindex="-1"></a>p(y \mid \theta) &amp;= \frac{1}{y!} \theta^{y} e^{-\theta} <span class="sc">\\</span></span>
<span id="cb44-558"><a href="#cb44-558" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{y!} e^{y \log \theta} \text{exp}(-e^{\log \theta}) <span class="sc">\\</span></span>
<span id="cb44-559"><a href="#cb44-559" aria-hidden="true" tabindex="-1"></a>&amp;= h(y) c(\phi) e^{\phi t(y)} <span class="sc">\\</span></span>
<span id="cb44-560"><a href="#cb44-560" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-561"><a href="#cb44-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-562"><a href="#cb44-562" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb44-563"><a href="#cb44-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-564"><a href="#cb44-564" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\phi = \log \theta$</span>
<span id="cb44-565"><a href="#cb44-565" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$h(y) = \frac{1}{y!}$</span>
<span id="cb44-566"><a href="#cb44-566" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$t(y) = y$</span>
<span id="cb44-567"><a href="#cb44-567" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$c(\phi) = \text{exp}(-e^\phi)$ (The book has a typo here).</span>
<span id="cb44-568"><a href="#cb44-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-569"><a href="#cb44-569" aria-hidden="true" tabindex="-1"></a><span class="fu">### Prior</span></span>
<span id="cb44-570"><a href="#cb44-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-571"><a href="#cb44-571" aria-hidden="true" tabindex="-1"></a>Then the prior is $p(\phi \mid n_0, t_0) = \text{exp}(n_0 e^{-\phi}) e^{n_0 t_0</span>
<span id="cb44-572"><a href="#cb44-572" aria-hidden="true" tabindex="-1"></a>\phi}$. For time, I will do change of variables to show how this induces a</span>
<span id="cb44-573"><a href="#cb44-573" aria-hidden="true" tabindex="-1"></a>$\text{Gamma}(n_0 t_0, n_0)$ density on $\theta$.</span>
<span id="cb44-574"><a href="#cb44-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-575"><a href="#cb44-575" aria-hidden="true" tabindex="-1"></a><span class="fu"># Exercises</span></span>
<span id="cb44-576"><a href="#cb44-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-577"><a href="#cb44-577" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.1</span></span>
<span id="cb44-578"><a href="#cb44-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-579"><a href="#cb44-579" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb44-580"><a href="#cb44-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-581"><a href="#cb44-581" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-582"><a href="#cb44-582" aria-hidden="true" tabindex="-1"></a>P(Y_1 = y_1, \dots, Y_{100} &amp;= y_100 \mid \theta) &amp;= \theta^{\sum y_i} (1 - \theta)^{100 - \sum y_i} <span class="sc">\\</span></span>
<span id="cb44-583"><a href="#cb44-583" aria-hidden="true" tabindex="-1"></a>P(\sum Y_i = y \mid \theta) &amp;= {100 \choose y} \theta^{y}(1 - \theta)^{100 - y}</span>
<span id="cb44-584"><a href="#cb44-584" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-585"><a href="#cb44-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-586"><a href="#cb44-586" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb44-587"><a href="#cb44-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-590"><a href="#cb44-590" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-591"><a href="#cb44-591" aria-hidden="true" tabindex="-1"></a><span class="co"># Hand-implementing this, but you can use dbinom easily</span></span>
<span id="cb44-592"><a href="#cb44-592" aria-hidden="true" tabindex="-1"></a>my_dbinom <span class="ot">=</span> <span class="cf">function</span>(y, n, theta) <span class="fu">choose</span>(n, y) <span class="sc">*</span> theta<span class="sc">^</span>y <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> theta)<span class="sc">^</span>(n <span class="sc">-</span> y)</span>
<span id="cb44-593"><a href="#cb44-593" aria-hidden="true" tabindex="-1"></a>theta.discrete <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb44-594"><a href="#cb44-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-595"><a href="#cb44-595" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">=</span> <span class="dv">57</span></span>
<span id="cb44-596"><a href="#cb44-596" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb44-597"><a href="#cb44-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-598"><a href="#cb44-598" aria-hidden="true" tabindex="-1"></a>ps <span class="ot">=</span> <span class="fu">sapply</span>(theta.discrete, <span class="cf">function</span>(theta) <span class="fu">my_dbinom</span>(Y, N, theta))</span>
<span id="cb44-599"><a href="#cb44-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-600"><a href="#cb44-600" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">theta =</span> theta.discrete, <span class="at">p =</span> ps)</span>
<span id="cb44-601"><a href="#cb44-601" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(df, <span class="dv">3</span>))</span>
<span id="cb44-602"><a href="#cb44-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-603"><a href="#cb44-603" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> p)) <span class="sc">+</span></span>
<span id="cb44-604"><a href="#cb44-604" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">'identity'</span>) <span class="sc">+</span></span>
<span id="cb44-605"><a href="#cb44-605" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> theta.discrete)</span>
<span id="cb44-606"><a href="#cb44-606" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-607"><a href="#cb44-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-608"><a href="#cb44-608" aria-hidden="true" tabindex="-1"></a><span class="fu">### c</span></span>
<span id="cb44-609"><a href="#cb44-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-610"><a href="#cb44-610" aria-hidden="true" tabindex="-1"></a>If we have a uniform prior on beliefs of $\theta \in <span class="sc">\{</span>0, 0.1, \dots, 1.0<span class="sc">\}</span>$,</span>
<span id="cb44-611"><a href="#cb44-611" aria-hidden="true" tabindex="-1"></a>then $P(\theta = 0.0) = P(\theta = 0.1) = \dots = 1/11$.</span>
<span id="cb44-612"><a href="#cb44-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-613"><a href="#cb44-613" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-614"><a href="#cb44-614" aria-hidden="true" tabindex="-1"></a>p(\theta \mid \sum Y_i = 57) &amp;= \frac{p(\sum Y_i = 57 \mid \theta)p(\theta)}{p(\sum Y_i = 57)} <span class="sc">\\</span></span>
<span id="cb44-615"><a href="#cb44-615" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{p(\sum Y_i = 57 \mid \theta)p(\theta)}{\sum_{\theta'} p(\sum Y_i = 57 \mid \theta') p(\theta')} </span>
<span id="cb44-616"><a href="#cb44-616" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-617"><a href="#cb44-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-618"><a href="#cb44-618" aria-hidden="true" tabindex="-1"></a>Notice that $p(\theta)$ is constant for all $\theta$, so it can be pulled out of</span>
<span id="cb44-619"><a href="#cb44-619" aria-hidden="true" tabindex="-1"></a>the sum at the bottom and cancelled with the numerator:</span>
<span id="cb44-620"><a href="#cb44-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-621"><a href="#cb44-621" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-622"><a href="#cb44-622" aria-hidden="true" tabindex="-1"></a>p(\theta \mid \sum Y_i = 57)</span>
<span id="cb44-623"><a href="#cb44-623" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{p(\sum Y_i = 57 \mid \theta)}{\sum_{\theta'} p(\sum Y_i = 57 \mid \theta')} <span class="sc">\\</span></span>
<span id="cb44-624"><a href="#cb44-624" aria-hidden="true" tabindex="-1"></a>&amp;\propto p(\sum Y_i = 57 \mid \theta)</span>
<span id="cb44-625"><a href="#cb44-625" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-626"><a href="#cb44-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-627"><a href="#cb44-627" aria-hidden="true" tabindex="-1"></a>since the denominator is the constant</span>
<span id="cb44-628"><a href="#cb44-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-631"><a href="#cb44-631" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-632"><a href="#cb44-632" aria-hidden="true" tabindex="-1"></a>denom <span class="ot">=</span> <span class="fu">sum</span>(ps)</span>
<span id="cb44-633"><a href="#cb44-633" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(denom, <span class="dv">3</span>))</span>
<span id="cb44-634"><a href="#cb44-634" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-635"><a href="#cb44-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-636"><a href="#cb44-636" aria-hidden="true" tabindex="-1"></a>So the posterior distribution has the same shape, but different scale.</span>
<span id="cb44-637"><a href="#cb44-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-640"><a href="#cb44-640" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-641"><a href="#cb44-641" aria-hidden="true" tabindex="-1"></a>posteriors <span class="ot">=</span> <span class="fu">sapply</span>(theta.discrete, <span class="cf">function</span>(theta) <span class="fu">my_dbinom</span>(Y, N, theta) <span class="sc">/</span> denom)</span>
<span id="cb44-642"><a href="#cb44-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-643"><a href="#cb44-643" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">theta =</span> theta.discrete, <span class="at">p =</span> posteriors)</span>
<span id="cb44-644"><a href="#cb44-644" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(df, <span class="dv">3</span>))</span>
<span id="cb44-645"><a href="#cb44-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-646"><a href="#cb44-646" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> p)) <span class="sc">+</span></span>
<span id="cb44-647"><a href="#cb44-647" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">'identity'</span>) <span class="sc">+</span></span>
<span id="cb44-648"><a href="#cb44-648" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> theta.discrete)</span>
<span id="cb44-649"><a href="#cb44-649" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-650"><a href="#cb44-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-651"><a href="#cb44-651" aria-hidden="true" tabindex="-1"></a>Notice the denominator calculation could have been ignored - we could simply normalize the proportional prior densities</span>
<span id="cb44-652"><a href="#cb44-652" aria-hidden="true" tabindex="-1"></a>$p(\sum Y_i = 57 \mid \theta)$ to 1.</span>
<span id="cb44-653"><a href="#cb44-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-654"><a href="#cb44-654" aria-hidden="true" tabindex="-1"></a><span class="fu">### d</span></span>
<span id="cb44-655"><a href="#cb44-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-656"><a href="#cb44-656" aria-hidden="true" tabindex="-1"></a>Since $p(\theta) = 1$, the density $p(\theta) \times P(\sum Y_i = 57 \mid</span>
<span id="cb44-657"><a href="#cb44-657" aria-hidden="true" tabindex="-1"></a>\theta) = P(\sum Y_i = 57 \mid \theta)$. From (a),</span>
<span id="cb44-658"><a href="#cb44-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-659"><a href="#cb44-659" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb44-660"><a href="#cb44-660" aria-hidden="true" tabindex="-1"></a>P(\sum Y_i = 57 \mid \theta) = {100 \choose 57} \theta^{57} (1 - \theta)^{43}</span>
<span id="cb44-661"><a href="#cb44-661" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb44-662"><a href="#cb44-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-663"><a href="#cb44-663" aria-hidden="true" tabindex="-1"></a>which is implemented in the <span class="in">`my_dbinom`</span> function. So</span>
<span id="cb44-664"><a href="#cb44-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-665"><a href="#cb44-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-668"><a href="#cb44-668" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-669"><a href="#cb44-669" aria-hidden="true" tabindex="-1"></a>theta.continuous <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.001</span>)</span>
<span id="cb44-670"><a href="#cb44-670" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(theta.continuous, <span class="fu">sapply</span>(theta.continuous, <span class="cf">function</span>(theta) <span class="fu">my_dbinom</span>(Y, N, theta)),</span>
<span id="cb44-671"><a href="#cb44-671" aria-hidden="true" tabindex="-1"></a>      <span class="at">geom =</span> <span class="st">'line'</span>)</span>
<span id="cb44-672"><a href="#cb44-672" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-673"><a href="#cb44-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-674"><a href="#cb44-674" aria-hidden="true" tabindex="-1"></a><span class="fu">### e</span></span>
<span id="cb44-675"><a href="#cb44-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-676"><a href="#cb44-676" aria-hidden="true" tabindex="-1"></a>Treat the uniform prior as a $\text{Beta}(1, 1)$ distribution. Then $\left(</span>
<span id="cb44-677"><a href="#cb44-677" aria-hidden="true" tabindex="-1"></a>\theta \mid \sum Y_i = y \right) \sim \text{Beta}(58, 44)$.</span>
<span id="cb44-678"><a href="#cb44-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-681"><a href="#cb44-681" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-682"><a href="#cb44-682" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(theta.continuous, <span class="fu">dbeta</span>(theta.continuous, <span class="dv">58</span>, <span class="dv">44</span>), <span class="at">geom =</span> <span class="st">'line'</span>)</span>
<span id="cb44-683"><a href="#cb44-683" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-684"><a href="#cb44-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-685"><a href="#cb44-685" aria-hidden="true" tabindex="-1"></a>(d) is the posterior density before normalization by the constant $p(\sum Y_i = 57) = 0.099$. (e) is the posterior density fully, after normalization. Notice that (d)</span>
<span id="cb44-686"><a href="#cb44-686" aria-hidden="true" tabindex="-1"></a>and (e) have the same shape, due to the lack of influence of $p(\theta)$ on</span>
<span id="cb44-687"><a href="#cb44-687" aria-hidden="true" tabindex="-1"></a>posterior calculation.</span>
<span id="cb44-688"><a href="#cb44-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-689"><a href="#cb44-689" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.2</span></span>
<span id="cb44-690"><a href="#cb44-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-691"><a href="#cb44-691" aria-hidden="true" tabindex="-1"></a>For consistency, I will rewrite these as done in Chapter 1, where $\theta_0 = a</span>
<span id="cb44-692"><a href="#cb44-692" aria-hidden="true" tabindex="-1"></a>/ (a + b)$ is the initial guess of $\theta$ and $w = a + b$ is the strength of</span>
<span id="cb44-693"><a href="#cb44-693" aria-hidden="true" tabindex="-1"></a>that guess. Then,</span>
<span id="cb44-694"><a href="#cb44-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-697"><a href="#cb44-697" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-698"><a href="#cb44-698" aria-hidden="true" tabindex="-1"></a><span class="co"># What is the expected value of theta after observing result y, given a Beta</span></span>
<span id="cb44-699"><a href="#cb44-699" aria-hidden="true" tabindex="-1"></a><span class="co"># prior parameterized by theta0 and w?</span></span>
<span id="cb44-700"><a href="#cb44-700" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb44-701"><a href="#cb44-701" aria-hidden="true" tabindex="-1"></a>exp.posterior <span class="ot">=</span> <span class="cf">function</span>(w, theta0, y) {</span>
<span id="cb44-702"><a href="#cb44-702" aria-hidden="true" tabindex="-1"></a>  (N <span class="sc">/</span> (w <span class="sc">+</span> N)) <span class="sc">*</span> (y <span class="sc">/</span> N) <span class="sc">+</span> (w <span class="sc">/</span> (w <span class="sc">+</span> N)) <span class="sc">*</span> theta0</span>
<span id="cb44-703"><a href="#cb44-703" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb44-704"><a href="#cb44-704" aria-hidden="true" tabindex="-1"></a>Theta0 <span class="ot">=</span> <span class="fu">rev</span>(<span class="fu">seq</span>(<span class="fl">0.0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.1</span>))</span>
<span id="cb44-705"><a href="#cb44-705" aria-hidden="true" tabindex="-1"></a>W <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">32</span>, <span class="at">by =</span> <span class="fl">0.5</span>)</span>
<span id="cb44-706"><a href="#cb44-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-707"><a href="#cb44-707" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="dv">57</span></span>
<span id="cb44-708"><a href="#cb44-708" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">outer</span>(Theta0, W, <span class="at">FUN =</span> <span class="cf">function</span>(theta0, w) <span class="fu">exp.posterior</span>(w, theta0, <span class="dv">57</span>))</span>
<span id="cb44-709"><a href="#cb44-709" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(d) <span class="ot">=</span> Theta0</span>
<span id="cb44-710"><a href="#cb44-710" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(d) <span class="ot">=</span> W</span>
<span id="cb44-711"><a href="#cb44-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-712"><a href="#cb44-712" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">melt</span>(d)</span>
<span id="cb44-713"><a href="#cb44-713" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(df) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">'theta0'</span>, <span class="st">'w'</span>, <span class="st">'theta'</span>)</span>
<span id="cb44-714"><a href="#cb44-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-715"><a href="#cb44-715" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> w, <span class="at">y =</span> theta0, <span class="at">z =</span> theta)) <span class="sc">+</span></span>
<span id="cb44-716"><a href="#cb44-716" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_contour</span>(<span class="fu">aes</span>(<span class="at">colour =</span> ..level..)) <span class="sc">+</span></span>
<span id="cb44-717"><a href="#cb44-717" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>), <span class="at">labels =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>)) <span class="sc">+</span></span>
<span id="cb44-718"><a href="#cb44-718" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">breaks =</span> Theta0)</span>
<span id="cb44-719"><a href="#cb44-719" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(directlabels)</span>
<span id="cb44-720"><a href="#cb44-720" aria-hidden="true" tabindex="-1"></a><span class="fu">direct.label</span>(p, <span class="at">method =</span> <span class="st">'bottom.pieces'</span>)</span>
<span id="cb44-721"><a href="#cb44-721" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-722"><a href="#cb44-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-723"><a href="#cb44-723" aria-hidden="true" tabindex="-1"></a>One can use this plot to determine whether or not they should believe that</span>
<span id="cb44-724"><a href="#cb44-724" aria-hidden="true" tabindex="-1"></a>$\theta &gt; 0.5$ by quantifying their prior beliefs about the proportion with two</span>
<span id="cb44-725"><a href="#cb44-725" aria-hidden="true" tabindex="-1"></a>factors: $\theta_0$, an initial estimate of the true proportion, and $w$, the</span>
<span id="cb44-726"><a href="#cb44-726" aria-hidden="true" tabindex="-1"></a>"sample size" of observed individuals that contributed to the initial estimate.</span>
<span id="cb44-727"><a href="#cb44-727" aria-hidden="true" tabindex="-1"></a>Then, viewing the corresponding contour on this plot would give an estimate of</span>
<span id="cb44-728"><a href="#cb44-728" aria-hidden="true" tabindex="-1"></a>the posterior proportion given these two variables. It is shown here that</span>
<span id="cb44-729"><a href="#cb44-729" aria-hidden="true" tabindex="-1"></a>$\theta &gt; 0.5$ for most prior beliefs except for those with relatively low and</span>
<span id="cb44-730"><a href="#cb44-730" aria-hidden="true" tabindex="-1"></a>very strong estimates about $\theta_0$.</span>
<span id="cb44-731"><a href="#cb44-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-732"><a href="#cb44-732" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.3</span></span>
<span id="cb44-733"><a href="#cb44-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-734"><a href="#cb44-734" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb44-735"><a href="#cb44-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-738"><a href="#cb44-738" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-739"><a href="#cb44-739" aria-hidden="true" tabindex="-1"></a>ya <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">12</span>, <span class="dv">9</span>, <span class="dv">12</span>, <span class="dv">14</span>, <span class="dv">13</span>, <span class="dv">13</span>, <span class="dv">15</span>, <span class="dv">8</span>, <span class="dv">15</span>, <span class="dv">6</span>)</span>
<span id="cb44-740"><a href="#cb44-740" aria-hidden="true" tabindex="-1"></a>yb <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">11</span>, <span class="dv">11</span>, <span class="dv">10</span>, <span class="dv">9</span>, <span class="dv">9</span>, <span class="dv">8</span>, <span class="dv">7</span>, <span class="dv">10</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">7</span>)</span>
<span id="cb44-741"><a href="#cb44-741" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-742"><a href="#cb44-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-743"><a href="#cb44-743" aria-hidden="true" tabindex="-1"></a>Notice that $\sum y_a = 117, n_a = 10, \sum y_b = 113, n_b = 13$.</span>
<span id="cb44-744"><a href="#cb44-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-745"><a href="#cb44-745" aria-hidden="true" tabindex="-1"></a>If</span>
<span id="cb44-746"><a href="#cb44-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-747"><a href="#cb44-747" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-748"><a href="#cb44-748" aria-hidden="true" tabindex="-1"></a>\theta_A &amp;\sim \text{Gamma}(120, 10) <span class="sc">\\</span></span>
<span id="cb44-749"><a href="#cb44-749" aria-hidden="true" tabindex="-1"></a>\theta_B &amp;\sim \text{Gamma}(12, 1)</span>
<span id="cb44-750"><a href="#cb44-750" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-751"><a href="#cb44-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-752"><a href="#cb44-752" aria-hidden="true" tabindex="-1"></a>then</span>
<span id="cb44-753"><a href="#cb44-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-754"><a href="#cb44-754" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-755"><a href="#cb44-755" aria-hidden="true" tabindex="-1"></a>\theta_A \mid \mathbf{y}_a &amp;\sim \text{Gamma}(120 + 117, 10 + 10) = \text{Gamma}(237, 20) <span class="sc">\\</span></span>
<span id="cb44-756"><a href="#cb44-756" aria-hidden="true" tabindex="-1"></a>\theta_B \mid \mathbf{y}_b &amp;\sim \text{Gamma}(12 + 113, 1 + 13) = \text{Gamma}(125, 14) <span class="sc">\\</span></span>
<span id="cb44-757"><a href="#cb44-757" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-758"><a href="#cb44-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-759"><a href="#cb44-759" aria-hidden="true" tabindex="-1"></a>So</span>
<span id="cb44-760"><a href="#cb44-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-761"><a href="#cb44-761" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-762"><a href="#cb44-762" aria-hidden="true" tabindex="-1"></a>\mathbb{E}(\theta_A) &amp;=  237/20 = 11.85<span class="sc">\\</span></span>
<span id="cb44-763"><a href="#cb44-763" aria-hidden="true" tabindex="-1"></a>\mathbb{E}(\theta_A) &amp;= 125/14 = 8.92<span class="sc">\\</span></span>
<span id="cb44-764"><a href="#cb44-764" aria-hidden="true" tabindex="-1"></a>\text{Var}(\theta_A) &amp;= 237/400 = 0.593<span class="sc">\\</span></span>
<span id="cb44-765"><a href="#cb44-765" aria-hidden="true" tabindex="-1"></a>\text{Var}(\theta_B) &amp;= 125/196 = 0.638<span class="sc">\\</span></span>
<span id="cb44-766"><a href="#cb44-766" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-767"><a href="#cb44-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-768"><a href="#cb44-768" aria-hidden="true" tabindex="-1"></a>95% quantile-based confidence intervals can be solved by setting the CDF of the</span>
<span id="cb44-769"><a href="#cb44-769" aria-hidden="true" tabindex="-1"></a>Gammas to $p$, and solving for $\theta$. Alternatively,</span>
<span id="cb44-770"><a href="#cb44-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-773"><a href="#cb44-773" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-774"><a href="#cb44-774" aria-hidden="true" tabindex="-1"></a><span class="fu">qgamma</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="dv">237</span>, <span class="dv">20</span>)</span>
<span id="cb44-775"><a href="#cb44-775" aria-hidden="true" tabindex="-1"></a><span class="fu">qgamma</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="dv">125</span>, <span class="dv">14</span>)</span>
<span id="cb44-776"><a href="#cb44-776" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-777"><a href="#cb44-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-778"><a href="#cb44-778" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb44-779"><a href="#cb44-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-782"><a href="#cb44-782" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-783"><a href="#cb44-783" aria-hidden="true" tabindex="-1"></a>ya <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">12</span>, <span class="dv">9</span>, <span class="dv">12</span>, <span class="dv">14</span>, <span class="dv">13</span>, <span class="dv">13</span>, <span class="dv">15</span>, <span class="dv">8</span>, <span class="dv">15</span>, <span class="dv">6</span>)</span>
<span id="cb44-784"><a href="#cb44-784" aria-hidden="true" tabindex="-1"></a>yb <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">11</span>, <span class="dv">11</span>, <span class="dv">10</span>, <span class="dv">9</span>, <span class="dv">9</span>, <span class="dv">8</span>, <span class="dv">7</span>, <span class="dv">10</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">7</span>)</span>
<span id="cb44-785"><a href="#cb44-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-786"><a href="#cb44-786" aria-hidden="true" tabindex="-1"></a>n0 <span class="ot">=</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span></span>
<span id="cb44-787"><a href="#cb44-787" aria-hidden="true" tabindex="-1"></a>exps <span class="ot">=</span> (<span class="dv">12</span> <span class="sc">*</span> n0 <span class="sc">+</span> <span class="fu">sum</span>(yb)) <span class="sc">/</span> (n0 <span class="sc">+</span> <span class="fu">length</span>(yb))</span>
<span id="cb44-788"><a href="#cb44-788" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(n0, exps, <span class="at">geom =</span> <span class="fu">c</span>(<span class="st">'point'</span>, <span class="st">'smooth'</span>))</span>
<span id="cb44-789"><a href="#cb44-789" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-790"><a href="#cb44-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-791"><a href="#cb44-791" aria-hidden="true" tabindex="-1"></a>Very strong prior beliefs that the expected value of $\theta \approx 12$ </span>
<span id="cb44-792"><a href="#cb44-792" aria-hidden="true" tabindex="-1"></a>would be necessary, because $\theta_{ML} = 8.69$ which is quite low. According</span>
<span id="cb44-793"><a href="#cb44-793" aria-hidden="true" tabindex="-1"></a>to the graph $n_0$ values of close to 50 are required.</span>
<span id="cb44-794"><a href="#cb44-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-795"><a href="#cb44-795" aria-hidden="true" tabindex="-1"></a><span class="fu">### c</span></span>
<span id="cb44-796"><a href="#cb44-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-797"><a href="#cb44-797" aria-hidden="true" tabindex="-1"></a>We have existing knowledge about population A. Knowing that B is related, we</span>
<span id="cb44-798"><a href="#cb44-798" aria-hidden="true" tabindex="-1"></a>have incorporated these beliefs into our prior on population B. However, nothing</span>
<span id="cb44-799"><a href="#cb44-799" aria-hidden="true" tabindex="-1"></a>more than a weak prior expectation of B to be similar to A should be encoded in</span>
<span id="cb44-800"><a href="#cb44-800" aria-hidden="true" tabindex="-1"></a>our analysis, since it is entirely possible that the parameter of B is quite </span>
<span id="cb44-801"><a href="#cb44-801" aria-hidden="true" tabindex="-1"></a>different from A. So we should view the populations as independent.</span>
<span id="cb44-802"><a href="#cb44-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-803"><a href="#cb44-803" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.4</span></span>
<span id="cb44-804"><a href="#cb44-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-805"><a href="#cb44-805" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb44-806"><a href="#cb44-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-807"><a href="#cb44-807" aria-hidden="true" tabindex="-1"></a>This is fairly straightforward like 3.1, skipping</span>
<span id="cb44-808"><a href="#cb44-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-809"><a href="#cb44-809" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb44-810"><a href="#cb44-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-811"><a href="#cb44-811" aria-hidden="true" tabindex="-1"></a>This is fairly straightforward like 3.1, skipping</span>
<span id="cb44-812"><a href="#cb44-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-813"><a href="#cb44-813" aria-hidden="true" tabindex="-1"></a><span class="fu">### c</span></span>
<span id="cb44-814"><a href="#cb44-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-817"><a href="#cb44-817" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-818"><a href="#cb44-818" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">=</span> <span class="cf">function</span>(theta) {</span>
<span id="cb44-819"><a href="#cb44-819" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">1</span><span class="sc">/</span><span class="dv">4</span>) <span class="sc">*</span> (<span class="fu">gamma</span>(<span class="dv">10</span>) <span class="sc">/</span> (<span class="fu">gamma</span>(<span class="dv">2</span>) <span class="sc">*</span> <span class="fu">gamma</span>(<span class="dv">8</span>))) <span class="sc">*</span> </span>
<span id="cb44-820"><a href="#cb44-820" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">3</span> <span class="sc">*</span> theta <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> theta)<span class="sc">^</span><span class="dv">7</span> <span class="sc">+</span> theta<span class="sc">^</span><span class="dv">7</span> <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> theta))</span>
<span id="cb44-821"><a href="#cb44-821" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb44-822"><a href="#cb44-822" aria-hidden="true" tabindex="-1"></a>thetas <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.005</span>)</span>
<span id="cb44-823"><a href="#cb44-823" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(thetas, <span class="fu">prior</span>(thetas), <span class="at">geom =</span> <span class="st">'line'</span>)</span>
<span id="cb44-824"><a href="#cb44-824" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-825"><a href="#cb44-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-826"><a href="#cb44-826" aria-hidden="true" tabindex="-1"></a>Some kind of bimodal prior on rates of teen recidivism - for example, you</span>
<span id="cb44-827"><a href="#cb44-827" aria-hidden="true" tabindex="-1"></a>believe that in some regions, teen recidivism is rather low, but there are some</span>
<span id="cb44-828"><a href="#cb44-828" aria-hidden="true" tabindex="-1"></a>regions where teen recidivism is high.</span>
<span id="cb44-829"><a href="#cb44-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-830"><a href="#cb44-830" aria-hidden="true" tabindex="-1"></a><span class="fu">### d</span></span>
<span id="cb44-831"><a href="#cb44-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-832"><a href="#cb44-832" aria-hidden="true" tabindex="-1"></a><span class="fu">#### i</span></span>
<span id="cb44-833"><a href="#cb44-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-834"><a href="#cb44-834" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-835"><a href="#cb44-835" aria-hidden="true" tabindex="-1"></a>p(\theta) \times p(y \mid \theta) &amp;= \frac{1}{4}\frac{\Gamma(10)}{\Gamma(2) \Gamma(8)} \left<span class="co">[</span><span class="ot"> 3\theta (1 - \theta)^7 + \theta^7 (1 - \theta) \right</span><span class="co">]</span> \times \left<span class="co">[</span><span class="ot"> {43 \choose 15} \theta^{15} (1 - \theta)^{28} \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb44-836"><a href="#cb44-836" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{4} \frac{\Gamma(44)}{\Gamma(16) \Gamma(29)} \frac{\Gamma(10)}{\Gamma(2) \Gamma(8)} \left<span class="co">[</span><span class="ot">3\theta^{16} (1 - \theta)^{35} + \theta^{22} (1 - \theta)^{29} \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb44-837"><a href="#cb44-837" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-838"><a href="#cb44-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-839"><a href="#cb44-839" aria-hidden="true" tabindex="-1"></a><span class="fu">#### ii</span></span>
<span id="cb44-840"><a href="#cb44-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-841"><a href="#cb44-841" aria-hidden="true" tabindex="-1"></a>This is proportional to some mixture with unknown weights of a $\text{Beta}(17,</span>
<span id="cb44-842"><a href="#cb44-842" aria-hidden="true" tabindex="-1"></a>36)$ and a $\text{Beta}(23, 30)$ which intuitively are the posterior densities</span>
<span id="cb44-843"><a href="#cb44-843" aria-hidden="true" tabindex="-1"></a>in parts a and b.</span>
<span id="cb44-844"><a href="#cb44-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-845"><a href="#cb44-845" aria-hidden="true" tabindex="-1"></a><span class="fu">#### iii</span></span>
<span id="cb44-846"><a href="#cb44-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-849"><a href="#cb44-849" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-850"><a href="#cb44-850" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">=</span> <span class="cf">function</span>(theta) {</span>
<span id="cb44-851"><a href="#cb44-851" aria-hidden="true" tabindex="-1"></a>  <span class="fu">prior</span>(theta) <span class="sc">*</span> <span class="fu">choose</span>(<span class="dv">43</span>, <span class="dv">15</span>) <span class="sc">*</span> theta<span class="sc">^</span><span class="dv">15</span> <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> theta)<span class="sc">^</span><span class="dv">28</span></span>
<span id="cb44-852"><a href="#cb44-852" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb44-853"><a href="#cb44-853" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(thetas, <span class="fu">posterior</span>(thetas), <span class="at">geom =</span> <span class="st">'line'</span>)</span>
<span id="cb44-854"><a href="#cb44-854" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Mode:"</span>, thetas[<span class="fu">which.max</span>(<span class="fu">posterior</span>(thetas))], <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb44-855"><a href="#cb44-855" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-856"><a href="#cb44-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-857"><a href="#cb44-857" aria-hidden="true" tabindex="-1"></a>Notice that</span>
<span id="cb44-858"><a href="#cb44-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-859"><a href="#cb44-859" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-860"><a href="#cb44-860" aria-hidden="true" tabindex="-1"></a>\text{mode}(\text{Beta}(17, 36)) &amp;= (17 - 1) / (17 + 36 - 2) = 0.313 <span class="sc">\\</span></span>
<span id="cb44-861"><a href="#cb44-861" aria-hidden="true" tabindex="-1"></a>\text{mode}(\text{Beta}(23, 30)) &amp;= (23 - 1) / (23 + 30 - 2) = 0.431</span>
<span id="cb44-862"><a href="#cb44-862" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-863"><a href="#cb44-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-864"><a href="#cb44-864" aria-hidden="true" tabindex="-1"></a>So the mode is between these two modes, although closer to that of the $\text{Beta}(17, 36)$.</span>
<span id="cb44-865"><a href="#cb44-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-866"><a href="#cb44-866" aria-hidden="true" tabindex="-1"></a><span class="fu">### e</span></span>
<span id="cb44-867"><a href="#cb44-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-868"><a href="#cb44-868" aria-hidden="true" tabindex="-1"></a>Unclear: general formula for any beta mixture with any weights and parameters?</span>
<span id="cb44-869"><a href="#cb44-869" aria-hidden="true" tabindex="-1"></a>Or general formula for any observed result for this model, given then prior in</span>
<span id="cb44-870"><a href="#cb44-870" aria-hidden="true" tabindex="-1"></a>c)?</span>
<span id="cb44-871"><a href="#cb44-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-872"><a href="#cb44-872" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.5</span></span>
<span id="cb44-873"><a href="#cb44-873" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-874"><a href="#cb44-874" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb44-875"><a href="#cb44-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-876"><a href="#cb44-876" aria-hidden="true" tabindex="-1"></a>First,</span>
<span id="cb44-877"><a href="#cb44-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-878"><a href="#cb44-878" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-879"><a href="#cb44-879" aria-hidden="true" tabindex="-1"></a>\tilde{p} &amp;= \sum_{k = 1}^K w_k p_k (\theta \mid n_{0, k}, t_{0, k}) <span class="sc">\\</span></span>
<span id="cb44-880"><a href="#cb44-880" aria-hidden="true" tabindex="-1"></a>&amp;= \sum_{k = 1}^K \left( w_k \kappa (n_{0, k}, t_{0, k}) c(\phi)^{n_{0, k}} \text{exp}(n_{0, k} t_{0, k} \phi) \right) <span class="sc">\\</span></span>
<span id="cb44-881"><a href="#cb44-881" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-882"><a href="#cb44-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-883"><a href="#cb44-883" aria-hidden="true" tabindex="-1"></a>Now</span>
<span id="cb44-884"><a href="#cb44-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-885"><a href="#cb44-885" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-886"><a href="#cb44-886" aria-hidden="true" tabindex="-1"></a>p(\phi \mid y_1, \dots, y_n) &amp;\propto p(\phi) p(y_1, \dots, y_n \mid \phi) <span class="sc">\\</span></span>
<span id="cb44-887"><a href="#cb44-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-888"><a href="#cb44-888" aria-hidden="true" tabindex="-1"></a>&amp;\propto \left<span class="co">[</span><span class="ot"> \sum_{k = 1}^K \left( w_k \kappa (n_{0, k}, t_{0, k}) c(\phi)^{n_{0, k}} \text{exp}(n_{0, k} t_{0, k} \phi) \right)  \right</span><span class="co">]</span> \times \left<span class="co">[</span><span class="ot"> \prod_{i = 1}^n h(y_i) c(\phi) \text{exp}(\phi t(y_i)) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb44-889"><a href="#cb44-889" aria-hidden="true" tabindex="-1"></a>&amp;\propto \left<span class="co">[</span><span class="ot"> \sum_{k = 1}^K \left( w_k \kappa (n_{0, k}, t_{0, k}) c(\phi)^{n_{0, k}} \text{exp}(n_{0, k} t_{0, k} \phi) \right)  \right</span><span class="co">]</span> \times \left<span class="co">[</span><span class="ot"> \text{exp}\left(\phi \sum_{i = 1}^n t(y_i)\right) c(\phi)^n \prod_{i = 1}^n h(y_i) \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb44-890"><a href="#cb44-890" aria-hidden="true" tabindex="-1"></a>&amp;\propto \left<span class="co">[</span><span class="ot"> \sum_{k = 1}^K \left( w_k \kappa (n_{0, k}, t_{0, k}) c(\phi)^{n_{0, k}} \text{exp}(n_{0, k} t_{0, k} \phi) \right)  \right</span><span class="co">]</span> \times \left<span class="co">[</span><span class="ot"> \text{exp}\left(\phi \sum_{i = 1}^n t(y_i)\right) c(\phi)^n \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb44-891"><a href="#cb44-891" aria-hidden="true" tabindex="-1"></a>&amp;\propto \sum_{k = 1}^K \left( w_k \kappa (n_{0, k}, t_{0, k}) c(\phi)^{n_{0, k} + n} \text{exp}\left(\phi \times \left<span class="co">[</span><span class="ot"> n_{0, k} t_{0, k}+ \sum_{i = 1}^n t(y_i) \right</span><span class="co">]</span> \right) \right) <span class="sc">\\</span></span>
<span id="cb44-892"><a href="#cb44-892" aria-hidden="true" tabindex="-1"></a>&amp;\propto \sum_{k = 1}^K w_k p\left(\theta \; \middle| \; n_0 + n, \; n_0 t_0 + \sum_{i = 1}^{n} t(y_i)\right)</span>
<span id="cb44-893"><a href="#cb44-893" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-894"><a href="#cb44-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-895"><a href="#cb44-895" aria-hidden="true" tabindex="-1"></a>So the posterior is another weighted mixture. However I don't believe the </span>
<span id="cb44-896"><a href="#cb44-896" aria-hidden="true" tabindex="-1"></a>weights of the relative components are preserved (neither are they in the Beta mixtures</span>
<span id="cb44-897"><a href="#cb44-897" aria-hidden="true" tabindex="-1"></a>above).</span>
<span id="cb44-898"><a href="#cb44-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-899"><a href="#cb44-899" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb44-900"><a href="#cb44-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-901"><a href="#cb44-901" aria-hidden="true" tabindex="-1"></a>Not specified whether we need to use the exponential family parameterization or</span>
<span id="cb44-902"><a href="#cb44-902" aria-hidden="true" tabindex="-1"></a>the standard parameterization. I will use the standard parameterization.</span>
<span id="cb44-903"><a href="#cb44-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-904"><a href="#cb44-904" aria-hidden="true" tabindex="-1"></a>Let</span>
<span id="cb44-905"><a href="#cb44-905" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-906"><a href="#cb44-906" aria-hidden="true" tabindex="-1"></a>\tilde{p} &amp;= \sum_{k = 1}^K w_k p_k (\theta \mid a_k, b_k) <span class="sc">\\</span></span>
<span id="cb44-907"><a href="#cb44-907" aria-hidden="true" tabindex="-1"></a>&amp;= \sum_{k = 1}^K \left( w_k \frac{b_k^{a_k}}{\Gamma(a_k)} x^{a_k - 1} e^{-b_k x} \right) <span class="sc">\\</span></span>
<span id="cb44-908"><a href="#cb44-908" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-909"><a href="#cb44-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-910"><a href="#cb44-910" aria-hidden="true" tabindex="-1"></a>Then</span>
<span id="cb44-911"><a href="#cb44-911" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-912"><a href="#cb44-912" aria-hidden="true" tabindex="-1"></a>p(\theta \mid y_1, \dots, y_n) &amp;\propto p(\theta) p(y_1, \dots, y_n \mid \theta) <span class="sc">\\</span></span>
<span id="cb44-913"><a href="#cb44-913" aria-hidden="true" tabindex="-1"></a>&amp;\propto \left<span class="co">[</span><span class="ot"> \sum_{k = 1}^K \left( w_k \frac{b_k^{a_k}}{\Gamma(a_k)} \theta^{a_k - 1} e^{-b_k \theta} \right) \right</span><span class="co">]</span> \times \left<span class="co">[</span><span class="ot"> \prod_{i = 1}^n \frac{1}{y_i !} \theta^{y_i} e^{-\theta} \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb44-914"><a href="#cb44-914" aria-hidden="true" tabindex="-1"></a>&amp;\propto \left<span class="co">[</span><span class="ot"> \sum_{k = 1}^K \left( w_k \frac{b_k^{a_k}}{\Gamma(a_k)} \theta^{a_k - 1} e^{-b_k \theta} \right) \right</span><span class="co">]</span> \times \left<span class="co">[</span><span class="ot"> \theta^{\sum y_i} e^{-n\theta} \right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb44-915"><a href="#cb44-915" aria-hidden="true" tabindex="-1"></a>&amp;\propto \sum_{k = 1}^K \left( w_k \frac{b_k^{a_k}}{\Gamma(a_k)} \theta^{a_k + \sum y_i - 1} e^{-(b_k + n)\theta} \right) <span class="sc">\\</span></span>
<span id="cb44-916"><a href="#cb44-916" aria-hidden="true" tabindex="-1"></a>&amp;\propto \sum_{k = 1}^K w_k p\left(\theta \; \middle| \; a_k + \sum y_i, \; b_k + n \right) <span class="sc">\\</span></span>
<span id="cb44-917"><a href="#cb44-917" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-918"><a href="#cb44-918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-919"><a href="#cb44-919" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.9</span></span>
<span id="cb44-920"><a href="#cb44-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-921"><a href="#cb44-921" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb44-922"><a href="#cb44-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-923"><a href="#cb44-923" aria-hidden="true" tabindex="-1"></a>We take advantage of the fact that the Galenshore distribution can be viewed as</span>
<span id="cb44-924"><a href="#cb44-924" aria-hidden="true" tabindex="-1"></a>an exponential model</span>
<span id="cb44-925"><a href="#cb44-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-926"><a href="#cb44-926" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-927"><a href="#cb44-927" aria-hidden="true" tabindex="-1"></a>p(y \mid \theta) &amp;= \frac{2}{\Gamma(a)} \theta^{2a} y^{2a - 1} e^{-\theta^2 y^2} <span class="sc">\\</span></span>
<span id="cb44-928"><a href="#cb44-928" aria-hidden="true" tabindex="-1"></a>&amp;= \left( \frac{2}{\Gamma(a)} y^{2a - 1} \right) \times \left(\theta^{2a} \right) \times \left( e^{-\theta^2 y^2} \right) <span class="sc">\\</span></span>
<span id="cb44-929"><a href="#cb44-929" aria-hidden="true" tabindex="-1"></a>&amp;= \left( \frac{2}{\Gamma(a)} y^{2a - 1} \right) \times \left(\theta^2 \right)^a \times \left( e^{\left(\theta^2\right) -y^2} \right) <span class="sc">\\</span></span>
<span id="cb44-930"><a href="#cb44-930" aria-hidden="true" tabindex="-1"></a>&amp;= h(y) c(\phi) e^{\phi t(y)}</span>
<span id="cb44-931"><a href="#cb44-931" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-932"><a href="#cb44-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-933"><a href="#cb44-933" aria-hidden="true" tabindex="-1"></a>Where</span>
<span id="cb44-934"><a href="#cb44-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-935"><a href="#cb44-935" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$h(y) = \frac{2}{\Gamma(a)} y^{2a - 1}$</span>
<span id="cb44-936"><a href="#cb44-936" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$c(\phi) = \phi^a$</span>
<span id="cb44-937"><a href="#cb44-937" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$t(y) = -(y^2)$</span>
<span id="cb44-938"><a href="#cb44-938" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\phi = \theta^2$</span>
<span id="cb44-939"><a href="#cb44-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-940"><a href="#cb44-940" aria-hidden="true" tabindex="-1"></a>Then, the conjugate priors for the $\phi$ parameterization are given by</span>
<span id="cb44-941"><a href="#cb44-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-942"><a href="#cb44-942" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-943"><a href="#cb44-943" aria-hidden="true" tabindex="-1"></a>p(\phi \mid n_0, t_0) &amp;= \kappa (n_0, t_0) \phi^{a n_0} \text{exp}(n_0 t_0 \phi) <span class="sc">\\</span></span>
<span id="cb44-944"><a href="#cb44-944" aria-hidden="true" tabindex="-1"></a>&amp;\propto \phi^{a n_0} \text{exp}(n_0 t_0 \phi) <span class="sc">\\</span></span>
<span id="cb44-945"><a href="#cb44-945" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-946"><a href="#cb44-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-947"><a href="#cb44-947" aria-hidden="true" tabindex="-1"></a>To obtain the priors for $\theta$, let $\theta = g(\phi) = \sqrt{\phi}$ and</span>
<span id="cb44-948"><a href="#cb44-948" aria-hidden="true" tabindex="-1"></a>$\phi = h(\theta) = \theta^2$. Notice $dh/d\theta = 2\theta$. By the change of</span>
<span id="cb44-949"><a href="#cb44-949" aria-hidden="true" tabindex="-1"></a>variables formula,</span>
<span id="cb44-950"><a href="#cb44-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-951"><a href="#cb44-951" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-952"><a href="#cb44-952" aria-hidden="true" tabindex="-1"></a>p(\theta \mid n_0, t_0) &amp;= p(h(\theta) \mid n_0, t_0) \times \left| \frac{dh}{d\theta} \right| <span class="sc">\\</span></span>
<span id="cb44-953"><a href="#cb44-953" aria-hidden="true" tabindex="-1"></a>&amp;\propto \kappa(n_0, t_0) \theta^{2a n_0} \text{exp}\left( n_0 t_0 \theta^2 \right) \times 2 \theta <span class="sc">\\</span></span>
<span id="cb44-954"><a href="#cb44-954" aria-hidden="true" tabindex="-1"></a>&amp;\propto \theta^{2a n_0 + 1} \text{exp}\left( n_0 t_0 \theta^2 \right) <span class="sc">\\</span></span>
<span id="cb44-955"><a href="#cb44-955" aria-hidden="true" tabindex="-1"></a>&amp;\propto \text{dgalenshore}\left(\theta, \underbrace{a n_0 + 1}_{a_{\text{Galenshore}}}, \underbrace{\sqrt{-n_0 t_0}}_{\theta_{\text{Galenshore}}} \right)</span>
<span id="cb44-956"><a href="#cb44-956" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-957"><a href="#cb44-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-958"><a href="#cb44-958" aria-hidden="true" tabindex="-1"></a>Notice this is true since $\text{dgalenshore}(y, a, \theta) \propto y^{2a - 1}</span>
<span id="cb44-959"><a href="#cb44-959" aria-hidden="true" tabindex="-1"></a>e^{- \theta^2 y^2}$ - the rest of the PDF is a constant that doesn't depend on</span>
<span id="cb44-960"><a href="#cb44-960" aria-hidden="true" tabindex="-1"></a>$y$. Also notice that $\sqrt{-n_0 t_0}$ is defined since $n_0 &gt; 0$ and $t_0$ is</span>
<span id="cb44-961"><a href="#cb44-961" aria-hidden="true" tabindex="-1"></a>the initial "guess" of $t(y) = - (y^2) &lt; 0$, so $-n_0 t_0 &gt; 0$.</span>
<span id="cb44-962"><a href="#cb44-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-965"><a href="#cb44-965" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-966"><a href="#cb44-966" aria-hidden="true" tabindex="-1"></a>dgalenshore <span class="ot">=</span> <span class="cf">function</span>(y, a, theta) {</span>
<span id="cb44-967"><a href="#cb44-967" aria-hidden="true" tabindex="-1"></a>  (<span class="dv">2</span> <span class="sc">/</span> <span class="fu">gamma</span>(a)) <span class="sc">*</span> theta<span class="sc">^</span>(<span class="dv">2</span> <span class="sc">*</span> a) <span class="sc">*</span> y<span class="sc">^</span>(<span class="dv">2</span> <span class="sc">*</span> a <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="dv">1</span> <span class="sc">*</span> (theta<span class="sc">^</span><span class="dv">2</span>) <span class="sc">*</span> y<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb44-968"><a href="#cb44-968" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb44-969"><a href="#cb44-969" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">seq</span>(<span class="fl">0.02</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.02</span>)</span>
<span id="cb44-970"><a href="#cb44-970" aria-hidden="true" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">rbind</span>(</span>
<span id="cb44-971"><a href="#cb44-971" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">y =</span> y, <span class="at">density =</span> <span class="fu">dgalenshore</span>(y, <span class="dv">1</span>, <span class="dv">1</span>), <span class="at">dist =</span> <span class="st">'alpha = 1, theta = 1'</span>),</span>
<span id="cb44-972"><a href="#cb44-972" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">y =</span> y, <span class="at">density =</span> <span class="fu">dgalenshore</span>(y, <span class="dv">1</span>, <span class="dv">3</span>), <span class="at">dist =</span> <span class="st">'alpha = 1, theta = 3'</span>),</span>
<span id="cb44-973"><a href="#cb44-973" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">y =</span> y, <span class="at">density =</span> <span class="fu">dgalenshore</span>(y, <span class="dv">3</span>, <span class="dv">1</span>), <span class="at">dist =</span> <span class="st">'alpha = 3, theta = 1'</span>),</span>
<span id="cb44-974"><a href="#cb44-974" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(<span class="at">y =</span> y, <span class="at">density =</span> <span class="fu">dgalenshore</span>(y, <span class="dv">4</span>, <span class="dv">2</span>), <span class="at">dist =</span> <span class="st">'alpha = 4, theta = 2'</span>)</span>
<span id="cb44-975"><a href="#cb44-975" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-976"><a href="#cb44-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-977"><a href="#cb44-977" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> y, <span class="at">y =</span> density, <span class="at">group =</span> dist, <span class="at">color =</span> dist)) <span class="sc">+</span></span>
<span id="cb44-978"><a href="#cb44-978" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>()</span>
<span id="cb44-979"><a href="#cb44-979" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-980"><a href="#cb44-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-981"><a href="#cb44-981" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb44-982"><a href="#cb44-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-983"><a href="#cb44-983" aria-hidden="true" tabindex="-1"></a>Since this is an exponential family, the posterior of $\phi$ is given by $p(\phi \mid n_0 + n, n_0 t_0 + n\bar{t}(\mathbf{y}))$. This means that</span>
<span id="cb44-984"><a href="#cb44-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-985"><a href="#cb44-985" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-986"><a href="#cb44-986" aria-hidden="true" tabindex="-1"></a>\theta \mid y_1, \dots, y_n &amp;\sim \text{Galenshore}\left(a (n_0 + n) + 1, \sqrt{- (n_0 + n) (n_0 t_0 + n \bar{t}(\mathbf{y}))} \right) <span class="sc">\\</span></span>
<span id="cb44-987"><a href="#cb44-987" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-988"><a href="#cb44-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-989"><a href="#cb44-989" aria-hidden="true" tabindex="-1"></a><span class="fu">### c</span></span>
<span id="cb44-990"><a href="#cb44-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-991"><a href="#cb44-991" aria-hidden="true" tabindex="-1"></a>By taking advantage of the exponential family we already know $t(y) = -(y^2)$ is our sufficient statistic.</span>
<span id="cb44-992"><a href="#cb44-992" aria-hidden="true" tabindex="-1"></a>Specifically when comparing multiple $Y_1, \dots, Y_n$, we have $\sum_{i = 1}^n</span>
<span id="cb44-993"><a href="#cb44-993" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>(y^2)$ as our sufficient statistic.</span>
<span id="cb44-994"><a href="#cb44-994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-995"><a href="#cb44-995" aria-hidden="true" tabindex="-1"></a>Note it was a technicality that I picked $t(y) = -(y^2)$. I could have</span>
<span id="cb44-996"><a href="#cb44-996" aria-hidden="true" tabindex="-1"></a>parameterized the exponential family such that $t(y) = y^2$, and the negative is</span>
<span id="cb44-997"><a href="#cb44-997" aria-hidden="true" tabindex="-1"></a>distributed in the other functions. Also notice that since $y^2 &gt; 0$, the</span>
<span id="cb44-998"><a href="#cb44-998" aria-hidden="true" tabindex="-1"></a>$t(y)$s provide the same information.</span>
<span id="cb44-999"><a href="#cb44-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1000"><a href="#cb44-1000" aria-hidden="true" tabindex="-1"></a><span class="fu">### d</span></span>
<span id="cb44-1001"><a href="#cb44-1001" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1002"><a href="#cb44-1002" aria-hidden="true" tabindex="-1"></a>From the formula for the expectation of a Galenshore distribution, if we have</span>
<span id="cb44-1003"><a href="#cb44-1003" aria-hidden="true" tabindex="-1"></a>$$\theta \mid y_1, \dots, y_n \sim \text{Galenshore}\left(a (n_0 + n) + 1, \sqrt{- (n_0 + n) (n_0 t_0 + n \bar{t}(\mathbf{y}))} \right)$$</span>
<span id="cb44-1004"><a href="#cb44-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1005"><a href="#cb44-1005" aria-hidden="true" tabindex="-1"></a>then</span>
<span id="cb44-1006"><a href="#cb44-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1007"><a href="#cb44-1007" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-1008"><a href="#cb44-1008" aria-hidden="true" tabindex="-1"></a>\mathbb{E}(\theta \mid y_1, \dots, y_n) = \frac{\Gamma\left(\frac{1}{2} a(n_0 + n) + 2 \right)}{ \sqrt{- (n_0 + n) (n_0 t_0 + n \bar{t}(\mathbf{y}))} \Gamma\left( a(n_0 + n) + 1 \right)}</span>
<span id="cb44-1009"><a href="#cb44-1009" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-1010"><a href="#cb44-1010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1011"><a href="#cb44-1011" aria-hidden="true" tabindex="-1"></a><span class="fu">### e</span></span>
<span id="cb44-1012"><a href="#cb44-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1013"><a href="#cb44-1013" aria-hidden="true" tabindex="-1"></a>This one looks tedious...</span>
<span id="cb44-1014"><a href="#cb44-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1015"><a href="#cb44-1015" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.10</span></span>
<span id="cb44-1016"><a href="#cb44-1016" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1017"><a href="#cb44-1017" aria-hidden="true" tabindex="-1"></a>Change of variables</span>
<span id="cb44-1018"><a href="#cb44-1018" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1019"><a href="#cb44-1019" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb44-1020"><a href="#cb44-1020" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1021"><a href="#cb44-1021" aria-hidden="true" tabindex="-1"></a>If $\psi = g(\theta) = \log \frac{\theta}{1 - \theta}$, then </span>
<span id="cb44-1022"><a href="#cb44-1022" aria-hidden="true" tabindex="-1"></a>let $\theta = h(\psi) = \frac{e^\psi}{1 + e^\psi}$. Then, by the change of</span>
<span id="cb44-1023"><a href="#cb44-1023" aria-hidden="true" tabindex="-1"></a>variables formula,</span>
<span id="cb44-1024"><a href="#cb44-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1025"><a href="#cb44-1025" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-1026"><a href="#cb44-1026" aria-hidden="true" tabindex="-1"></a>p_{\psi}(\psi) &amp;= p_{\theta}(h(\psi)) \times \left| \frac{dh}{d\psi} \right| <span class="sc">\\</span></span>
<span id="cb44-1027"><a href="#cb44-1027" aria-hidden="true" tabindex="-1"></a>&amp;= \left<span class="co">[</span><span class="ot"> \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} \left( \frac{e^\psi}{1 + e^\psi}  \right)^{a - 1} \left( 1 - \frac{e^\psi}{1 + e^\psi} \right)^{b - 1} \right</span><span class="co">]</span> \times \frac{e^\psi}{(e^\psi + 1)^2} <span class="sc">\\</span></span>
<span id="cb44-1028"><a href="#cb44-1028" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)} \left<span class="co">[</span><span class="ot"> \left( \frac{e^\psi}{1 + e^\psi}  \right)^{a - 1} \left( \frac{1}{1 + e^\psi} \right)^{b - 1} \right</span><span class="co">]</span> \times \frac{e^\psi}{(e^\psi + 1)^2} <span class="sc">\\</span></span>
<span id="cb44-1029"><a href="#cb44-1029" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)} \left<span class="co">[</span><span class="ot"> \left( \frac{e^\psi}{1 + e^\psi}  \right)^a \left( \frac{1 + e^\psi}{e^\psi} \right) \left( \frac{1}{1 + e^\psi} \right)^b \left(\frac{1 + e^\psi}{1} \right) \right</span><span class="co">]</span> \times \frac{e^\psi}{(e^\psi + 1)^2} <span class="sc">\\</span></span>
<span id="cb44-1030"><a href="#cb44-1030" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)} \left<span class="co">[</span><span class="ot"> \left( \frac{e^\psi}{1 + e^\psi}  \right)^a \left( \frac{1}{1 + e^\psi} \right)^b \frac{(e^\psi + 1)^2}{e^\psi} \right</span><span class="co">]</span> \times \frac{e^\psi}{(e^\psi + 1)^2} <span class="sc">\\</span></span>
<span id="cb44-1031"><a href="#cb44-1031" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)} \left( \frac{e^\psi}{1 + e^\psi}  \right)^a \left( \frac{1}{1 + e^\psi} \right)^b</span>
<span id="cb44-1032"><a href="#cb44-1032" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-1033"><a href="#cb44-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1036"><a href="#cb44-1036" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-1037"><a href="#cb44-1037" aria-hidden="true" tabindex="-1"></a>h <span class="ot">=</span> <span class="cf">function</span>(psi) {</span>
<span id="cb44-1038"><a href="#cb44-1038" aria-hidden="true" tabindex="-1"></a>  <span class="fu">exp</span>(psi) <span class="sc">/</span> (<span class="fu">exp</span>(psi) <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb44-1039"><a href="#cb44-1039" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb44-1040"><a href="#cb44-1040" aria-hidden="true" tabindex="-1"></a>dh <span class="ot">=</span> <span class="cf">function</span>(psi) {</span>
<span id="cb44-1041"><a href="#cb44-1041" aria-hidden="true" tabindex="-1"></a>  <span class="fu">exp</span>(psi) <span class="sc">/</span> (<span class="fu">exp</span>(psi) <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb44-1042"><a href="#cb44-1042" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb44-1043"><a href="#cb44-1043" aria-hidden="true" tabindex="-1"></a>dpsi <span class="ot">=</span> <span class="cf">function</span>(psi, a, b) {</span>
<span id="cb44-1044"><a href="#cb44-1044" aria-hidden="true" tabindex="-1"></a>  (<span class="fu">gamma</span>(a <span class="sc">+</span> b) <span class="sc">/</span> (<span class="fu">gamma</span>(a) <span class="sc">*</span> <span class="fu">gamma</span>(b))) <span class="sc">*</span> <span class="fu">h</span>(psi)<span class="sc">^</span>(a <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">h</span>(psi))<span class="sc">^</span>(b <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> <span class="fu">dh</span>(psi)</span>
<span id="cb44-1045"><a href="#cb44-1045" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb44-1046"><a href="#cb44-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1047"><a href="#cb44-1047" aria-hidden="true" tabindex="-1"></a><span class="co"># To verify this is a valid PDF - with various a and b, integral should be approximately 1</span></span>
<span id="cb44-1048"><a href="#cb44-1048" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(<span class="cf">function</span>(p) <span class="fu">dpsi</span>(p, <span class="dv">2</span>, <span class="dv">2</span>), <span class="sc">-</span><span class="dv">100</span>, <span class="dv">100</span>)</span>
<span id="cb44-1049"><a href="#cb44-1049" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(<span class="cf">function</span>(p) <span class="fu">dpsi</span>(p, <span class="dv">4</span>, <span class="dv">8</span>), <span class="sc">-</span><span class="dv">100</span>, <span class="dv">100</span>)</span>
<span id="cb44-1050"><a href="#cb44-1050" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(<span class="cf">function</span>(p) <span class="fu">dpsi</span>(p, <span class="dv">10</span>, <span class="dv">1</span>), <span class="sc">-</span><span class="dv">100</span>, <span class="dv">100</span>)</span>
<span id="cb44-1051"><a href="#cb44-1051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1052"><a href="#cb44-1052" aria-hidden="true" tabindex="-1"></a>psi <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.05</span>)</span>
<span id="cb44-1053"><a href="#cb44-1053" aria-hidden="true" tabindex="-1"></a>density <span class="ot">=</span> <span class="fu">dpsi</span>(psi, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb44-1054"><a href="#cb44-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1055"><a href="#cb44-1055" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(psi, density, <span class="at">geom =</span> <span class="st">'line'</span>)</span>
<span id="cb44-1056"><a href="#cb44-1056" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-1057"><a href="#cb44-1057" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1058"><a href="#cb44-1058" aria-hidden="true" tabindex="-1"></a>This prior is also visualized via Monte-Carlo simulation in exercise 4.6. One</span>
<span id="cb44-1059"><a href="#cb44-1059" aria-hidden="true" tabindex="-1"></a>perhaps counterintuitive result here is that for a Beta-Binomial model,  an </span>
<span id="cb44-1060"><a href="#cb44-1060" aria-hidden="true" tabindex="-1"></a>uninformative (uniform) prior on $\theta$ *does* result in an informative prior</span>
<span id="cb44-1061"><a href="#cb44-1061" aria-hidden="true" tabindex="-1"></a>on the log-odds. This was apparently a "major criticism of Bayesian inference" </span>
<span id="cb44-1062"><a href="#cb44-1062" aria-hidden="true" tabindex="-1"></a>led by R.A. Fisher. For more information about this (and an introduction into </span>
<span id="cb44-1063"><a href="#cb44-1063" aria-hidden="true" tabindex="-1"></a>the Jeffreys' prior exercises next up), see [these lecture </span>
<span id="cb44-1064"><a href="#cb44-1064" aria-hidden="true" tabindex="-1"></a>notes](https://www2.stat.duke.edu/courses/Fall11/sta114/jeffreys.pdf).</span>
<span id="cb44-1065"><a href="#cb44-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1066"><a href="#cb44-1066" aria-hidden="true" tabindex="-1"></a>Point: it is impossible to have a totally diffuse prior on a random variable</span>
<span id="cb44-1067"><a href="#cb44-1067" aria-hidden="true" tabindex="-1"></a>with infinite support such as the log-odds, though...</span>
<span id="cb44-1068"><a href="#cb44-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1069"><a href="#cb44-1069" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb44-1070"><a href="#cb44-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1071"><a href="#cb44-1071" aria-hidden="true" tabindex="-1"></a>If $\psi = g(\theta) = \log \theta$, then</span>
<span id="cb44-1072"><a href="#cb44-1072" aria-hidden="true" tabindex="-1"></a>let $\theta = h(\psi) = e^\psi$. Then, by the change of</span>
<span id="cb44-1073"><a href="#cb44-1073" aria-hidden="true" tabindex="-1"></a>variables formula,</span>
<span id="cb44-1074"><a href="#cb44-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1075"><a href="#cb44-1075" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-1076"><a href="#cb44-1076" aria-hidden="true" tabindex="-1"></a>p_{\psi}(\psi) &amp;= p_{\theta}(h(\psi)) \times \left| \frac{dh}{d\psi} \right| <span class="sc">\\</span></span>
<span id="cb44-1077"><a href="#cb44-1077" aria-hidden="true" tabindex="-1"></a>&amp;= \left<span class="co">[</span><span class="ot"> \frac{b^a}{\Gamma(a)} \text{exp}\left(\psi (a - 1)\right) \text{exp}\left( -b e^\psi \right)  \right</span><span class="co">]</span> \times \text{exp}\left( \psi \right) <span class="sc">\\</span></span>
<span id="cb44-1078"><a href="#cb44-1078" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{b^a}{\Gamma(a)} \text{exp}\left(a\psi - \psi - b e^\psi + \psi \right) <span class="sc">\\</span></span>
<span id="cb44-1079"><a href="#cb44-1079" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{b^a}{\Gamma(a)} \text{exp}\left(a\psi - \psi - b e^\psi + \psi \right) <span class="sc">\\</span></span>
<span id="cb44-1080"><a href="#cb44-1080" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{b^a}{\Gamma(a)} \text{exp}\left(a\psi - b e^\psi \right)</span>
<span id="cb44-1081"><a href="#cb44-1081" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-1082"><a href="#cb44-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1085"><a href="#cb44-1085" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb44-1086"><a href="#cb44-1086" aria-hidden="true" tabindex="-1"></a>dpsi <span class="ot">=</span> <span class="cf">function</span>(psi, a, b) {</span>
<span id="cb44-1087"><a href="#cb44-1087" aria-hidden="true" tabindex="-1"></a>  (b<span class="sc">^</span>a) <span class="sc">/</span> (<span class="fu">gamma</span>(a)) <span class="sc">*</span> <span class="fu">exp</span>(a <span class="sc">*</span> psi <span class="sc">-</span> b <span class="sc">*</span> <span class="fu">exp</span>(psi))</span>
<span id="cb44-1088"><a href="#cb44-1088" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb44-1089"><a href="#cb44-1089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1090"><a href="#cb44-1090" aria-hidden="true" tabindex="-1"></a><span class="co"># To verify this is a valid PDF - with various a and b, integral should be approximately 1</span></span>
<span id="cb44-1091"><a href="#cb44-1091" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(<span class="cf">function</span>(p) <span class="fu">dpsi</span>(p, <span class="dv">2</span>, <span class="dv">2</span>), <span class="sc">-</span><span class="dv">100</span>, <span class="dv">100</span>)</span>
<span id="cb44-1092"><a href="#cb44-1092" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(<span class="cf">function</span>(p) <span class="fu">dpsi</span>(p, <span class="dv">4</span>, <span class="dv">8</span>), <span class="sc">-</span><span class="dv">100</span>, <span class="dv">100</span>)</span>
<span id="cb44-1093"><a href="#cb44-1093" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(<span class="cf">function</span>(p) <span class="fu">dpsi</span>(p, <span class="dv">10</span>, <span class="dv">1</span>), <span class="sc">-</span><span class="dv">100</span>, <span class="dv">100</span>)</span>
<span id="cb44-1094"><a href="#cb44-1094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1095"><a href="#cb44-1095" aria-hidden="true" tabindex="-1"></a>psi <span class="ot">=</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">10</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.05</span>)</span>
<span id="cb44-1096"><a href="#cb44-1096" aria-hidden="true" tabindex="-1"></a>density <span class="ot">=</span> <span class="fu">dpsi</span>(psi, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb44-1097"><a href="#cb44-1097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1098"><a href="#cb44-1098" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(psi, density, <span class="at">geom =</span> <span class="st">'line'</span>)</span>
<span id="cb44-1099"><a href="#cb44-1099" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb44-1100"><a href="#cb44-1100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1101"><a href="#cb44-1101" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.12</span></span>
<span id="cb44-1102"><a href="#cb44-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1103"><a href="#cb44-1103" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb44-1104"><a href="#cb44-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1105"><a href="#cb44-1105" aria-hidden="true" tabindex="-1"></a>If $Y$ is Binomial, then $p(y \mid \theta) = {n \choose y} \theta^y (1 - \theta)^{n - y}$.</span>
<span id="cb44-1106"><a href="#cb44-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1107"><a href="#cb44-1107" aria-hidden="true" tabindex="-1"></a>So $I(\theta) = -\mathbb{E}(\partial^2 \ell(y \mid \theta) / \partial \theta^2 )$ where $\ell(y \mid \theta) = \log p(y \mid \theta)$. Now,</span>
<span id="cb44-1108"><a href="#cb44-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1109"><a href="#cb44-1109" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-1110"><a href="#cb44-1110" aria-hidden="true" tabindex="-1"></a>\ell(y \mid \theta) &amp;= \log p(y \mid \theta) <span class="sc">\\</span></span>
<span id="cb44-1111"><a href="#cb44-1111" aria-hidden="true" tabindex="-1"></a>&amp;= \log \left( {n \choose y} \theta^y (1 - \theta)^{n - y} \right) <span class="sc">\\</span></span>
<span id="cb44-1112"><a href="#cb44-1112" aria-hidden="true" tabindex="-1"></a>&amp;= \log \left( {n \choose y} \right) + y \log(\theta) + (n - y) \log(1 - \theta) <span class="sc">\\</span></span>
<span id="cb44-1113"><a href="#cb44-1113" aria-hidden="true" tabindex="-1"></a>\ell_\theta(y \mid \theta) &amp;= \frac{y}{\theta}- \frac{n - y}{1 - \theta} <span class="sc">\\</span></span>
<span id="cb44-1114"><a href="#cb44-1114" aria-hidden="true" tabindex="-1"></a>\ell_{\theta \theta}(y \mid \theta) &amp;= - \frac{y}{\theta^2} - \frac{n - y}{(1 - \theta)^2} <span class="sc">\\</span></span>
<span id="cb44-1115"><a href="#cb44-1115" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-1116"><a href="#cb44-1116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1117"><a href="#cb44-1117" aria-hidden="true" tabindex="-1"></a>So</span>
<span id="cb44-1118"><a href="#cb44-1118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1119"><a href="#cb44-1119" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-1120"><a href="#cb44-1120" aria-hidden="true" tabindex="-1"></a>I(\theta) &amp;= -\mathbb{E}\left( -\frac{y}{\theta^2} - \frac{n - y}{(1 - \theta)^2} \right) <span class="sc">\\</span></span>
<span id="cb44-1121"><a href="#cb44-1121" aria-hidden="true" tabindex="-1"></a>&amp;= - \left( -\frac{1}{\theta^2} \mathbb{E}(y) - \frac{1}{(1 - \theta)^2} \mathbb{E}(n - y) \right) <span class="sc">\\</span></span>
<span id="cb44-1122"><a href="#cb44-1122" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{n\theta}{\theta^2} + \frac{n - n\theta}{(1 - \theta)^2} <span class="sc">\\</span></span>
<span id="cb44-1123"><a href="#cb44-1123" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{n}{\theta} + \frac{n}{1 - \theta} <span class="sc">\\</span></span>
<span id="cb44-1124"><a href="#cb44-1124" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{n}{\theta (1 - \theta)}</span>
<span id="cb44-1125"><a href="#cb44-1125" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-1126"><a href="#cb44-1126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1127"><a href="#cb44-1127" aria-hidden="true" tabindex="-1"></a>So Jeffreys' prior distribution is</span>
<span id="cb44-1128"><a href="#cb44-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1129"><a href="#cb44-1129" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-1130"><a href="#cb44-1130" aria-hidden="true" tabindex="-1"></a>p_J(\theta) &amp;= c \times \sqrt{\frac{n}{\theta (1 - \theta)}} <span class="sc">\\</span></span>
<span id="cb44-1131"><a href="#cb44-1131" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-1132"><a href="#cb44-1132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1133"><a href="#cb44-1133" aria-hidden="true" tabindex="-1"></a>where</span>
<span id="cb44-1134"><a href="#cb44-1134" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb44-1135"><a href="#cb44-1135" aria-hidden="true" tabindex="-1"></a>c = \left( \int_0^1 \sqrt{\frac{n}{\theta(1 - \theta)}} \; d\theta \right)^{-1}.</span>
<span id="cb44-1136"><a href="#cb44-1136" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb44-1137"><a href="#cb44-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1138"><a href="#cb44-1138" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb44-1139"><a href="#cb44-1139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1140"><a href="#cb44-1140" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-1141"><a href="#cb44-1141" aria-hidden="true" tabindex="-1"></a>\ell(y \mid \psi) &amp;= \log p(y \mid \psi) <span class="sc">\\</span></span>
<span id="cb44-1142"><a href="#cb44-1142" aria-hidden="true" tabindex="-1"></a>&amp;= \log \left( {n \choose y} e^{\psi y} (1 + e^\psi)^{-n} \right) <span class="sc">\\</span></span>
<span id="cb44-1143"><a href="#cb44-1143" aria-hidden="true" tabindex="-1"></a>&amp;= \log {n \choose y} + \psi y - n \log \left(1 + e^\psi \right) <span class="sc">\\</span></span>
<span id="cb44-1144"><a href="#cb44-1144" aria-hidden="true" tabindex="-1"></a>\ell_\psi(y \mid \psi) &amp;= y - \frac{n e^\psi}{e^\psi + 1} <span class="sc">\\</span></span>
<span id="cb44-1145"><a href="#cb44-1145" aria-hidden="true" tabindex="-1"></a>\ell_{\psi \psi}(y \mid \psi) &amp;= - \frac{n e^\psi}{\left( e^\psi + 1 \right)^2}</span>
<span id="cb44-1146"><a href="#cb44-1146" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-1147"><a href="#cb44-1147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1148"><a href="#cb44-1148" aria-hidden="true" tabindex="-1"></a>So</span>
<span id="cb44-1149"><a href="#cb44-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1150"><a href="#cb44-1150" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-1151"><a href="#cb44-1151" aria-hidden="true" tabindex="-1"></a>I(\psi) &amp;= -\mathbb{E}\left( - \frac{n e^\psi}{\left( e^\psi + 1 \right)^2}\right) <span class="sc">\\</span></span>
<span id="cb44-1152"><a href="#cb44-1152" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{n e^\psi}{\left( e^\psi + 1 \right)^2} <span class="sc">\\</span></span>
<span id="cb44-1153"><a href="#cb44-1153" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-1154"><a href="#cb44-1154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1155"><a href="#cb44-1155" aria-hidden="true" tabindex="-1"></a>Then Jeffreys prior is</span>
<span id="cb44-1156"><a href="#cb44-1156" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-1157"><a href="#cb44-1157" aria-hidden="true" tabindex="-1"></a>p_J(\psi) &amp;\propto \sqrt{ \frac{n e^\psi}{\left( e^\psi + 1 \right)^2} } <span class="sc">\\</span></span>
<span id="cb44-1158"><a href="#cb44-1158" aria-hidden="true" tabindex="-1"></a>&amp;\propto \frac{\sqrt{n e^\psi}}{e^\psi + 1}</span>
<span id="cb44-1159"><a href="#cb44-1159" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-1160"><a href="#cb44-1160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1161"><a href="#cb44-1161" aria-hidden="true" tabindex="-1"></a><span class="fu">### c</span></span>
<span id="cb44-1162"><a href="#cb44-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1163"><a href="#cb44-1163" aria-hidden="true" tabindex="-1"></a>If $\psi = g(\theta) = \log \frac{\theta}{1 - \theta}$, then </span>
<span id="cb44-1164"><a href="#cb44-1164" aria-hidden="true" tabindex="-1"></a>let $\theta = h(\psi) = \frac{e^\psi}{1 + e^\psi}$. Then, by the change of</span>
<span id="cb44-1165"><a href="#cb44-1165" aria-hidden="true" tabindex="-1"></a>variables formula,</span>
<span id="cb44-1166"><a href="#cb44-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1167"><a href="#cb44-1167" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-1168"><a href="#cb44-1168" aria-hidden="true" tabindex="-1"></a>p_{\psi}(\psi) &amp;\propto p_{\theta}(h(\psi)) \times \left| \frac{dh}{d\psi} \right| <span class="sc">\\</span></span>
<span id="cb44-1169"><a href="#cb44-1169" aria-hidden="true" tabindex="-1"></a>&amp;\propto \sqrt{\frac{n}{\frac{e^\psi}{1 + e^\psi} \left(1 - \frac{e^\psi}{1 + e^\psi}\right)}} \times \frac{e^\psi}{(e^\psi + 1)^2} <span class="sc">\\</span></span>
<span id="cb44-1170"><a href="#cb44-1170" aria-hidden="true" tabindex="-1"></a>&amp;\propto \sqrt{\frac{n(e^\psi + 1)^2}{e^\psi} } \times \frac{e^\psi}{(e^\psi + 1)^2} <span class="sc">\\</span></span>
<span id="cb44-1171"><a href="#cb44-1171" aria-hidden="true" tabindex="-1"></a>&amp;\propto \frac{\sqrt{n}(e^\psi + 1)}{\sqrt{e^\psi}} \times \frac{e^\psi}{(e^\psi + 1)^2} <span class="sc">\\</span></span>
<span id="cb44-1172"><a href="#cb44-1172" aria-hidden="true" tabindex="-1"></a>&amp;\propto \frac{\sqrt{n}\sqrt{e^\psi}}{e^\psi + 1} <span class="sc">\\</span></span>
<span id="cb44-1173"><a href="#cb44-1173" aria-hidden="true" tabindex="-1"></a>&amp;\propto p_J(\psi).</span>
<span id="cb44-1174"><a href="#cb44-1174" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-1175"><a href="#cb44-1175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1176"><a href="#cb44-1176" aria-hidden="true" tabindex="-1"></a>In this case, it has been demonstrated that Jeffreys' prior is invariant under monotone transformation.</span>
<span id="cb44-1177"><a href="#cb44-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1178"><a href="#cb44-1178" aria-hidden="true" tabindex="-1"></a><span class="fu">## 3.13</span></span>
<span id="cb44-1179"><a href="#cb44-1179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1180"><a href="#cb44-1180" aria-hidden="true" tabindex="-1"></a>Improper Jeffreys' prior</span>
<span id="cb44-1181"><a href="#cb44-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1182"><a href="#cb44-1182" aria-hidden="true" tabindex="-1"></a><span class="fu">### a</span></span>
<span id="cb44-1183"><a href="#cb44-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1184"><a href="#cb44-1184" aria-hidden="true" tabindex="-1"></a>If $Y \sim \text{Poisson}(\theta)$, $p(y) = \frac{\theta^y e^{-\theta}}{y!}$.</span>
<span id="cb44-1185"><a href="#cb44-1185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1186"><a href="#cb44-1186" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-1187"><a href="#cb44-1187" aria-hidden="true" tabindex="-1"></a>\ell(y \mid \theta) &amp;= \log p(y \mid \theta) <span class="sc">\\</span></span>
<span id="cb44-1188"><a href="#cb44-1188" aria-hidden="true" tabindex="-1"></a>&amp;= \log \left( \frac{\theta^y e^{-\theta}}{y!} \right) <span class="sc">\\</span></span>
<span id="cb44-1189"><a href="#cb44-1189" aria-hidden="true" tabindex="-1"></a>&amp;= \log \left( \frac{1}{y!} \right) + y \log (\theta) - \theta <span class="sc">\\</span></span>
<span id="cb44-1190"><a href="#cb44-1190" aria-hidden="true" tabindex="-1"></a>\ell_\theta(y \mid \theta) &amp;= \frac{y}{\theta} - 1<span class="sc">\\</span></span>
<span id="cb44-1191"><a href="#cb44-1191" aria-hidden="true" tabindex="-1"></a>\ell_{\theta \theta}(y \mid \theta) &amp;= -\frac{y}{\theta^2} <span class="sc">\\</span></span>
<span id="cb44-1192"><a href="#cb44-1192" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-1193"><a href="#cb44-1193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1194"><a href="#cb44-1194" aria-hidden="true" tabindex="-1"></a>So</span>
<span id="cb44-1195"><a href="#cb44-1195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1196"><a href="#cb44-1196" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-1197"><a href="#cb44-1197" aria-hidden="true" tabindex="-1"></a>I(\psi) &amp;= -\mathbb{E}\left( - \frac{y}{\theta^2} \right) <span class="sc">\\</span></span>
<span id="cb44-1198"><a href="#cb44-1198" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{\theta^2} \mathbb{E}(y) <span class="sc">\\</span></span>
<span id="cb44-1199"><a href="#cb44-1199" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{\theta}{\theta^2} <span class="sc">\\</span></span>
<span id="cb44-1200"><a href="#cb44-1200" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{\theta}</span>
<span id="cb44-1201"><a href="#cb44-1201" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-1202"><a href="#cb44-1202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1203"><a href="#cb44-1203" aria-hidden="true" tabindex="-1"></a>and Jeffreys' prior is</span>
<span id="cb44-1204"><a href="#cb44-1204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1205"><a href="#cb44-1205" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-1206"><a href="#cb44-1206" aria-hidden="true" tabindex="-1"></a>p_J(\theta) &amp;= c \times \sqrt{\frac{1}{\theta}} <span class="sc">\\</span></span>
<span id="cb44-1207"><a href="#cb44-1207" aria-hidden="true" tabindex="-1"></a>&amp;= c \times \frac{1}{\sqrt{\theta}}</span>
<span id="cb44-1208"><a href="#cb44-1208" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-1209"><a href="#cb44-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1210"><a href="#cb44-1210" aria-hidden="true" tabindex="-1"></a>Notice that, to be a proper probability distribution, it must be the case that</span>
<span id="cb44-1211"><a href="#cb44-1211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1212"><a href="#cb44-1212" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-1213"><a href="#cb44-1213" aria-hidden="true" tabindex="-1"></a>\int_0^{\infty} c \times \frac{1}{\sqrt{\theta}} \; d\theta = c \int_0^{\infty} \frac{1}{\sqrt{\theta}} \; d\theta = 1</span>
<span id="cb44-1214"><a href="#cb44-1214" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-1215"><a href="#cb44-1215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1216"><a href="#cb44-1216" aria-hidden="true" tabindex="-1"></a>However, this integral does not converge ($p$-test, $p = 1/2$), so there is no</span>
<span id="cb44-1217"><a href="#cb44-1217" aria-hidden="true" tabindex="-1"></a>value of $c$ such that this is a valid probability distribution. Thus (per the</span>
<span id="cb44-1218"><a href="#cb44-1218" aria-hidden="true" tabindex="-1"></a>name of the exercise) this is an *improper* Jeffreys' prior.</span>
<span id="cb44-1219"><a href="#cb44-1219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1220"><a href="#cb44-1220" aria-hidden="true" tabindex="-1"></a><span class="fu">### b</span></span>
<span id="cb44-1221"><a href="#cb44-1221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1222"><a href="#cb44-1222" aria-hidden="true" tabindex="-1"></a>However, even if $p_J(\theta)$ is not a valid probability distribution, we can</span>
<span id="cb44-1223"><a href="#cb44-1223" aria-hidden="true" tabindex="-1"></a>still imagine performing "inference" using this prior.</span>
<span id="cb44-1224"><a href="#cb44-1224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1225"><a href="#cb44-1225" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-1226"><a href="#cb44-1226" aria-hidden="true" tabindex="-1"></a>f(\theta, y) &amp;= \sqrt{I(\theta)} \times p(y \mid \theta) <span class="sc">\\</span></span>
<span id="cb44-1227"><a href="#cb44-1227" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{\sqrt{\theta}} \times  \frac{\theta^y e^{-\theta}}{y!} <span class="sc">\\</span></span>
<span id="cb44-1228"><a href="#cb44-1228" aria-hidden="true" tabindex="-1"></a>&amp;= \theta^{-1/2} \times  \frac{\theta^y e^{-\theta}}{\Gamma(y + 1)} <span class="sc">\\</span></span>
<span id="cb44-1229"><a href="#cb44-1229" aria-hidden="true" tabindex="-1"></a>&amp;= \theta^{y - 1/2} e^{-\theta} \frac{1}{\Gamma(y + 1)} <span class="sc">\\</span></span>
<span id="cb44-1230"><a href="#cb44-1230" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-1231"><a href="#cb44-1231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1232"><a href="#cb44-1232" aria-hidden="true" tabindex="-1"></a>As a function of $\theta$ only, this is proportional to</span>
<span id="cb44-1233"><a href="#cb44-1233" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb44-1234"><a href="#cb44-1234" aria-hidden="true" tabindex="-1"></a>\dots &amp;\propto \theta^{y - 1/2} e^{-\theta} <span class="sc">\\</span></span>
<span id="cb44-1235"><a href="#cb44-1235" aria-hidden="true" tabindex="-1"></a>&amp;\propto \text{dgamma}(\theta, y + \frac{1/2}, 1)</span>
<span id="cb44-1236"><a href="#cb44-1236" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb44-1237"><a href="#cb44-1237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1238"><a href="#cb44-1238" aria-hidden="true" tabindex="-1"></a>Since $\Gamma(y + \frac{1/2}, 1)$, $y &gt;= 0$ *is* a valid parameterization of a </span>
<span id="cb44-1239"><a href="#cb44-1239" aria-hidden="true" tabindex="-1"></a>Gamma density, it follows that we can normalize $f(\theta, y)$ such that it </span>
<span id="cb44-1240"><a href="#cb44-1240" aria-hidden="true" tabindex="-1"></a>represents a valid posterior density for $\theta$. If we actually calculate</span>
<span id="cb44-1241"><a href="#cb44-1241" aria-hidden="true" tabindex="-1"></a>$f(\theta, y) / \int f(\theta, y) \; d\theta$ (which I won't do here), we will</span>
<span id="cb44-1242"><a href="#cb44-1242" aria-hidden="true" tabindex="-1"></a>get such a Gamma density.</span>
<span id="cb44-1243"><a href="#cb44-1243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-1244"><a href="#cb44-1244" aria-hidden="true" tabindex="-1"></a>Notes: the prior in a) can be thought of as an improper $\text{Gamma}(1/2, 0)$. </span>
<span id="cb44-1245"><a href="#cb44-1245" aria-hidden="true" tabindex="-1"></a>Since improper Jeffreys' priors are not real probability densities, </span>
<span id="cb44-1246"><a href="#cb44-1246" aria-hidden="true" tabindex="-1"></a>their usage is controversial for some. (Who?)</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>